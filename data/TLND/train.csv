BIC,description
1,"Steps to reproduce:

1) Create a cinder volume with 1GB
2) Upload the volume created in step 1 to image
3) Create a cinder volume from the image created in step 2

The volume gets created correctly but the disk extend is throwing an exception with message "" A specified parameter was not correct""."
1,"When nova-network is being used, nova-api-metadata requires db access which in some configurations means the compute nodes still connect to the database.
It connects to the db when trying to map the fixed ip address to an instance uuid:
fixed_ip = network.API().get_fixed_ip_by_address(ctxt, address)
This should be changed to do an rpc call to conductor."
1,"With NVP advanced service plugin, router creation is asynchronous while all service call is synchronous, so it is possible that advanced service request is called before edge deployment completed.
One solution is to check the router status before deploying an advanced service. If the router is not in ACTIVE status, the service deployment request would return ""Router not ready"" error."
1,"In preparing the patch for bug 1250596 I found that _destroy_router_namespaces does not properly destroy the metadata proxy.

It may be that it was not called because _destroy_router_namespaces requires a router_info object that is not available in this context. Maru suggested refactoring _destroy_metadata_proxy and _spawn_metadata_proxy to enable this."
1,"If a user tries to upload a file that is larger than 2GiB the upload fails due to an error OverflowError.
Actually this happens in python/socket.py, but it can be fixed either in swift/common/utils.py line 2406 or swift/common/middleware/formpost.py line 245. In both cases it's possible to use a long(-1) as the default instead of None (which would be converted to an int(-1) later on).
Note that this happens on 64bit systems."
0,"at line
https://github.com/openstack/neutron/blob/master/neutron/services/vpn/service_drivers/ipsec.py#L172
it is obvious the router must have gateway interface set then it can be used as vpnservce router."
0,"There was a comment on https://review.openstack.org/#/c/90583/ (after it was approved for merge) to add an extra test case to handle more than 1 ephemeral disk, as well as correct a comment in the code to be more accurate. The purpose of this bug is to add the extra test case for _get_instance_block_device_info to test more than 1 ephemeral disk and correct one of the comments in _get_instance_block_device_info."
0,Current metadata agent supports only http connection to nova metadata service. Implement https support.
0,Fibre Channel zoning code requires that the storage drivers return an initiator_target_map from initialize_connection and terminate_connection.
1,"ofagent's arp responder has some LOG.info which can be triggered by tenant's OSes.
they allow bad tenants flood host logs."
1,"No issues if there are atleast 1 secgroup defined for the server.
If no secgroups are defined for the server, it fails with 400 error.
$ nova --debug list-secgroup vp25q00cs-osfe11b124f4.isg.apple.com
.
.
.
RESP: [400] CaseInsensitiveDict({'date': 'Wed, 12 Mar 2014 17:08:11 GMT', 'content-length': '141', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61'})
RESP BODY: {""badRequest"": {""message"": ""The server could not comply with the request since it is either malformed or otherwise incorrect."", ""code"": 400}}
DEBUG (shell:740) The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 737, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 673, in main
    args.func(self.cs, args)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/shell.py"", line 1904, in do_list_secgroup
    groups = server.list_security_group()
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 328, in list_security_group
    return self.manager.list_security_group(self)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 883, in list_security_group
    base.getid(server), 'security_groups', SecurityGroup)
  File ""/Library/Python/2.7/site-packages/novaclient/base.py"", line 61, in _list
    _resp, body = self.api.client.get(url)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 229, in get
    return self._cs_request(url, 'GET', **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 213, in _cs_request
    **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 195, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 189, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
ERROR: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)"
1,"When creating a snapshot from a VM instance, following error appears in Nova compute log:
 2013-12-17 10:39:47.943 28210 ERROR nova.openstack.common.rpc.amqp [req-3a82cad9-3213-47e8-8a42-0f6e2a75008a a2eecc1caf5f40c5ab50405b68730c20 e255029e4a614ed1a5412192db588e74] Exception during message handling
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp **args)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp payload)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp pass
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 319, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp % image_id, instance=instance)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 309, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2293, in snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp task_states.IMAGE_SNAPSHOT)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2324, in _snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp update_task_state)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1399, in snapshot
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp snapshot_backend.snapshot_extract(out_path, image_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/imagebackend.py"", line 531, in snapshot_extract
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp images.convert_image(snap, target, out_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/images.py"", line 179, in convert_image
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp utils.execute(*cmd, run_as_root=run_as_root)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/utils.py"", line 177, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp return processutils.execute(*cmd, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp cmd=' '.join(cmd))
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Command: qemu-img convert -O qcow2 rbd:instances/instance-0000000f_disk /var/lib/nova/instances/snapshots/tmp63tXq0/b916d3ca7fbe46cba4ca7ce5e0138ee9
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Exit code: 1
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stdout: ''
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stderr: ""qemu-img: error connecting\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk': Operation not supported\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk'\n"""
1,"When there is hypervisor gets removed, the compute_node_get_all() will not return stat for new added hypervisors.

In the following codes of compute_node_get_all() of nova/db/sqlalchemy, it assume all the record in compute_node_stats should have a matched compute node. However in current implementation of nova conductor API of compute_node_delete(), the records in compute_node_stats is not deleted. Therefore when a hypervisor gets removed, there is no node matching the record of compute_node_stats which belongs to the removed hypervisor in following codes. As a result, all the nodes will be set with 'stats' of [].

    # Join ComputeNode & ComputeNodeStat manually.
    # NOTE(msdubov): ComputeNode and ComputeNodeStat map 1-to-Many.
    # Running time is (asymptotically) optimal due to the use
    # of iterators (itertools.groupby() for ComputeNodeStat and
    # iter() for ComputeNode) - we handle each record only once.
    compute_nodes.sort(key=lambda node: node['id'])
    compute_nodes_iter = iter(compute_nodes)
    for nid, nsts in itertools.groupby(stats, lambda s: s['compute_node_id']):
        for node in compute_nodes_iter:
            if node['id'] == nid:
                node['stats'] = list(nsts)
                break
            else:
                node['stats'] = []

    return compute_nodes

We need enhance either nova conductor API to clean up all the record related with instance."
1,"Seem to be experiencing a bug with libvirt.xml device formatting when --ephemeral flag is used after initial booth and then use of nova stop/start or nova reboot --hard. We are using following libvirt options in nova.conf for storage:
libvirt_images_type=lvm
libvirt_images_volume_group=vglocal
When normally using nova boot with a flavor that has ephemeral defined it create two LVM volumes appropriatly ex.
instance-0000077e_disk
instance-0000077e_disk.local
The instance libvirt.xml contains disk devices entry as follows:
<devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
If we use ""nova boot --flavor 757c75fa-0b6d-4d4f-a128-27813009bff4 --image caa978e0-acae-4205-a4a4-2cf159c166fd --nic net-id=44f2fb0b-0a7a-475c-8fff-54cd4b37958b --ephemeral size=1 --ephemeral size=1 localdisk-1"" the LVM disks for ephemeral goes through enumeration logic whether there is one or more --ephemeral options
 instance-000007ed_disk
 instance-000007ed_disk.eph0
 instance-000007ed_disk.eph1
The instance libvirt.xml after instance spawn has disk device entries like below and the instances happily boots.
 <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph0""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph1""/>
      <target bus=""virtio"" dev=""vdc""/>
    </disk>
If nova stop/start or nova reboot --hard is executed the instance is destroyed and libvirt.xml gets recreated. At this stage whatever values we passed with --ephemeral are not respected and libvirt.xml revirts to configuration that would have been generated without the use of the --ephemeral option like below where we only have one extra disk and it is not using the enumerated naming.
  <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
This causes instances booting to fail at this stage. The nova block_device_mapping table has records for all 3 devices."
1,"novaclient has following option
--ephemeral size=<size>[,format=<format>]
                        Create and attach a local ephemeral block device of
                        <size> GB and format it to <format>.
so
nova boot --flavor 21 --key_name mykey --image 43ca519b-979b-4803-95ad-b9f160f1a337 --security_group default --ephemeral size=1 --ephemeral size=2,format=ext4 test12
should work
however, the eph disk created is ext3 ,ignore the option specified by format"
1,"process_port_binding is always called when a port is updated: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L617
However, its current implementation: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L202
Return True, hence triggering a notification in many cases.
The code has a check for returning false when the host is not set and the vif is already bound: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L213
However, this is perhaps not triggered in many cases.
As an example, in this job: http://logs.openstack.org/20/57420/9/experimental/check-tempest-devstack-vm-neutron-isolated-parallel/269a314
Of 538 update_port calls to the plugin, process_port_binding turns need_port_update_notify from False to Truue in
It is therefore worth ensuring no notification is sent if the host doesn't change (the other binding parameters should not change on update and even if they can they should not trigger a notification) in 72 cases out of 82 cases where the flag was false before invoking process_port_binding
Looking at the logs - the port should not have been notified because of bindings in any case."
1,"Steps to Reproduce:
1. Create network, subnet
2.Create pool,member vip and healthmonitor.
neutron lb-pool-list
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| id | name | provider | lb_method | protocol | admin_state_up | status |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| d488b38e-c8f1-4db6-bb5f-8b2500bef0ed | pool1 | haproxy | ROUND_ROBIN | HTTP | True | ACTIVE |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
neutron lb-member-list
+--------------------------------------+-----------+---------------+----------------+--------+
| id | address | protocol_port | admin_state_up | status |
+--------------------------------------+-----------+---------------+----------------+--------+
| 303f3d66-c51c-4c92-b5da-d43440db9fb2 | 10.10.1.5 | 80 | True | ACTIVE |
| ac58985b-49de-480e-97e5-bff80a538c87 | 10.10.1.4 | 80 | True | ACTIVE |
+--------------------------------------+-----------+---------------+----------------+--------+
4. Validate the algorithm and try to send request from server to client .
5. Check the lp-pool-stats for the pool created.
Actual Results:
neutron lb-pool-stats pool1
+--------------------+-------+
| Field | Value |
+--------------------+-------+
| active_connections | 0 |
| bytes_in | 1440 |
| bytes_out | 560 |
| total_connections | 0 |
+--------------------+-------+
Connections in response for this command always show 0
Expected Results: Active connection should show the value when the connection is active and total connenction should show total number of active connections"
0,"The links in the GET /v1 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v1/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
0,"Glance v2 image property quota enforcement can be unintuitive.
There are a number of reasons an image may have more properties than the image_propery_quota allows. E.g. if the quota is lowered or when it is first created. If this happens then any request to modify an image must result in the image being under the new quota. This means that even if the user is removing quotas they can still get an 413 overlimit from glance if the result would still be over the limit.
This is not a great user experience and is unintuitive. Ideally a user should be able to remove properties or any other action except for adding a property when they are over their quota for a given image."
1,"When lazy message translation is enabled in Nova, the check_update.sh calls generate_sample.sh, which uses a copy of oslo's config/generator.py which produces the following message:
CRITICAL nova [-] TypeError: Message objects do not support addition.
The config/generator.py module installs i18n without lazy enabled (named parameter 'lazy' not specified):
gettextutils.install('nova')
To gather information about the projects options, it loads the project modules looking for entry points. When these modules are loaded, they may contain code to enable lazy. In the case of Nova this is the nova/cmds/__init__.py which calls:
gettextutils.enable_lazy()
This means that the messages returned with information for the entry points are lazy enabled. Thus when config/generator.py tries to work with the help message for the option associated with the Nova modules:
opt_help += ' (' + OPT_TYPES[opt_type] + ')'
it fails because opt_help is a gettextutils.Message instance, which doesn't support addition."
1,"If fixed IP allocation fails, for example because nova's network interfaces got renamed after a reboot, nova will loop continuously trying, and failing, to create a new instance. For every attempted spawn the instance will end up with an additional fixed IP allocated to it. This is because the code is associating the IP, but not disassociating it if the function fails."
0,"The BigSwitch servermanager uses the synchronized decorators on rest backend calls. It currently sets the external flag to True, which isn't necessary since there aren't multiple independent processes running the plugin on the same host.
This results in the unnecessary creation of a lock file when it can just be handled by the default in-memory locks."
0,"The LVMISERDriver takes an option 'num_iser_scan_tries', but it appears that this is not being processed correctly.

The 'num_iscsi_scan_tries' option was renamed to 'num_volume_device_scan_tries', but setting this ISER option still sets the old configuration field which is never read.

(I have not tested this behavior, just found via code inspection.)"
0,"At now NEC plugin assumes REST API of OpenFlow controller starts with /, but NEC OpenFlow controller supports a prefix for REST API. NEC plugin allows to use URI prefix when talking with OpenFlow controller."
0,"This bug is relate with the following bug:
https://bugs.launchpad.net/neutron/+bug/1321864

The fix patch for the bug delete overlap check for fixed ip and allowed ip address range.

I think that we also need to remove the following code if we do not need to check fixed_ip and allowed address pair overlap:
https://github.com/openstack/neutron/blob/master/neutron/db/allowedaddresspairs_db.py
51 for fixed_ip in port['fixed_ips']:
52 if ((fixed_ip['ip_address'] == address_pair['ip_address'])
53 and (port['mac_address'] ==
54 address_pair['mac_address'])):
55 raise addr_pair.AddressPairMatchesPortFixedIPAndMac()

https://github.com/openstack/neutron/blob/master/neutron/extensions/allowedaddresspairs.py
35class AddressPairMatchesPortFixedIPAndMac(nexception.InvalidInput):
36 message = _(""Port's Fixed IP and Mac Address match an address pair entry."")"
0,"test_uri_length_limit honours HTTP_PROXY (if set). This means the test queries to http://localhost:$port/ hit the wrong localhost when $HTTP_PROXY is another host, resulting in a meaningless test scenario, and apparent failures with HTTP status codes other than REQUEST_URI_TOO_LARGE (414)."
1,"2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.base [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] Request body: {u'subnets': None}
2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.resource [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] create failed
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/site-packages/neutron/api/v2/resource.py"", line 87, in resource
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 357, in create
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource allow_bulk=self._allow_bulk)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 573, in prepare_request_body
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource bulk_body = [prep_req_body(item) for item in body[collection]]
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource TypeError: 'NoneType' object is not iterable"
0,Updates network description and validates multicast ip range check
0,"All the operations (create, update or delete) haven't been covered by unit tests.
Bug #1324450 about the delete operations would have been caught."
0,"BaremetalHostManager could distinguish baremetal hosts by checking ""baremetal_driver"" exists in capabilities or not. However, now BaremetalHostManager cannot, because capabilities are not reported to scheduler and BaremetalHostManager always receives empty capabilities. As a result, BaremetalHostManager just does the same thing as the original HostManager."
1,"When running the driver certification tests for Icehouse, we found a bug with the 3PAR FC/iSCSI drivers.
The tempest test calls cinder to create a clone of a volume, and then immediately calls delete on the clone. The 3PAR won't allow you to do that unless you stop the copy first. You get a very generic exception, that isn't helpful."
1,"The DHCP requests were not being responded to after they were seen on the undercloud network interface. The neutron services were restarted in an attempt to ensure they had the newest configuration and knew they were supposed to respond to the requests.
Rather than using the heat stack create (called in devtest_overcloud.sh) to test, it was simple to use the following to directly boot a baremetal node.
    nova boot --flavor $(nova flavor-list | grep ""|[[:space:]]*baremetal[[:space:]]*|"" | awk '{print $2}) \
          --image $(nova image-list | grep ""|[[:space:]]*overcloud-control[[:space:]]*|"" | awk '{print $2}') \
          bm-test1
Whilst the baremetal node was attempting to pxe boot a restart of the neutron services was performed. This allowed the baremetal node to boot.
It has been observed that a neutron restart was needed for each subsequent reboot of the baremetal nodes to succeed."
0," SSHPool is using paramiko.AutoAddPolicy() as default. This may lead security issue without being notified. The utility should allow customized usage when create the pool or session. Also the host_keys file should be allowed to be customized so that any driver utilizing the SSHPool should have their customized security setting or delegate to customer's scenario & configuration to determine the policy and key files."""
1,"During the investigation of bug 1231704,
I noticed it is valid that there is no session persistence attribute
and it means no session persistence mechanism is used in load balancing.
It seems an intentional behavior according to the code:
https://github.com/openstack/neutron/blob/master/neutron/db/loadbalancer/loadbalancer_db.py#L241
IMHO, it is confusing that it is determined whether session_persisntece is returned or not by session persistence mode. I would suggest that returning ""session_persistence: null"" (or {}) when no session_persistence is used.
In addition, if we send {'session_persistence': null} in lb-vip-update, session_persistence is cleared.
It looks reasonable to me that ""session_persistence = null"" means no session_persistence is used.
From the point of view of API, I believe all defined attributes in an extension should be returned in a response.
I would like to ask opinions from the community."
0,"Unit tests for neutron's linux dhcp section use an invalid prefix and host address.
https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_linux_dhcp.py#L129"
1,"cinder/volume/drivers/netapp/iscsi.py and
cinder/volume/drivers/netapp/utils.py are used as follows.

  raise exception.InvalidInput(data=msg)

I think the following is correct.

  raise exception.InvalidInput(reason=msg)"
1,The db query used to look for instances in need of cleanup lacks a filter for soft-deleted instances.
1,"""os-start/os-stop"" server action does not work for V2.1 API.
Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1,"I've tried to remove a pool that has 2 members and a health monitor, operation failed with the following popup:
""Error: Unable to delete pool. 409-{u'NeutronError': {u'message': u'Pool f5004d04-4461-4a9a-aa7c-04a9bdfde974 is still in use', u'type': u'PoolInUse', u'detail': u''}}""
I did expect this operation to fail, I just didn't expect it to be available in horizon while the pool still has other objects associated with it and I didn't expect it to leave the pool in ""PENDING_DELETE"" status.
The exception from the log file:
2013-10-20 16:12:13.564 22804 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Unable to destroy device for pool: f5004d04-4461-4a9a-aa7c-04a9bdfde974
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 244, in destroy_device
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager self.driver.destroy(pool_id)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 92, in destroy
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager ns.garbage_collect_namespace()
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 141, in garbage_collect_namespace
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager self.netns.delete(self.namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 440, in delete
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager self._as_root('delete', name, use_root_namespace=True)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 206, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager kwargs.get('use_root_namespace', False))
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 65, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager root_helper=root_helper)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/utils.py"", line 61, in execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager raise RuntimeError(m)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager RuntimeError:
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'delete', 'qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974']
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Exit code: 255
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stdout: ''
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stderr: 'Cannot remove /var/run/netns/qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974: Device or resource busy\n'
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager"
1,"VMware VCDriver currently reports the disk usage 'local_gb_used' metric as the capacity of a randomly chosen datastore in the vCenter cluster. The right value would be the aggregate capacity of all datastores in the cluster.
Eg.
With a cluster having 3 Datastores (3x 100GB LUN), only 100GB appears in ""free_disk_gb"" when you execute ""nova hypervisor-show""."
0,"IOopsFilter will take instance in following state
task_states.RESIZE_MIGRATING, task_states.REBUILDING, task_states.RESIZE_PREP, task_states.IMAGE_SNAPSHOT, task_states.IMAGE_BACKUP
into consideration ,if the instance on this host exceed the maximum number of instance allowed to do I/O
the scheduler will fail
we need to take UNSHELVING and RESCUING into consideration"
0,If an exception occurs in _heal_instance_info_cache the actual stack trace is not shown and is hard to track down something is failing as it's log level is debug.
1,"The method at vmops.py _get_datacenter_ref_and_name does not calculate datacenter properly.

    def _get_datacenter_ref_and_name(self):
        """"""Get the datacenter name and the reference.""""""
        dc_obj = self._session._call_method(vim_util, ""get_objects"",
                ""Datacenter"", [""name""])
        vm_util._cancel_retrieve_if_necessary(self._session, dc_obj)
        return dc_obj.objects[0].obj, dc_obj.objects[0].propSet[0].val

This will not be correct on systems with more than one datacenter.

Stack trace from logs:"
1,"if L2 agent uses enhanced-security-group-rpc, in bellow case there will be a KeyError in neutron server:
1. Create security group with IPv6 ingress rule but no IPv4 ingress rule.
  (or delete IPv4 ingress rule from default security group)
2. Launch a VM on an IPv4 subnet, making it member of sec group created earlier

Instance will not get its network info. Neutron server starts reporting following errors and sends them to agent on each request for devices info:

2014-09-24 02:01:51.353 ERROR oslo.messaging.rpc.dispatcher [req-9b631b65-a753-4292-8442-98936a31db74 None None] Exception during message handling: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/neutron/neutron/api/rpc/handlers/securitygroups_rpc.py"", line 75, in security_group_info_for_devices
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher return self.plugin.security_group_info_for_ports(context, ports)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 201, in security_group_info_for_ports
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher return self._get_security_group_member_ips(context, sg_info)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 209, in _get_security_group_member_ips
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher if ip not in sg_info['sg_member_ips'][sg_id][ethertype]:
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher KeyError: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher
2014-09-24 02:01:51.354 ERROR oslo.messaging._drivers.common [req-9b631b65-a753-4292-8442-98936a31db74 None None] Returning exception 'IPv4' to caller"
0,"Import cStringIO
cStringIO.StringIO()
should be :
from six.moves import cStringIO
cStringIO.StringIO()=>cStringIO
For Python3 compatible."
1,"When a delete is issued for an instance that doesn't have a cell_name in the db, a delete is broadcast to all cells. As that message passes through the cells rpc layer the instance is converted from an object to a dict. This causes a problem when it gets to the cell since the delete methods expect to receive an object.
2014-03-13 15:21:49.717 31063 ERROR nova.cells.messaging [req-d2d5f4ea-4010-405a-b52a-8f19d3991498 10110789 5877036] Error processing message locally: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1291, in _process_message_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1101, in instance_delete_everywhere
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self.compute_api.delete(message.ctxt, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 216, in _wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1710, in delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self._delete_instance(context, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1700, in _delete_instance
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging task_state=task_states.DELETING)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1395, in _delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging if instance.disable_terminate:
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging"
1,"Test Step:
1. Create a type-1, backend IBM storwize SVC, without replication .
2. Create a type-2, backend IBM storwize SVC, with replication = TRUE.
3. Create a volume without type information:
command :銆€cinder create 1
check the volume status, the information of the volume :
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None | 1 | None | false | |
The volume type is none.
4. Do Retype to enable replication for the volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will always be failed, check the c-vol.log, it was found :
2014-09-16 09:16:01.664 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mVolume ffd8f99a-9e17-4943-ac56-eee2e060a86a: driver error when trying to retype, falling back to generic mechanism.^[[00m
2014-09-16 09:16:01.665 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mInvalid volume type: id cannot be None^[[00m
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m host)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 926, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m model_update = self.replication.create_replica(ctxt, volume)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 73, in create_replica
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m vol_type = volume_types.get_volume_type(ctxt, vol_type)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/volume_types.py"", line 103, in get_volume_type
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m raise exception.InvalidVolumeType(reason=msg)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mInvalidVolumeType: Invalid volume type: id cannot be None
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m
2014-09-16 09:16:01.737 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mException during message handling: Volume migration failed: Retype requires migration but is not allowed.^[[00m
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m incoming.message))
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1434, in retype
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m raise exception.VolumeMigrationFailed(reason=msg)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mVolumeMigrationFailed: Volume migration failed: Retype requires migration but is not allowed.
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m
It looks the volume type can't be ""none"" if retype to a volume with replication=TRUE.
5. Do retype to try if all the retype are disabled for the ""none"" type volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-1
Since type-1 replication =false, the operation was successful.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None | 1 | type-1 | false | |
6. Do retype to enable replication again with command:
 cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will be success since the current operation is from type-1 to type-2.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None | 1 | type-2 | false | |
From the front test, the retype to enable replication with ""none"" volume type have issues"
0,"This looks to be due to tests test_get_port_vnic_info_2 and 3 sharing some code and is easily reproduced by running these two tests alone with no concurrency.
./run_tests.sh --concurrency 1 test_get_port_vnic_info_2 test_get_port_vnic_info_3
The above always results in:
Traceback (most recent call last):
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2615, in test_get_port_vnic_info_3
    self._test_get_port_vnic_info()
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 1201, in patched
    return func(*args, **keywargs)
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2607, in _test_get_port_vnic_info
    fields=['binding:vnic_type', 'network_id'])
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 2 times."
0,"Given an attribute name and a list of values equal_any() is meant to produce a WHERE clause which returns rows for which the column (denoted by an attribute of an SQLAlchemy model) is equal to ANY of passed values that involves using of SQL OR operator. In fact, AND operator is used to combine equality expressions.
E.g. for a model:
class Instance(BaseModel):
    __tablename__ = 'instances'
   id = sa.Column('id', sa.Integer, primary_key=True)
   ...
   task_state = sa.Column('task_state', sa.String(30))
using of equal_any():
  q = model_query(context, Instance).
  constraint = Constraint({'task_state': equal_any('error', 'deleting')})
  q = constraint.apply(Instance, q)
will produce:
SELECT * from instances
WHERE task_state = 'error' AND task_state = 'deleting'
instead of expected:
SELECT * from instances
WHERE task_state = 'error' OR task_state = 'deleting'"
1,"In _process_routers, the L3 agent makes an RPC call each time that _process_routers is called to get the external network id as long as it was not configured using the gateway_external_network_id configuration option.
This adds some time process a router. Since the external id will not be changing, we should be able to get away with fetching and saving this value once."
1,"""cinder retype"" seems to work perfectly while retyping a volume having a initial volume-type i.e. while volume is created with volume-type.
but it gets stuck at 'retyping' state while retyping a volume having a volume-type = None (for the volumes created with no volume-type)
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f | available | None | 1 | None | false | |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
openstack@ubuntu:~$ cinder type-list
+------------------------------------------------+---------------+
| ID | Name |
+------------------------------------------------+---------------+
| ef8ad765-cea2-4fba-9162-a10d73a6c823 | abcd |
+------------------------------------------------+---------------+
openstack@ubuntu:~$ cinder retype f508dacd-845d-4d09-b69a-6412ec93213f abcd
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f | retyping | None | 1 | None | false | |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+"
0,"In test/functional/tests.py, the TestFileComparison suite uses ""time.asctime("" to format dates in if-modified-since type headers. They should be in HTTP-date or ISO-86601 or whatever format is correct there."
1,"Some plugins, at least openvswitch and ml2, are not inheriting the test case for allowed address pairs.
This means that the bits related to address pairs use cases in these plugins are currently not covered by unit tests."
1,"When migrate vm on hyperV, command fails with the following error:
2013-10-25 03:35:40.299 12396 ERROR nova.openstack.common.rpc.amqp [req-b542e0fd-74f5-4e53-889c-48a3b44e2887 3a75a18c8b60480d9369b25ab06519b3 0d44e4afd3d448c6acf0089df2dc7658] Exception during message handling
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\amqp.py"", line 461, in _process_data
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\dispatcher.py"", line 172, in dispatch
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 90, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 73, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\compute\manager.py"", line 4103, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\driver.py"", line 118, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 44, in wrapper
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp return function(self, *args, **kwds)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 76, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp recover_method(context, instance_ref, dest, block_migration)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 69, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp dest)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 231, in live_migrate_vm
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp disk_paths = self._get_physical_disk_paths(vm_name)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 114, in _get_physical_disk_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp ide_paths = self._vmutils.get_controller_volume_paths(ide_ctrl_path)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\vmutils.py"", line 553, in get_controller_volume_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp ""parent"": controller_path})
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 1009, in query
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp return [ _wmi_object (obj, instance_of, fields) for obj in self._raw_query(wql) ]
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\win32com\client\util.py"", line 84, in next
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp return _get_good_object_(self._iter_.next(), resultCLSID = self.resultCLSID)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp com_error: (-2147217385, 'OLE error 0x80041017', None, None)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp"
1,"2013-10-17 06:00:23.066 ERROR nova.openstack.common.threadgroup [-] (u'A string is required here, not %s', 'list')
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
objects support now requied known the value you saved in the objects, but pci extra_info design for 'unkonw' usage. so can not get the type info.
refer the trace:
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup x.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return self.thread.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return self._exit_event.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return hubs.get_hub().switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return self.greenlet.switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup result = function(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/service.py"", line 65, in run_service
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup service.start()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup self.manager.pre_start_hook()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 801, in pre_start_hook
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup self.update_available_resource(nova.context.get_admin_context())
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 4868, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup rt.update_available_resource(context)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return f(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 292, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup 'pci_passthrough_devices')))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 189, in set_hvdevs
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup dev_obj = pci_device.PciDevice.create(dev)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/pci_device.py"", line 174, in create
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup pci_device.update_device(dev_dict)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/pci_device.py"", line 136, in update_device
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup self.extra_info = extra_info
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/base.py"", line 68, in setter
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup field.coerce(self, name, value))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/fields.py"", line 308, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup obj, '%s[""%s""]' % (attr, key), element)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/objects/fields.py"", line 214, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup value.__class__.__name__)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup ValueError: (u'A string is required here, not %s', 'list')"
0,"some of tests use different method of assertTrue(isinstance(A, B)) or assertEqual(type(A), B). The correct way is to use assertIsInstance(A, B) provided by testtools"
0,"Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues."
0,We should avoid to assign builtin function with other value. That case will lead calling type failure in other place.
1,"The vmwareapi test class that tests the VMwareESXDriver has a test_pause and test_unpause method that simply passes:
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L632
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L635
Those APIs aren't supported by the VMwareESXDriver so they should actually test that the code raises NotImplementedError.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L957
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L961
I think the test class was doing a pass for the esx driver because the test class for the vcdriver extends the esx driver test class, but the method isn't any different for the vcdriver test class so it should just assert it raises NotImplementedError either way."
0,"The l2-pop mechanism driver uses an ARP responder to avoid ARP broadcast but it only works with the LB agent.
We need to implement an ARP responder for the OVS agent also."
0,The Midonet plugin currently does not support associating floating IPs with ports at floating IP creation time. This bug is created to add this support.
0,"The test_cell_update_without_type_specified test fails sporadically:
======================================================================
2013-10-04 18:17:30.357 | FAIL: nova.tests.api.openstack.compute.contrib.test_cells.CellsTest.test_cell_update_without_type_specified
2013-10-04 18:17:30.358 | tags: worker-3
2013-10-04 18:17:30.358 | ----------------------------------------------------------------------
2013-10-04 18:17:30.358 | Empty attachments:
2013-10-04 18:17:30.359 | pythonlogging:''
2013-10-04 18:17:30.359 | stderr
2013-10-04 18:17:30.359 | stdout
2013-10-04 18:17:30.360 |
2013-10-04 18:17:30.360 | Traceback (most recent call last):
2013-10-04 18:17:30.360 | File ""nova/tests/api/openstack/compute/contrib/test_cells.py"", line 290, in test_cell_update_without_type_specified
2013-10-04 18:17:30.360 | self.assertEqual(cell['type'], 'parent')
2013-10-04 18:17:30.361 | File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-04 18:17:30.361 | self.assertThat(observed, matcher, message)
2013-10-04 18:17:30.361 | File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-04 18:17:30.362 | raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-04 18:17:30.362 | MismatchError: 'child' != 'parent'
2013-10-04 18:17:30.362 | ======================================================================
More info:
https://jenkins02.openstack.org/job/gate-nova-python26/6256/console
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIk1pc21hdGNoRXJyb3I6ICdjaGlsZCcgIT0gJ3BhcmVudCdcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiOTAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM4MDkxMTY5ODYzMH0="
1,"nova-api service will fail to start when the nova-api command is invoked from a directory containing a file with the same name as the one specified in the configuration key 'api_paste_config' if it is not an absolute path (the default is 'api-paste.ini').
[fedora@devstack1 devstack]$ pwd
/home/fedora/devstack
[fedora@devstack1 devstack]$ grep api_paste_conf /etc/nova/nova.conf
api_paste_config = api-paste.ini
[fedora@devstack1 devstack]$ touch api-paste.ini
[fedora@devstack1 devstack]$ nova-api
2013-12-09 09:18:40.082 DEBUG nova.wsgi [-] Loading app ec2 from api-paste.ini from (pid=4817) load_app /opt/stack/nova/nova/wsgi.py:485
2013-12-09 09:18:40.083 CRITICAL nova [-] Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova Traceback (most recent call last):
2013-12-09 09:18:40.083 TRACE nova File ""/usr/bin/nova-api"", line 10, in <module>
2013-12-09 09:18:40.083 TRACE nova sys.exit(main())
2013-12-09 09:18:40.083 TRACE nova File ""/opt/stack/nova/nova/cmd/api.py"", line 49, in main
2013-12-09 09:18:40.083 TRACE nova max_url_len=16384)
2013-12-09 09:18:40.083 TRACE nova File ""/opt/stack/nova/nova/service.py"", line 308, in __init__
2013-12-09 09:18:40.083 TRACE nova self.app = self.loader.load_app(name)
2013-12-09 09:18:40.083 TRACE nova File ""/opt/stack/nova/nova/wsgi.py"", line 486, in load_app
2013-12-09 09:18:40.083 TRACE nova return deploy.loadapp(""config:%s"" % self.config_path, name=name)
2013-12-09 09:18:40.083 TRACE nova File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2013-12-09 09:18:40.083 TRACE nova return loadobj(APP, uri, name=name, **kw)
2013-12-09 09:18:40.083 TRACE nova File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 271, in loadobj
2013-12-09 09:18:40.083 TRACE nova global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 296, in loadcontext
2013-12-09 09:18:40.083 TRACE nova global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 308, in _loadconfig
2013-12-09 09:18:40.083 TRACE nova ""argument given"" % uri)
2013-12-09 09:18:40.083 TRACE nova ValueError: Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova"
0,I'm writing additional functional tests which require sudo. There is also additional shared code from some existing agent tests which would be nice to use. It makes sense to refactor the functional tests to provide a base class which all these classes can inherit from.
0,Missing log.exception in nvp plugin for create_router
1,"IBM storwize_svc can not get the right host in following case:
Assume I have a SVC host using FC protocol. I configured 3 remote WWPNs for it, for example: rwwpn-1, rwwpn-2, rwwpn-3, and the host doesn't support iscsi so it has no iscsi_name property. When I try to attach a volume and do initialize_connection, I can get the host by only one of the 3 remote WWPNs, for example rwwpn-1, but I can not the host by the other two ones."
1,"I see that sometimes vmware driver reports 0 stats. Please take a look at the following log file for more information: http://162.209.83.206/logs/51404/6/screen-n-cpu.txt.gz
excerpts from log file:
2013-11-18 15:41:03.994 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for datastore Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for host Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for resourcePool Reason: None
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:390
2013-11-18 15:41:04.030 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: VCPU information unavailable _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:397
During this time we cannot spawn any server. Look at the http://162.209.83.206/logs/51404/6/screen-n-sch.txt.gz
excerpts from log file:
2013-11-18 15:41:52.475 DEBUG nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter AvailabilityZoneFilter returned 1 host(s) get_filtered_objects /opt/stack/nova/nova/filters.py:88
2013-11-18 15:41:52.476 DEBUG nova.scheduler.filters.ram_filter [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] (Ubuntu1204Server, domain-c26(c1)) ram:-576 disk:0 io_ops:0 instances:1 does not have 64 MB usable ram, it only has -576.0 MB usable ram. host_passes /opt/stack/nova/nova/scheduler/filters/ram_filter.py:60
2013-11-18 15:41:52.476 INFO nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter RamFilter returned 0 hosts
2013-11-18 15:41:52.477 WARNING nova.scheduler.driver [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] [instance: 1a648022-1783-4874-8b41-c3f4c89d8500] Setting instance to ERROR state."
0,"When live migrating a VM, nova-compute send the new port-binding info to neutron. But the host sent in the port-binding info is the previous host, not the host on which the VM is migrated.

I've attached a patch to resolv this, but I will ask for review as soon as I will hae time to investigate deeper."
0,"When creating a volume from a snapshot on Windows, a shadow copy volume is exported as an iSCSI disk. The issue is that this export is readonly and cannot be mounted to instances. As this export cannot be modified, to make the new volume usable, the solution would be to copy the image to a new path and import it as the desired volume."
1,"Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 182, in run_periodic_tasks
TRACE nova.openstack.common.periodic_task task(self, context)
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5049, in update_available_resource
TRACE nova.openstack.common.periodic_task rt.update_available_resource(context)
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
TRACE nova.openstack.common.periodic_task return f(*args, **kwargs)
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/compute/resource_tracker.py"", line 300, in update_available_resource
TRACE nova.openstack.common.periodic_task resources = self.driver.get_available_resource(self.nodename)
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3943, in get_available_resource
TRACE nova.openstack.common.periodic_task stats = self.host_state.get_host_stats(refresh=True)
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5016, in get_host_stats
TRACE nova.openstack.common.periodic_task self.update_status()
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5052, in update_status
TRACE nova.openstack.common.periodic_task data[""vcpus_used""] = self.driver.get_vcpu_used()
TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3626, in get_vcpu_used
TRACE nova.openstack.common.periodic_task total += len(vcpus[1])
TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task
http://logs.openstack.org/51/63551/6/gate/gate-tempest-dsvm-postgres-full/4860441/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-01-30_22_38_29_401
Seen in the gate
logstash query: message:""TypeError: 'NoneType' object has no attribute '__getitem__'"" AND filename:""logs/screen-n-cpu.txt"""
0,"__builtin__ does not exist in Python 3, use six.moves.builtins instead."
1,"nova cell showing should raise more information on v2 API, instead of ""The resource could not be found. """
1,"Error message in create_snapshot not appropriate while volume is migrating,as follows:
    msg = _(""Volume cannot be deleted while migrating"")"
1,"Former bugs:
  https://bugs.launchpad.net/ossa/+bug/1343604
  https://bugs.launchpad.net/ossa/+bug/1345233

The ssh_execute method is still affected in Cinder and Nova Icehouse release.
It is prone to password leak if:
- passwords are used on the command line
- execution fail
- calling code catch and log the exception

The missing fix from oslo-incubator to be merged is: 6a60f84258c2be3391541dbe02e30b8e836f6c22"
0,"http://lists.openstack.org/pipermail/openstack-dev/2013-November/018980.html

> Hi all,
>
> We had a discussion of the modules that are incubated in Oslo.
>
> https://etherpad.openstack.org/p/icehouse-oslo-status
>
> One of the conclusions we came to was to deprecate/remove uuidutils in
> this cycle.
>
> The first step into this change should be to remove generate_uuid() from
> uuidutils.
>
> The reason is that 1) generating the UUID string seems trivial enough to
> not need a function and 2) string representation of uuid4 is not what we
> want in all projects.
>
> To address this, a patch is now on gerrit.
> https://review.openstack.org/#/c/56152/
>
> Each project should directly use the standard uuid module or implement its
> own helper function to generate uuids if this patch gets in.
>
> Any thoughts on this change? Thanks.
>

Unfortunately it looks like that change went through before I caught up on
email. Shouldn't we have removed its use in the downstream projects (at
least integrated projects) before removing it from Oslo?

Doug"
0,We have a general desire though to stop returning / passing arbitrary dicts in the virt driver API - On this report we would like to create typed objects for consoles that will be used by drivers to return values on the compute manager.
0,"Cinder has many places where code manually constructs the same root_helper instead of using the
cinder.utils.get_root_helper to do the same thing."
1,"In nova/api/ec2/__init__.py there are codes like:
154 def __call__(self, req):
155 access_key = str(req.params['AWSAccessKeyId'])
156 failures_key = ""authfailures-%s"" % access_key
157 failures = int(self.mc.get(failures_key) or 0)
158 if failures >= CONF.lockout_attempts:
159 detail = _(""Too many failed authentications."")
160 raise webob.exc.HTTPForbidden(detail=detail)
But webob.exc.HTTPForbidden should use the 'explanation' parameter to show the error message.
The source can be referred to
https://github.com/Pylons/webob/blob/master/webob/exc.py#L666"
0,"http://lists.openstack.org/pipermail/openstack-dev/2014-January/023759.html
We've decided to rename oslo.sphinx to oslosphinx. This will require small changes in the doc builds for a lot of the other projects.
The problem seems to be when we pip install -e oslo.config on the system, then pip install oslo.sphinx in a venv. oslo.config is unavailable in the venv, apparently because the namespace package for o.s causes the egg-link for o.c to be ignored."
1,"If grizzly is deployed without using quantum-db-manage and letting neutron to create tables, there are not created tables servicedefinitions and servicetypes. These tables are dropped later when using LoadBalancerPlugin. When creating db scheme with quantum-db-manage, these tables are created and dropped correctly."
0,"Really trying to narrow this one down fully, and just putting this up because this is as far as I've gotten.
Basically, the lines in neutron/tests/base.py:
  line 159: self.addCleanup(CONF.reset)
  line 182: self.useFixture(self.messaging_conf)
cause cfg.CONF to get totally wiped out in the ""database"" config. I don't yet understand why this is the case.
if you then run any test that extends BaseTestCase, and then run neutron/tests/unit/test_db_plugin.py -> NeutronDbPluginV2AsMixinTestCase in the same process, these two tests fail:
Traceback (most recent call last):
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py"", line 3943, in setUp
    self.plugin = importutils.import_object(DB_PLUGIN_KLASS)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
    return import_class(import_str)(*args, **kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
I'm getting this error running tox on a subset of tests, however it's difficult to reproduce as the subprocesses have to work out just right.
To reproduce, just install nose and do:
.tox/py27/bin/nosetests -v neutron.tests.unit.test_db_plugin:DbModelTestCase neutron.tests.unit.test_db_plugin:NeutronDbPluginV2AsMixinTestCase
That is, DbModelTestCase is a harmless test but because it runs base.BaseTestCase first, cfg.CONF gets blown away.
I don't know what the solution should be here, cfg.CONF shouldn't be reset but I don't know what ""messaging_conffixture.ConfFixture"" is or how ""CONF.reset"" was supposed to work as it blows away DB config. The cfg.CONF in the first place seems to get set up via this path:
  <string>(7)exec2()
  /Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py(26)<module>()
-> from neutron.api import extensions
  /Users/classic/dev/redhat/openstack/neutron/neutron/api/extensions.py(31)<module>()
-> from neutron import manager
  /Users/classic/dev/redhat/openstack/neutron/neutron/manager.py(20)<module>()
-> from neutron.common import rpc as n_rpc
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/rpc.py(22)<module>()
-> from neutron import context
  /Users/classic/dev/redhat/openstack/neutron/neutron/context.py(26)<module>()
-> from neutron import policy
  /Users/classic/dev/redhat/openstack/neutron/neutron/policy.py(55)<module>()
-> cfg.CONF.import_opt('policy_file', 'neutron.common.config')
  /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/config/cfg.py(1764)import_opt()
-> __import__(module_str)
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/config.py(135)<module>()
-> max_overflow=20, pool_timeout=10)
> /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/options.py(145)set_defaults()
-> conf.register_opts(database_opts, group='database')
e.g. oslo.db set_defaults() sets it up."
1,"In the code to disassociate floating IPs in the BigSwitch plugin, it incorrectly updates the tenant's network on the backend controller rather than the external network."
1,"stopped one of swift service [s-object]
Take back up of any existing volume
observed exception in c-bak service logs however status of backup remains in 'creating' only
commands :
stop s-object service
ssatya@autojuno:~/juno/devstack$ cinder backup-create b7d8b000-d69c-4411-ab00-a3911c64ecdc --name test_backup2
+-----------+--------------------------------------+
| Property | Value |
+-----------+--------------------------------------+
| id | c19ecea3-a57c-49b4-94ed-22854a031f53 |
| name | test_backup2 |
| volume_id | b7d8b000-d69c-4411-ab00-a3911c64ecdc |
+-----------+--------------------------------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | None |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+------------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available | test | 1 | None | false | |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | backing-up | test | 1 | None | false | |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available | test | 1 | None | false | |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test | 1 | None | false | |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct 1 13:50:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct 1 13:55:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| ID | Volume ID | Status | Name | Size | Object Count | Container |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | creating | test_backup2 | 1 | None | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup | 1 | 2 | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+"
1,"When using the Ironic Nova driver, a stopped server is still presented as Running even when the server is stopped. Checking via the Ironic API correctly shows the instance as powered down:
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | ACTIVE | - | Running | private=10.1.0.10 |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ nova stop 5b43d631-91e1-4384-9b87-93283b3ae958
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | SHUTOFF | - | Running | private=10.1.0.10 |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ ping 10.1.0.10
PING 10.1.0.10 (10.1.0.10) 56(84) bytes of data.
From 172.24.4.2 icmp_seq=1 Destination Host Unreachable
From 172.24.4.2 icmp_seq=5 Destination Host Unreachable
From 172.24.4.2 icmp_seq=6 Destination Host Unreachable
From 172.24.4.2 icmp_seq=7 Destination Host Unreachable
From 172.24.4.2 icmp_seq=8 Destination Host Unreachable
--- 10.1.0.10 ping statistics ---
9 packets transmitted, 0 received, +5 errors, 100% packet loss, time 8000ms
stack@ironic:~/logs/screen$ ironic node-list
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| UUID | Instance UUID | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| 91e81c38-4dce-412b-8a1b-a914d28943e4 | 5b43d631-91e1-4384-9b87-93283b3ae958 | power off | active | False |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+"
0,"The method ' _create_port_for_pip' should try and check if a Neutron Port exists and reuse it.
If the Port is not found it should create a new Port."
1,"Current behavior of nuage plugin assumes a clean slate on the VSD (back-end controller) as part of the neutron startup. There are use-cases where this assumption is invalid. Also, there could exist non default net-partitions on the VSD for which behavior is not the same as for default net-partition. This needs to be fixed."
1,"In cases where Nova falls back to doing a local delete at the API (compute host down), it is possible for the delete.end notification to get emitted without deleted_at being set.

Steps to reproduce (with notifications enabled):
1) Create instance and wait for it to go active
2) Take compute host for the instance down
3) Delete instance
4) Observe compute.instance.delete.end notification with a blank deleted_at

This seems to be happening because the Instance.destroy() function does not update the instance with the latest state from the database after calling db.instance_destroy. It is important that the instance is updated with the data from the database as the driver automatically sets the deleted_at value."
1,"nova-network is currently broken due to direct DB access in dhcpbridge.

This issue was found using a multihost devstack setup where the non-controller node has an empty sql connection string.

"
0,"A great number of test files still uses json.loads() instead of using the wrapper provided by glance.openstack.common.jsonutils
All tests that use json directly should make the switch to the corresponding wrapper in glance.common.openstack.jsonutils"
0,The tests for the daemon loop in the ovs tunnel are not mocking out the polling call so they take 30+ seconds each.
1,"This can happen if a deployment is interrupted at just the wrong time.
2014-01-25 06:53:38,781.781 14556 DEBUG nova.compute.manager [req-e1958f79-b0c0-4c80-b284-85bb56f1541d None None] [instance: e21e6bca-b528-4922-9f59-7a1a6534ec8d] Current state is 1, state in DB is 1. _init_instance /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py:720
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/service.py"", line 172, in start
    self.manager.init_host()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 805, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 684, in _init_instance
    self.driver.plug_vifs(instance, net_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 538, in plug_vifs
    self._plug_vifs(instance, network_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 543, in _plug_vifs
    node = _get_baremetal_node_by_instance_uuid(instance['uuid'])
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 85, in _get_baremetal_node_by_instance_uuid
    node = db.bm_node_get_by_instance_uuid(ctx, instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/api.py"", line 101, in bm_node_get_by_instance_uuid
    instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 112, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/sqlalchemy/api.py"", line 152, in bm_node_get_by_instance_uuid
    raise exception.InstanceNotFound(instance_id=instance_uuid)
InstanceNotFound: Instance 84c6090b-bf42-4c6a-b2ff-afb22b5ff156 could not be found.
If there is no allocated node, we can just skip that part of delete."
1,"Here:
https://github.com/openstack/nova/blob/HEAD/nova/api/metadata/handler.py#L173
a constant time comparison should be used, more information on this type of attack here: http://codahale.com/a-lesson-in-timing-attacks/
An example constant time comparison in Python can be found here: https://github.com/django/django/blob/master/django/utils/crypto.py#L80 or via the PyCA cryptography library: https://cryptography.io/en/latest/hazmat/primitives/constant-time/"
0,"I have seen this failure several times. And it doesn't happen always, so it seems like a race condition issue. And until, I just saw it on py27, so not sure if it's existed in py26.
2014-01-23 15:12:33.910 | FAIL: glance.tests.unit.v2.test_registry_client.TestRegistryV2Client.test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.910 | ----------------------------------------------------------------------
2014-01-23 15:12:33.911 | _StringException: Traceback (most recent call last):
2014-01-23 15:12:33.911 | File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_registry_client.py"", line 268, in test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.911 | unjsonify=False)
2014-01-23 15:12:33.911 | File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/utils.py"", line 472, in assertEqualImages
2014-01-23 15:12:33.911 | self.assertEqual(images[i]['id'], value)
2014-01-23 15:12:33.911 | File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 324, in assertEqual
2014-01-23 15:12:33.911 | self.assertThat(observed, matcher, message)
2014-01-23 15:12:33.911 | File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 414, in assertThat
2014-01-23 15:12:33.911 | raise MismatchError(matchee, matcher, mismatch, verbose)
2014-01-23 15:12:33.911 | MismatchError: !=:
2014-01-23 15:12:33.911 | reference = u'db4ddeb5-edd6-4557-b635-6ecb4e5265a0'
2014-01-23 15:12:33.912 | actual = '406c995a-70e0-4010-a6eb-9dff61a2d2a7'
http://logs.openstack.org/19/67019/2/check/gate-glance-python27/783d216/console.html"
0,"As part of the changes to support multiple tunnel_types [1] and deprecate enable_tunneling [2] in the agent configuration, it was decided during the review to program tunnels using port endpoints instead of using tunnel IDs. Once the OVS agent can concurrently support both GRE and VXLAN tunnel types in the context of ML2, this becomes relevant and will prevent tunnels sharing the same tunnel ID from polluting their BUM traffic between each other.
[1] https://review.openstack.org/#/c/33107/16
[2] https://review.openstack.org/#/c/34779/"
1,"the libvirt driver was running into this issue https://bitbucket.org/eventlet/eventlet/issue/137/use-of-threading-locks-causes-deadlock when trying to connect to the libvirtd daemon.
the driver has logging messages during the _connect method which was called within a native thread. The logging code takes out a eventlet lock and mixing OS native threads and eventlet locks causes deadlocks due to the above eventlet bug. The resulting deadlock was enough to hang the nova-compute process."
1,"When using VMwareVC nova driver and VMwareVcVMDK cinder driver, booting from volume via the Horizon UI fails. The instance boots with ERROR status and the log shows ""Image could not be found"". In addition, the user is unable to access the instances index page in Horizon due to an error 500 (other pages work, however). Steps to reproduce:
(Using horizon)
1. Create a volume from an image
2. Boot an instance from the volume
Expected result:
1. An instance is booted from the volume successfully
2. User is redirected to the instances index page in Horizon
Actual result:
1. Instance fails to boot with status ERROR
2. User is redirected to instances index page but page fails with 500 error. In debug mode, user sees TypeError at /project/instances: string indices must be integers (see link to trace below)
Nova log error:
 Traceback (most recent call last):
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1037, in _build_instance
     set_access_ip=set_access_ip)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
     LOG.exception(_('Instance failed to spawn'), instance=instance)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _spawn
     block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 623, in spawn
     admin_password, network_info, block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 208, in spawn
     disk_type, vif_model, image_linked_clone) = _get_image_properties()
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 187, in
     instance)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 184, in
     meta_data = image_service.show(context, image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 290, in show
     _reraise_translated_image_exception(image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 288, in show
     image = self._client.call(context, 1, 'get', image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 212, in call
     return getattr(client.images, method)(*args, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 114, in
     % urllib.quote(str(image_id)))
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 272,
     return self._http_request(url, method, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 233,
     raise exc.from_response(resp, body_str)
 ImageNotFound: Image could not be found.
Horizon error:
    Request Method: GET
    Request URL: http://10.20.72.218/project/instances/
    Django Version: 1.5.4
    Exception Type: TypeError
    Exception Value:
    string indices must be integers
    Exception Location: /opt/stack/horizon/openstack_dashboard/wsgi/../../openstack_dashboard/ dashboards/project/instances/views.py in get_data, line 92
    Python Executable: /usr/bin/python
    Python Version: 2.7.3"
1,"Currently the extend method in the cinder volume_actions extension checks for KeyError and ValueError, however the negative tests in Tempest include passing None in to the new_size which results in an unhandled trace in the test logs.
Add TypeError to the list of exceptions we look for."
1,"In method downgrade_cisco in migration folsom_initial instead of local method drop_tables op.drop_tables is used
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 468, in downgrade
    downgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 541, in downgrade_cisco
    op.drop_tables(
AttributeError: 'module' object has no attribute 'drop_tables'"
0,"Removed the exception classes in the following because not referenced
anywhere.
-MissingArgumentError
-NotAuthorized"
0,OpenFlow controller which nec plugin talks to sometimes returns retry-after when it is busy. It is better to honor retry-after header to avoid unnecessary user-visible errors due to temporary busy condition.
0,"Remove unused code in test_attach_interfaces.py:
body = jsonutils.dumps({'port_id': FAKE_PORT_ID1}) is unused in test_attach_interface_without_network_id().
We should remove it."
1,"after commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 my users are unable to boot vm attached to provider networks, this is a serious regression for me as we mostly use provider networks.

bug which originated the commit https://bugs.launchpad.net/ubuntu/+source/nova/+bug/1284718"
1,"unshelve and resize instance (created by bootable volume) unnecessarily logs 鈥榠mage not found鈥?error/warning messages
In both the cases, it logs following misleading error/warning messages in the compute.log when image_id_or_uri is passed as None to nova/compute/utils->get_image_metadata method.
14-09-05 03:41:54.834 ERROR glanceclient.common.http [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin] Request returned failure status 404.
14-09-05 03:41:54.834 WARNING nova.compute.utils [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin] [instance: d5b137ab-19a1-484a-a828-6a229ec66950] Can't access image : Image could not be found."
1,This issue is related to different values that MSVM_ComputerSystem's Caption property can have on different locales.
1,"When running the Tempest tests in parallel, 2 tests are failing due to instances spawning with ERROR:
  setUpClass (tempest.api.compute.admin.test_fixed_ips.FixedIPsTestXml)
  setUpClass (tempest.api.compute.admin.test_fixed_ips_negative.FixedIPsNegativeTestJson)
The VimFaultException seen in the nova scheduler log is:
  Cannot complete the operation because the file or folder [datastore1] vmware_base already exists
Full Traceback here (non-wrapped version here: http://paste.openstack.org/show/60502/):
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1053, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1456, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1453, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 587, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 437, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1549, in _check_if_folder_file_exists
    ds_ref)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1534, in _mkdir
    createParentDirectories=False)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 795, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 200, in vim_request_handler
    raise error_util.VimFaultException(fault_list, excep)
VimFaultException: Server raised fault: 'Cannot complete the operation because the file or folder [datastore1] vmware_base already exists'"
1,"Corner case with networks without a subnet leads to a spurious exception during net-migration, because the validation logic is called twice: before migration and after."
1,"Client side of the network RPC API sets 'requested_networks' parameter in deallocate_for_instance() call [1]
while server side expects 'fixed_ips' parameter [2]
[1] https://github.com/openstack/nova/blob/master/nova/network/rpcapi.py#L183
[2] https://github.com/openstack/nova/blob/master/nova/network/manager.py#L555"
1,"In miration e197124d4b9_add_unique_constrain mistake in usage drop_constraint parameter type_ and positional agruments name
and table_name.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/e197124d4b9_add_unique_constrain.py"", line 64, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)
The same mistake was already fixed in miration 63afba73813_ovs_tunnelendpoints_id_unique."
1,"Version: Havana w/ Ubuntu Repos. with Ceph for RBD.
When creating Launching a instance with ""Boot from image (Creates a new volume)"" this creates the instance fine and all is well however if you shutdown the instance I can't turn it back on again.
I get the following error in the nova-compute.log when trying to power on an shutdown instance.
#######################################################################################
2013-10-29 00:48:33.859 2746 WARNING nova.compute.utils [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] [instance: cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09] Can't access image $
2013-10-29 00:48:34.040 2746 WARNING nova.virt.libvirt.vif [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Deprecated: The LibvirtHybridOVSBridgeDriver VIF driver is now de$
2013-10-29 00:48:34.578 2746 ERROR nova.openstack.common.rpc.amqp [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Exception during message handling
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp pass
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1832, in start_instance
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp self._power_on(context, instance)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1819, in _power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1948, in power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp self._hard_reboot(context, instance, network_info, block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1903, in _hard_reboot
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4318, in get_instance_disk_info
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp dk_size = int(os.path.getsize(path))
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/genericpath.py"", line 49, in getsize
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp return os.stat(filename).st_size
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp OSError: [Errno 2] No such file or directory: '/var/lib/nova/instances/cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09/disk'
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp
#######################################################################################
On Closer inspection It seems the libvirt.xml file for the instance gets all screwed up.
This is the libvirt.xml file for this instance before shutdown.
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""network"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source protocol=""rbd"" name=""volumes/volume-5abadeb8-49e9-4628-a54d-d742d6f2e012"">
        <host name=""10.100.96.10"" port=""6789""/>
        <host name=""10.100.96.11"" port=""6789""/>
        <host name=""10.100.96.12"" port=""6789""/>
      </source>
      <auth username=""volumes"">
        <secret type=""ceph"" uuid=""13a673af-ff80-3036-8310-4c72f566673d""/>
      </auth>
      <target bus=""virtio"" dev=""vda""/>
      <serial>5abadeb8-49e9-4628-a54d-d742d6f2e012</serial>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################
All looks fine here i can see the ceph mons etc.
Next i'll shutdown the instance and as expected the file stays the same now this is the file if i try to power the instance back on again.
you'll notice all of the details regarding the disk have changed and now it thinks its a qcow2 disk located on the local hard drive... how does this happen?
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""file"" device=""disk"">
      <driver name=""qemu"" type=""qcow2"" cache=""none""/>
      <source file=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################"
1,"The Hyper-V agent is currently obtaining aggregated metrics instead of values for each individual port.
This leads to erroneous values in case instances have multiple ports."
1,"When an update for a dhcp port fails, a keyerror on missing network_id while attempting to log a trace masks the underlying exception. An example is here:

http://logs.openstack.org/91/94891/1/check/check-tempest-dsvm-neutron/c87b4fc/logs/screen-q-svc.txt.gz?level=TRACE

This is because on update, the dhcp agent only sends either this:

https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L770

or this:

https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L787

This is somewhat a corner case, but it would be good to address the issue to see what actually went wrong."
0,"These two tests in neutron trunk are failing on my 64-bit xubuntu 12.04 using python 2.7:
http://paste.openstack.org/show/47323/
When I run the NECPluginV2._validate_portinfo method in python, I get this:
mriedem@ubuntu:~$ python
Python 2.7.3 (default, Aug 1 2012, 05:16:07)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> datapath_id = '0x1234567890abcdef'
>>> dpid = int(datapath_id, 16)
>>> dpid
1311768467294899695L
>>> dpid > 0xffffffffffffffffL
False
>>> hex(dpid)
'0x1234567890abcdefL'
>>>
So the int(datapath_id, 16) conversion is returning it as a longinteger, so the 'L' is appended at the end.
http://docs.python.org/2/library/functions.html#int
""If the argument is outside the integer range, the function returns a long object instead.""
http://docs.python.org/2/reference/lexical_analysis.html#integers
I'm able to fix this in the tests by simply using assertIn rather than assertEqual:
self.assertIn('0x1234567890abcdef', portinfo['datapath_id'])
I'm not sure if this is a bug in the tests though or something that should be handled in the _validate_portinfo method, i.e. if you get past all of the current validation OK, then simply return the input argument in the dict that goes back."
1,"Thin-provisioned LVs may not be activated automatically in all cases. (RHEL 6.5 does not activate them automatically by default.)

This causes the volume clone from snapshot dd operation to fail, as the source device does not exist in /dev/mapper/. We should ensure the LV is active before attempting to clone to a new volume."
0,"DVR implementation assumes that dvr_vmarp_table_update() method is supported by L3 Router service plugin. With the vendor's version of L3 Router Service plugins that assumption may not be true and hence the invocation of this method throws an exception.

I noticed this during the testing of Arista's L3 router plugin. I notice that other similar plugins from other vendors are on their way, and will hit this issue as well."
1,"The consistency watchdog for the BigSwitch plugin server manager currently does not work. It incorrectly calls rest_call on a servers list rather than calling it on the server pool object.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L554"
0,"If specifying false string (""False"") as ""block_migration"" parameter of migrate_live API like the following, nova considers it as True.
$ curl -i 'http://10.21.42.81:8774/v2/[..]/servers/[..]/action' -X POST [..] -d '{""os-migrateLive"": {""disk_over_commit"": ""False"", ""block_migration"": ""False"", ""host"": ""localhost""}}'
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency."
0,"$ grep 'LOG.warn([^_]' * -r

$ grep 'LOG.warn([^_]' * -r
nova/virt/xenapi/vm_utils.py: LOG.warn(msg % e)
nova/virt/libvirt/driver.py: LOG.warn(msg)
nova/virt/libvirt/volume.py: LOG.warn(msg)
nova/virt/libvirt/volume.py: LOG.warn(msg)
nova/virt/libvirt/volume.py: LOG.warn(msg)
nova/virt/libvirt/volume.py: LOG.warn(msg)
nova/tests/scheduler/test_host_manager.py: host_manager.LOG.warn(""No service for compute ID 5"")
nova/tests/image/fake.py: LOG.warn('Unable to find image id %s. Have images: %s',
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/api.py: LOG.warn(msg)
nova/db/sqlalchemy/migrate_repo/versions/186_new_bdm_format.py: LOG.warn(""Got an unexpected block device %s""
nova/openstack/common/db/sqlalchemy/session.py: LOG.warn(msg % remaining)
nova/openstack/common/rpc/impl_zmq.py: LOG.warn(emsg)
nova/compute/api.py: LOG.warn(msg)
nova/compute/api.py: LOG.warn(msg)
nova/compute/api.py: LOG.warn(msg)
nova/compute/manager.py: LOG.warn(err_str % exc, instance=instance)
nova/compute/manager.py: LOG.warn(e, instance=instance)
nova/network/linux_net.py: LOG.warn(msg % {'num': num_rules, 'float': floating_ip})
nova/network/manager.py: LOG.warn(oversize_msg)
nova/api/openstack/compute/plugins/v3/quota_sets.py: LOG.warn(msg)
nova/api/openstack/compute/contrib/quotas.py: LOG.warn(msg)
nova/api/metadata/vendordata_json.py: LOG.warn(logprefix + _(""file does not exist""))
nova/api/metadata/vendordata_json.py: LOG.warn(logprefix + _(""Unexpected IOError when reading""))
nova/api/metadata/vendordata_json.py: LOG.warn(logprefix + _(""failed to load json""))

They shoud make use of ""_"" defined in nova/openstack/common/gettextutils.py, like this:

  LOG.warn(_(""parent device '%s' not found""), dev)"
0,"when using centos or redhat 6.5, it has mysql 5.1, with default engine of myISAM. Openstack projects require InnoDB. I see some code, neutron/db/model_base.py#L25, that looks to try to set the engine type for create tables. But after an install, tables are instead created with default engine myISAM.
Where seeing this on neutron and ceilometer."
1,"If I create more than one VM at a time (with nova boot option --num-instances), sometimes the flooding flow for broadcast, multicast and unknown unicast are not added on certain l2 agents.
And conversely, when I delete more than one VM at a time, the flooding rule are not purge on certain l2 agents when it's necessary.
I made this test on the trunk version with OVS agent."
1,"Create a volume with volume type mapped to gold profile. [gold profile mapped to datastore1]
Now attach to instance and observe that volume is placed in datastore1.
Now detach above volume .
Now change volume type of above volume to Silver profile [Silver profile mapped to datastore2]
Now re attach volume to instance and observe that volume is still placed in datastore1 instead of datastore2."
0,"The following failure is happening sporadically when running the unit tests:
ft1.11275: neutron.tests.unit.test_linux_ip_lib.TestDeviceExists.test_device_exists_StringException: Empty attachments:
  pythonlogging:''
  pythonlogging:'neutron.api.extensions'
  stderr
  stdout
Traceback (most recent call last):
  File ""neutron/tests/unit/test_linux_ip_lib.py"", line 785, in test_device_exists
    _execute.assert_called_once_with('o', 'link', ('show', 'eth0'))
  File ""/home/jenkins/workspace/gate-neutron-python27/.tox/py27/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 0 times."
1,"I noticed that floating IPs would be removed momentarily from the router on agent restart. This would cause a momentarily outage in network connectivity. This should be avoided.
I noticed this when testing with this change: https://review.openstack.org/#/c/30988/ which prevents routers from being destroyed and re-added."
1,"Image status change ""active"" to ""saving"" when duplicate upload image to glance,details are as follows:
create image:
curl -i -X POST -H ""Content-Type:application/json"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -d '{""name"":""test_img"",""container_format"":""bare"",""disk_format"":""qcow2"",""visibility"":""public""}' http://127.0.0.1:9292/v2/images
upload image:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./test.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file
duplicate upload:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./xx.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file"
1,"As the comment in https://review.openstack.org/#/c/70901/6/nova/api/openstack/compute/plugins/v3/pause_server.py
After apply instance look helper, some compute api will invoke instance.save(), it will raise InstanceNotFound also. We should catch them."
0,"Seen here:
http://logs.openstack.org/84/67584/5/gate/gate-swift-python27/cf6c70e/console.html
Just need to evaluate and track."
1,"MetaInterfaceDriver communicates with neutron-server using REST API.
If a user intend to use internalurl for neutron-server endpoint, MataInterfaceDriver fails.
This is because MetaInterfaceDriver does not specify endpoint_type. Thus it assumes using publicurl.
---
class MetaInterfaceDriver(LinuxInterfaceDriver):
    def __init__(self, conf):
        super(MetaInterfaceDriver, self).__init__(conf)
        from neutronclient.v2_0 import client
        self.neutron = client.Client(
            username=self.conf.admin_user,
            password=self.conf.admin_password,
            tenant_name=self.conf.admin_tenant_name,
            auth_url=self.conf.auth_url,
            auth_strategy=self.conf.auth_strategy,
            region_name=self.conf.auth_region
        )
---
Note that MetaInterfaceDriver is used with Metaplugin only."
1,"I am using stable havana nova.
I got this exception while I delete my kvm instance, but the qemu process of this instance become to 'defunct' status by some unknown reason(may be a qemu/kvm bug), and then the periodic task stopped unexpectly everytime, then the resources of this compute node will never be reported, because of this exception below, I think we should handle this exception while running periodic task.
2014-01-16 15:53:28.421 47954 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task task(self, context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 5617, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task rt.update_available_resource(context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task return f(*args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 281, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task resources = self.driver.get_available_resource(self.nodename)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4275, in get_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task stats = self.host_state.get_host_stats(refresh=True)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5350, in get_host_stats
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task self.update_status()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5386, in update_status
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task data[""vcpus_used""] = self.driver.get_vcpu_used()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3949, in get_vcpu_used
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task vcpus = dom.vcpus()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task rv = execute(f,*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task rv = meth(*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 2222, in vcpus
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task if ret == -1: raise libvirtError ('virDomainGetVcpus() failed', dom=self)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task libvirtError: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task
and the exception while I delete this instance:
2014-01-16 15:13:26.640 47954 ERROR nova.openstack.common.rpc.amqp [req-03ed9463-0740-4423-bf1e-2334ed29ee5c 9537af4d80e546409b670673f9a81388 3179fc9d69d747b4a06f27a6d2334050] Exception during message handling
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp **args)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 400, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp payload)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 290, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp pass
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 276, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 341, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 318, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 305, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2128, in terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp do_terminate_instance(instance, bdms, clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp return f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2120, in do_terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp reservations=reservations)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/hooks.py"", line 105, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp rv = f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2091, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp user_id=user_id)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2063, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1984, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp requested_networks)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1974, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp context=context, clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 883, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp self._destroy(instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 838, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp instance=instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 810, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp virt_dom.destroy()
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp rv = execute(f,*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp rv = meth(*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 760, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp libvirtError: Failed to terminate process 61945 with SIGKILL: Device or resource busy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp"
1,"An example of a failure has been observed here:
http://logs.openstack.org/80/113580/3/experimental/check-tempest-dsvm-neutron-dvr/a0e0c32/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-13_00_13_00_674
More triaging needed but I suspect this is caused by interleaved create/delete requests of router resources."
1,"when the thin lvm pool needs to be created, it is incorrectly sized
a 64G VG results in a 64M POOL
the offending code seems to be here: https://github.com/openstack/cinder/blob/d951222c1440b31f5e994d43b4277344e0d8f63f/cinder/brick/local_dev/lvm.py#L333"
1,"This patch verifies Floating IP status is updated correctly according to blue print.
https://review.openstack.org/#/c/102700/
https://blueprints.launchpad.net/neutron/+spec/fip-op-status
VMWWare-Minesweeper fails consistently. The Jenkins gates pass ok
Please check"
1,"3PAR Snapshot stuck in Error_Deleting status
Scenario
1. Create a volume vol1
2. Snapshot the volume vol1_snap
3. Create volume from snapshot vol1_snap_to_vol2
4. Try to delete snapshot
Talked to walt, we should be able to recover from this and return status to availalbe after sometime, instead of always being stuck in Error_Deleting status
2013-11-11 15:18:29.135 ERROR cinder.openstack.common.rpc.amqp [req-c8a80e4e-3679-42c6-919e-f82ab06a0970 0d4a18c85ff447d7b0dce7017793ce9c 0982d8dd597b40fb96b67e199137b06d] Exception during message handling
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp **args)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 820, in wrapper
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 425, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp {'status': 'error_deleting'})
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 413, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp self.driver.delete_snapshot(snapshot_ref)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 154, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp self.common.delete_snapshot(snapshot)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 982, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp self.client.deleteVolume(snap_name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/client.py"", line 216, in deleteVolume
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp response, body = self.http.delete('/volumes/%s' % name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 327, in delete
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp return self._cs_request(url, 'DELETE', **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 231, in _cs_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 205, in _time_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp resp, body = self.request(url, method, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 199, in request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp raise exceptions.from_response(resp, body)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 32 - volume has a child
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp"
1,"Running latest devstack on Trusty Ubuntu, the Neutron agent fails to start with the following backtrace:
2014-04-01 13:49:54.227 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['uname', '-r'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.232 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['uname', '-r']
Exit code: 0
Stdout: '3.13.0-20-generic\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.233 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.348 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version']
Exit code: 0
Stdout: 'ovs-vsctl (Open vSwitch) 2.0.1\nCompiled Feb 23 2014 14:42:32\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.349 DEBUG neutron.agent.linux.ovs_lib [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Checking OVS version for VXLAN support installed klm version is None, installed Linux version is ['3.13.0'], installed user version is 2.0 from (pid=14900) check_ovs_vxlan_version /opt/stack/neutron/neutron/agent/linux/ovs_lib.py:541
2014-04-01 13:49:54.350 CRITICAL neutron [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron Traceback (most recent call last):
2014-04-01 13:49:54.350 TRACE neutron File ""/usr/local/bin/neutron-openvswitch-agent"", line 10, in <module>
2014-04-01 13:49:54.350 TRACE neutron sys.exit(main())
2014-04-01 13:49:54.350 TRACE neutron File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1360, in main
2014-04-01 13:49:54.350 TRACE neutron agent = OVSNeutronAgent(**agent_config)
2014-04-01 13:49:54.350 TRACE neutron File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 214, in __init__
2014-04-01 13:49:54.350 TRACE neutron self._check_ovs_version()
2014-04-01 13:49:54.350 TRACE neutron File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 232, in _check_ovs_version
2014-04-01 13:49:54.350 TRACE neutron ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-04-01 13:49:54.350 TRACE neutron File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 550, in check_ovs_vxlan_version
2014-04-01 13:49:54.350 TRACE neutron 'kernel', 'VXLAN')
2014-04-01 13:49:54.350 TRACE neutron File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 507, in _compare_installed_and_required_version
2014-04-01 13:49:54.350 TRACE neutron installed_kernel_version) >= dist_version.StrictVersion(
2014-04-01 13:49:54.350 TRACE neutron File ""/usr/lib/python2.7/distutils/version.py"", line 40, in __init__
2014-04-01 13:49:54.350 TRACE neutron self.parse(vstring)
2014-04-01 13:49:54.350 TRACE neutron File ""/usr/lib/python2.7/distutils/version.py"", line 107, in parse
2014-04-01 13:49:54.350 TRACE neutron raise ValueError, ""invalid version number '%s'"" % vstring
2014-04-01 13:49:54.350 TRACE neutron ValueError: invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron
q-agt failed to start
This is due to bug #1291535 and this fix: https://git.openstack.org/cgit/openstack/neutron/commit/?id=b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c"
1,"http://logs.openstack.org/52/73152/8/check/check-tempest-dsvm-full/9352c04/console.html.gz#_2014-05-13_18_12_38_547
At client side the only way to know an instance action is doable is to making sure the status is a permanent status like ACTIVE or SHUTOFF and no action in progress, so the task-state is None.
In the above linked case tempest stopped the instance and the instance reached the ""SHUTOFF/None"".
'State transition ""ACTIVE/powering-off"" ==> ""SHUTOFF/None"" after 10 second wait'
Cool, at this time we can start the instance right ? No, other attribute needs to be checked.
The start attempt was rewarded with 409 :
 u'Instance 7bc9de3b-1960-476f-b964-2ab2da986ec7 in task_state powering-off. Cannot start while the instance is in this state'
The below line indicates the task state, silently moved back to ""SHUTOFF/powering-off"" , before the 'start' attempt.
2014-05-13 18:09:13,610 State transition ""SHUTOFF/powering-off"" ==> ""SHUTOFF/None"" after 1 second wait
Please do not set the 'None' task-state when the 'powering-off' is not finished."
0,"Deleting a network which has no ports and no subnets on it produces the following stack trace in neutron-server logs when using PostgreSql:
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 526, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource super(OVSNeutronPluginV2, self).delete_network(context, id)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 999, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource for p in ports)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2227, in __iter__
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource return self._execute_and_instances(context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2242, in _execute_and_instances
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource result = conn.execute(querycontext.statement, self._params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1584, in _execute_clauseelement
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource compiled_sql, distilled_params
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource cursor.execute(statement, parameters)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource NotSupportedError: (NotSupportedError) SELECT FOR UPDATE/SHARE cannot be applied to the nullable side of an outer join
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource 'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner, ipallocations_1.port_id AS ipallocations_1_port_id, ipallocations_1.ip_address AS ipallocations_1_ip_address, ipallocations_1.subnet_id AS ipallocations_1_subnet_id, ipallocations_1.network_id AS ipallocations_1_network_id, allowedaddresspairs_1.port_id AS allowedaddresspairs_1_port_id, allowedaddresspairs_1.mac_address AS allowedaddresspairs_1_mac_address, allowedaddresspairs_1.ip_address AS allowedaddresspairs_1_ip_address, extradhcpopts_1.id AS extradhcpopts_1_id, extradhcpopts_1.port_id AS extradhcpopts_1_port_id, extradhcpopts_1.opt_name AS extradhcpopts_1_opt_name, extradhcpopts_1.opt_value AS extradhcpopts_1_opt_value, portbindingports_1.port_id AS portbindingports_1_port_id, portbindingports_1.host AS portbindingports_1_host, securitygroupportbindings_1.port_id AS securitygroupportbindings_1_port_id, securitygroupportbindings_1.security_group_id AS securitygroupportbindings_1_security_group_id \nFROM ports LEFT OUTER JOIN portbindingports ON ports.id = portbindingports.port_id LEFT OUTER JOIN ipallocations AS ipallocations_1 ON ports.id = ipallocations_1.port_id LEFT OUTER JOIN allowedaddresspairs AS allowedaddresspairs_1 ON ports.id = allowedaddresspairs_1.port_id LEFT OUTER JOIN extradhcpopts AS extradhcpopts_1 ON ports.id = extradhcpopts_1.port_id LEFT OUTER JOIN portbindingports AS portbindingports_1 ON ports.id = portbindingports_1.port_id LEFT OUTER JOIN securitygroupportbindings AS securitygroupportbindings_1 ON ports.id = securitygroupportbindings_1.port_id \nWHERE ports.tenant_id = %(tenant_id_1)s AND ports.network_id IN (%(network_id_1)s) FOR UPDATE' {'network_id_1': u'f8344378-ed58-4ce3-99ae-408c8591cda9', 'tenant_id_1': u'df3896201a174787b65b2b098aad2968'}
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource
2013-12-17 12:43:49.920 7347 INFO neutron.wsgi [req-b8b4b523-cf50-4bcb-8356-19bbc4b9d057 8549dfbbb00940b9b3659230186bd26d df3896201a174787b65b2b098aad2968] 192.168.1.51 - - [17/Dec/2013 12:43:49] ""DELETE /v2.0/networks/f8344378-ed58-4ce3-99ae-408c8591cda9.json HTTP/1.1"" 500 230 0.063668"
0,"FAIL: glance.tests.functional.db.test_registry.TestRegistryDriver.test_image_paginate
tags: worker-0
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""glance/tests/functional/db/base.py"", line 711, in test_image_paginate
    self.assertEqual(extra_uuids, [i['id'] for i in page])
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = ['0be9ce54-f56b-41d5-bef6-9ca75e846b59',
 'a75dc71b-df55-41e3-9218-f465b791b7da']
actual = [u'a75dc71b-df55-41e3-9218-f465b791b7da',
 u'0be9ce54-f56b-41d5-bef6-9ca75e846b59']
http://logs.openstack.org/58/74858/1/check/gate-glance-python27/732468f/console.html#_2014-02-19_23_03_01_798"
1,"reproduce steps:
1. create an instance under Folsom
2. update nova to Havana
3. resize the instance to another host
4. confirm the resize
5. examine the instance dir on source host
you will find the instance-0000xxxx_resize dir exists there which was not deleted while confirming resize.
the reason is that:
in the _cleanup_resize in libvirt driver:
def _cleanup_resize(self, instance, network_info):
        target = libvirt_utils.get_instance_path(instance) + ""_resize""
we get the instance path by using get_instance_path method in libvirt utils,
but we check the original instance dir of pre-grizzly instances' before we return it,
if this instance is a resized one which original instance dir exists on another host(the dest host),
the wrong instance path with uuid will be returned, and then the `target` existing check will be failed,
then the instance-xxxx_resize dir will never be deleted.
def get_instance_path(instance, forceold=False, relative=False):
    """"""Determine the correct path for instance storage.
    This method determines the directory name for instance storage, while
    handling the fact that we changed the naming style to something more
    unique in the grizzly release.
    :param instance: the instance we want a path for
    :param forceold: force the use of the pre-grizzly format
    :param relative: if True, just the relative path is returned
    :returns: a path to store information about that instance
    """"""
    pre_grizzly_name = os.path.join(CONF.instances_path, instance['name'])
    if forceold or os.path.exists(pre_grizzly_name): ############### here we check the original instance dir, but if we have resized the instance to another host, this check will be failed, and a wrong dir with instance uuid will be returned.
        if relative:
            return instance['name']
        return pre_grizzly_name
    if relative:
        return instance['uuid']
    return os.path.join(CONF.instances_path, instance['uuid'])"
0,Add initiator target map in initialize_connection and terminate_connection of the EMC SMI-S FC driver as it is required by the FC zone manager.
1,"1) create profile say 'default' having content tagged to nfs data store.
2) Now 'default' profile in /etc/cinder/cinder.conf for ""pbm_default_policy=default""
3) now restart cinder api service.
4) Now create volume with out any volume type ""cinder create --name testvol 1""
5) Now attach to instance.
6) As per default profile define respective volume is present to nfs data store.
how ever default profile details not appearing in "" VM storage policies pane"" . See attached snapshot for reference
ssatya@devstack:/etc/cinder$ cat cinder.conf | grep pbm
pbm_default_policy=default
ssatya@devstack:/etc/cinder$ cinder list | grep in-use
| 4db6af03-7c60-469d-803e-2ba21893fc0d | in-use | testvol2 | 1 | None | false | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| 862219d5-941a-4f96-85a8-9b73a83d4507 | in-use | testvol3 | 1 | None | false | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| d33d102f-f863-441d-bf87-29465cef3760 | in-use | ThickSliver_vol1 | 1 | ThickSliver_volume | false | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |"
1,"The fix for bug 1333654 ensured events for instance without host are not accepted.
However, the instances without the host are still being passed to the compute API layer.
This is likely to result in keyerrors as the one found here: http://logs.openstack.org/51/109451/2/check/check-tempest-dsvm-neutron-full/ad70f74/logs/screen-n-api.txt.gz#_2014-07-25_01_41_48_068
The fix for this bug should be straightforward."
1,"curl -i 'http://cloudcontroller:8774/v2/fdbb1e8f23eb40c89f3a677e2621b95c/servers/e2461ba7-3624-4d43-a456-acb87a0fb6f9/action' -X POST -H ""X-Auth-Project-Id: admin"" -H ""User-Agent: python-novaclient"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""X-Auth-Token: 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-2CM0-JKKjNf-NG9X3NpXPDt1mvaP1brXvzoq0W9IhxvE5THBLyrVrzEk5s+cZlDfo6QqyPtRqGgs80WNdOe3UQ8pL14E+SPoc3QIv66G6on6wOL9JK7KAtVbcfE9ucCgZmLI9hQGzn7J5GyXHzP0dRNRxEw+39P4pKOdTLj7dIQA7-PGEHdajFkMVWIIkD--c+G8pojKzZjESrGWpJh8wzmH8Awkcr5qEvvkEqEXqHKsf4ILdX2d90C4nfYYRDYpggVaVn5H8eK2z4dYkuUdRAf03c7kUzE9n12PabhSxssFNtdF-Dm8VSM0eF1BkBYByA"" -d '{""createImage"": {""name"": ""snapx2"", ""metadata"": {""a"": ""b""}}}'
os@os2:~/devstack$ glance image-show 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59
+---------------------------------+----------------------------------------------------------------------------------+
| Property | Value |
+---------------------------------+----------------------------------------------------------------------------------+
| Property 'bdm_v2' | True |
| Property 'block_device_mapping' | [{""guest_format"": null, ""boot_index"": 0, ""no_device"": null, ""snapshot_id"": |
| | ""2fb5111f-b618-4d8b-b47f-142ed4762064"", ""delete_on_termination"": null, |
| | ""disk_bus"": ""virtio"", ""image_id"": null, ""source_type"": ""snapshot"", |
| | ""device_type"": ""disk"", ""volume_id"": null, ""destination_type"": ""volume"", |
| | ""volume_size"": null}] |
| Property 'checksum' | 4eada48c2843d2a262c814ddc92ecf2c |
| Property 'container_format' | ami |
| Property 'disk_format' | ami |
| Property 'image_id' | da82a342-aeac-407a-bf9d-cf28bf68dc6b |
| Property 'image_name' | cirros-0.3.2-x86_64-uec |
| Property 'kernel_id' | 3b6a8424-1454-4394-810f-2adc95c6e326 |
| Property 'min_disk' | 0 |
| Property 'min_ram' | 0 |
| Property 'ramdisk_id' | aa76f1bd-8167-4586-8185-4347dc4951e0 |
| Property 'root_device_name' | /dev/vda |
| Property 'size' | 25165824 |
| created_at | 2014-07-23T03:16:18 |
| deleted | False |
| id | 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59 |
| is_public | False |
| min_disk | 0 |
| min_ram | 0 |
| name | snapx2 |
| owner | fdbb1e8f23eb40c89f3a677e2621b95c |
| protected | False |
| size | 0 |
| status | active |
| updated_at | 2014-07-23T03:16:18 |
+---------------------------------+----------------------------------------------------------------------------------+"
1,"Logstash query:
@message:""DBError: (IntegrityError) null value in column \""network_id\"" violates not-null constraint"" AND @fields.filename:""logs/screen-q-svc.txt""
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/logs/screen-q-svc.txt.gz#_2013-10-14_10_13_01_431
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/console.html
2013-10-14 10:16:28.034 | ======================================================================
2013-10-14 10:16:28.034 | FAIL: tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | ----------------------------------------------------------------------
2013-10-14 10:16:28.035 | _StringException: Traceback (most recent call last):
2013-10-14 10:16:28.035 | File ""tempest/api/volume/test_volumes_actions.py"", line 55, in tearDownClass
2013-10-14 10:16:28.036 | super(VolumesActionsTest, cls).tearDownClass()
2013-10-14 10:16:28.036 | File ""tempest/api/volume/base.py"", line 72, in tearDownClass
2013-10-14 10:16:28.036 | cls.isolated_creds.clear_isolated_creds()
2013-10-14 10:16:28.037 | File ""tempest/common/isolated_creds.py"", line 453, in clear_isolated_creds
2013-10-14 10:16:28.037 | self._clear_isolated_net_resources()
2013-10-14 10:16:28.037 | File ""tempest/common/isolated_creds.py"", line 445, in _clear_isolated_net_resources
2013-10-14 10:16:28.038 | self._clear_isolated_network(network['id'], network['name'])
2013-10-14 10:16:28.038 | File ""tempest/common/isolated_creds.py"", line 399, in _clear_isolated_network
2013-10-14 10:16:28.038 | net_client.delete_network(network_id)
2013-10-14 10:16:28.038 | File ""tempest/services/network/json/network_client.py"", line 76, in delete_network
2013-10-14 10:16:28.039 | resp, body = self.delete(uri, self.headers)
2013-10-14 10:16:28.039 | File ""tempest/common/rest_client.py"", line 308, in delete
2013-10-14 10:16:28.039 | return self.request('DELETE', url, headers)
2013-10-14 10:16:28.040 | File ""tempest/common/rest_client.py"", line 436, in request
2013-10-14 10:16:28.040 | resp, resp_body)
2013-10-14 10:16:28.040 | File ""tempest/common/rest_client.py"", line 522, in _error_checker
2013-10-14 10:16:28.041 | raise exceptions.ComputeFault(message)
2013-10-14 10:16:28.041 | ComputeFault: Got compute fault
2013-10-14 10:16:28.041 | Details: {""NeutronError"": ""Request Failed: internal server error while processing your request.""}"
0,"We need to improve the hacking rule to avoid markers of author for the tag:

'.. moduleauthor'"
1,"Tested with Havana rc2 from the UCA on Precise.
If the cinder-volumes (in a default configuration) LVM volume group does not exist, cinder will try to create a volume, which is bound to fail. The volume then gets stuck in the ""creating"" state and can't be deleted. The log will contain:
2013-10-17 09:29:58.188 16676 ERROR cinder.brick.local_dev.lvm [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Unable to locate Volume Group cinder-volumes
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Error encountered during initialization of driver: LVMISCSIDriver
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager Traceback (most recent call last):
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 190, in init_host
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager self.driver.check_for_setup_error()
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager File ""/usr/lib/python2.7/dist-packages/cinder/volume/drivers/lvm.py"", line 94, in check_for_setup_error
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager raise exception.VolumeBackendAPIException(data=message)
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager
2013-10-17 09:30:47.198 16676 WARNING cinder.volume.manager [req-d5f8a463-c097-4518-829b-504bf02763b2 None None] Unable to update stats, driver is uninitialized
Resetting the state with ""cinder reset-state"" will get the volume to the ""available"" state, which it isn't. Deleting or force-deleting will also fail, getting stuck in ""deleting"" state forever. The only solution I found was to directly kill the relevant rows in the volumes table and cinder-manage db sync.
I know in grizzly, cinder-volume would refuse to start if it couldn't find the volume-group. I thought that behavior was better. Failing when attempting to create the volume, instead of getting stuck in the creating state, would also be acceptable."
1,"It is currently possible to have swift-recon report on disk usage by two means:
swift-recon -d OR
swift-recon --all (which includes -d)
Disk usage provides two options:
--human-readable to provide usage in MB/GB/TB++ rather than bytes
--top <integer> to list devices with the most usage
These options are currently usable when using -d but not --all.
This is because the options are not passed to disk_usage when using all:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L930
Versus when using -d:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L965"
1,"This has been found on master:
1) create a DVR router: neutron router-create test --distributed
2) set external gateway: neutron router-gateway-set test public
3) observe stacktrace: http://paste.openstack.org/show/90577/ in L3 Agent's log
This does not seem to upset the L3 Agent and its ability to work correctly, but it would be good to eradicate the trace."
0,"If the NSX controller returns a 503, like below:
http://paste.openstack.org/show/69125/
(full log):
http://208.91.1.172/logs/neutron/69361/12/401769/logs/screen-q-svc.txt.gz?level=TRACE#_2014-02-23_01_09_35_875
Tempest failures may be observed, like:
http://208.91.1.172/logs/neutron/69361/12/401769"
1,"Nova Compute fails to start when there are multiple nova compute services running on different VMs(nova compute VMs) and each Vm managing multiple cluster in a vCenter and instances are provisioned on them.
Explanation:
Lets say, one nova compute vm(C1) is managing 5 clusters, and another(C2) managing 5 clusters. C1 manages n number of instances. Suppose in C2 compute service gets restarted, it fails to start.
Reason:
on the start up of the nova-compute, it checks the instances reported by the driver are still associated with this host. If they are not, it destroys them.
method _destroy_evacuated_instances calls the driver list_instances, which lists all the instances in the vCenter, though they are managed by different compute. Instead it should return only the vms which are managed by c1/c2.
log file attached."
1,"In this API, parameters are checked whether they are integer or not.
But when pass float values to this API, error doesn't occur.
Float values should be forbidden."
0,"Due to the fact that we sometimes need to manually terminate a volume's connection through volume api, it's possile that these backend drivers throw exceptions while doing that. Currently exceptions are bubbled up to volume API and are not being handled. This patch logs exception in volume manager and then raises VolumeBackendAPIException to caller."
1,"This is using the latest nova from trunk. In our deployment, we had a hypervisor go down and the tenant issued a hard reboot prior. When attempting a reboot on a guest with the state HARD_REBOOT, nova controller throws this error in it's logs and returns 'ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500)' to the user:
2014-03-06 18:21:00,535 (routes.middleware): DEBUG middleware __call__ Matched POST /tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Route path: '/{project_id}/servers/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>}
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>, 'project_id': u'tenant1', 'id': u'778032b2-469d-445e-abde-7b9b0b673324'}
2014-03-06 18:21:00,537 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Action: 'action', body: {""reboot"": {""type"": ""SOFT""}}
2014-03-06 18:21:00,538 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Calling method <bound method Controller._action_reboot of <nova.api.openstack.compute.contrib.keypairs.Controller object at 0x4c35a50>>
2014-03-06 18:21:00,747 (nova.api.openstack): ERROR __init__ _error Caught error: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
Traceback (most recent call last):
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/__init__.py"", line 125, in __call__
    return req.get_response(self.application)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1320, in send
    application, catch_exc_info=False)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1284, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 598, in __call__
    return self.app(env, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 925, in __call__
    content_type, body, accept)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 987, in _process_stack
    action_result = self.dispatch(meth, request, action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 1074, in dispatch
    return method(req=request, **action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/compute/servers.py"", line 1145, in _action_reboot
    self.compute_api.reboot(context, instance, reboot_type)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 2073, in reboot
    instance.save(expected_task_state=[None, task_states.REBOOTING])
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/instance.py"", line 472, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/api.py"", line 739, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 128, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2164, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2215, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
2014-03-06 18:21:00,750 (nova.api.openstack): INFO __init__ _error http://nova-controller.isg.apple.com:8774/v2/tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action returned with HTTP 500
The actual error message back to the user must be something along the lines of 'Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard' with a 4xx HTTP code."
0,"glance.tests.unit.utils:FakeDB uses glance.db.simple.api
FakeDB implements reset with:
    @staticmethod
    def reset():
        simple_db.DATA = {
            'images': {},
            'members': [],
            'tags': {},
            'locations': [],
            'tasks': {},
            'task_info': {}
        }
while api already has:
def reset():
    global DATA
    DATA = {
        'images': {},
        'members': [],
        'tags': {},
        'locations': [],
        'tasks': {},
        'task_info': {}
    }
the redundant code in glance.tests.unit.utils:FakeDB.reset should be replaced with simple_db.reset()"
0,"Found in commit 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
Original metadata is retrieved unnecessarily from the database when delete flag is set to True in the following methods in cinder volume api.
update_volume_metadata
update_volume_admin_metadata
update_snapshot_metadata"
1,"one simple log typos in compute/manager.py rebuild_instance function.
             else:
                    image_ref = orig_image_ref = instance.image_ref
                    LOG.info(_(""disk not on shared storagerebuilding from:""
                               "" '%s'"") % str(image_ref))
should change to
                    LOG.info(_(""disk not on shared storage, rebuilding from:""
                               "" '%s'"") % str(image_ref))"
1,"According to this bug https://bugs.launchpad.net/nova/+bug/1256306
for one image in _base dir 03d8e206-6500-4d91-b47d-ee74897f9b4e
2 locks were created
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-03d8e206-6500-4d91-b47d-ee74897f9b4e
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-_var_lib_nova_instances__base_03d8e206-6500-4d91-b47d-ee74897f9b4e
generally locks are used to protect concurrent data access, so the lock can't work as they expected (mutual access)
in current code fetch_image from glance use lock nova-xxxxx while copy image from _base to target directory use nova_var_lib_xxx
should they use same lock?"
1,"Stoptinstance response elements shown as below:

Sample Request to stop the specified instance:
===
https://ec2.amazonaws.com/?Action=StopInstances
&InstanceId.1=i-10a64379
&AUTHPARAMS
==

Response elements are:
==
"":<StopInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-30edb813-5802-4fa2-8a83-9dbcb751264e</requestId>
  <return>true</return>
</StopInstancesResponse>
""

But as per the AWS API reference doc, the response elements shown be as below:
==
<StopInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>64</code>
          <name>stopping</name>
      </currentState>
      <previousState>
          <code>16</code>
          <name>running</name>
      </previousState>
  </instancesSet>
</StopInstancesResponse>
===

The <instanceSet> information missing in the response elements."
0,remove it!
0,"the extra_dhcp_opts allows empty string ' ', as an option_value, which dnsmasq has been detected ans segment faulting when encountering a tag:ece4c8aa-15c9-4f6b-8c42-7d4e285734bf,option:server-ip-address, option in the opts file.
Checks are need in the create and update protions of the extra_dhcp_opts extension to prevent this."
0,"tox -e py27 neutron.tests.unit.test_l3_plugin
fails as follows. This is bacause necessary oslo config isn't initialized properly by L3AgentDbIntTestCase and L3AgentDbSepTestCase
The error log follows.
 $ tox -e py27 neutron.tests.unit.test_l3_plugin
py27 develop-inst-nodeps: /home/yamahata/openstack/tacker/neutron-l3-plugin/upstream/neutron-l3-db-refacotr-0
py27 runtests: commands[0] | python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin
running testr
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --list
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpNLNI_l
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpBl_vq6
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpO3NkaL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpc3FrWL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmppz6MIP
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmprcHfnW
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpVW99Uk
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpMC3U19
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpiqP4jI
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpK6j9JS
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmp_cFhYl
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --load-list /tmp/tmpNXI0Mi
======================================================================
FAIL:
neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_floatingips
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_router_gateway_op_agent
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbIntTestCase.test_l3_agent_routers_query_floatingips
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2060, in setUp
    self.core_plugin = TestL3NatIntPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_ignore_interfaces_with_moreThanOneIp
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: process-returncode
tags: worker-10
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
======================================================================
FAIL: process-returncode
tags: worker-1
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 304 (-13543) tests in 27.382s (-600.855s)
FAILED (id=17, failures=6 (-2))
error: testr failed (1)
ERROR: InvocationError: '/neutron/.tox/py27/bin/python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin'
__________________________________________________________________ summary __________________________________________________________________
ERROR: py27: commands failed"
0,"As reported in bug 1236970, plugins without quota extensions fail to start due to missing table in the database.
This is the side effect of the recent commit: Quota DB driver has been recently enabled by default""
https://review.openstack.org/#/c/49993

If a plugin has no support of quota extensions, quota_driver should fallback to the config quota driver.
Otherwise we need to add quota extension support to all plugins."
0,"If ipv6 module is not loaded in kernel ip6tables command doesn't work and fails in openvswitch-agent when processing ports:
2014-08-05 15:20:57.089 3944 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Error while processing VIF ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1262, in rpc_loop
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent ovs_restarted)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1090, in process_network_ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent port_info.get('updated', set()))
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 247, in setup_port_filters
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self.prepare_devices_filter(new_devices)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 164, in prepare_devices_filter
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self.firewall.prepare_port_filter(device)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self.gen.next()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/firewall.py"", line 108, in defer_apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self.filter_defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_firewall.py"", line 370, in filter_defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self.iptables.defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 353, in defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent self._apply()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 369, in _apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent return self._apply_synchronized()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 400, in _apply_synchronized
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent root_helper=self.root_helper)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/utils.py"", line 76, in execute
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent raise RuntimeError(m)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent RuntimeError:
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip6tables-restore', '-c']
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Exit code: 2
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stdout: ''
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stderr: ""ip6tables-restore v1.4.21: ip6tables-restore: unable to initialize table 'filter'\n\nError occurred at line: 2\nTry `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.\n""
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
2014-08-05 15:20:58.261 3944 INFO neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Agent out of sync with plugin!
2014-08-05 15:20:58.749 3944 INFO neutron.agent.securitygroups_rpc [-] Preparing filters for devices set([u'5e646c57-0ce4-4705-9281-2cf991cd4135', u'1e0ea538-74a4-429d-97fb-08fbae37ad47'])"
1,Hyper-V can change the order of the mounted drives when rebooting a host and thus passthrough disks can be assigned to the wrong instance resulting in a critical scenario.
1,"When database exception raises in update_device_down, the linuxbridge-agent doesn't deal with them in a proper way that causes accessing not-existed local variables.
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent Traceback (most recent call last):
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 997, in daemon_loop
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent sync = self.process_network_devices(device_info)
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 894, in process_network_devices
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent resync_b = self.treat_devices_removed(device_info['removed'])
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 963, in treat_devices_removed
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent if details['exists']:
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent UnboundLocalError: local variable 'details' referenced before assignment
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
2014-04-22 20:35:53.437 494 DEBUG neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Loop iteration exceeded interval (2 vs. 51.2715768814)! daemon_loop /usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1011
2014-04-22 20:35:53.438 494 INFO neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Agent out of sync with plugin!"
1,"I run in multi node setup with ML2, L2-population and Linuxbridge MD, and vxlan TypeDriver.
I create a VM with ip 10.0.0.105 and mac 00:00:00:55:55:55
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001 lladdr 00:00:00:55:55:55 PERMANENT
I delete it.
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001 FAILED
I recreate a VM with the same ip/mac :
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001 FAILED
in q-agt.log, we can see that the ""ip neigh add"" command fails."
1,"When the 3PAR driver creates a vlun and then has to fetch it, the fetching could return the wrong vlun.
When a volume is exported to the same host more than once, we will have multiple LUN ids associated with that volume.
We have to make sure we get the correct LUN for the VLUN we just created. This is especially important
 in a multi-attach scenario."
1,"This was reported by Michael Turek as he was testing this while the patches were still in flight See: https://review.openstack.org/#/c/114938/26/nova/virt/hardware.py
As described on there - the code there makes a bad assumption about the format in which it will get the data in the scheduler, which results in:
2014-09-15 10:45:44.906 ERROR oslo.messaging.rpc.dispatcher [req-f29a469e-268d-49bf-abfa-0ccb228d768c admin admin] Exception during message handling: An object of type InstanceNUMACell is required here
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher return func(*args, **kwargs)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/scheduler/manager.py"", line 175, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 300, in _schedule
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher chosen_host.obj.consume_from_instance(context, instance_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 252, in consume_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher self, instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/hardware.py"", line 978, in get_host_numa_usage_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher instance_numa_topology = instance_topology_from_instance(instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/hardware.py"", line 949, in instance_topology_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher cells=cells)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/base.py"", line 242, in __init__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher self[key] = kwargs[key]
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/base.py"", line 474, in __setitem__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher setattr(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/base.py"", line 75, in setter
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher field_value = field.coerce(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/fields.py"", line 388, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher obj, '%s[%i]' % (attr, index), element)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/objects/fields.py"", line 474, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher self._obj_name)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher ValueError: An object of type InstanceNUMACell is required here"
0,"When I do run_tests.sh in cinder I get the following error at the end of the run.
vagrant@devstack:/opt/stack/cinder$ ./run_tests.sh
 [SNIP]
Slowest 10 tests took 11.54 secs:
cinder.tests.api.contrib.test_volume_actions.VolumeRetypeActionsTest
    test_update_readonly_flag 1.12
cinder.tests.test_storwize_svc.StorwizeSVCDriverTestCase
    test_storwize_svc_host_maps 1.39
    test_storwize_svc_multi_host_maps 1.11
    test_storwize_svc_retype_need_copy 1.09
    test_storwize_svc_retype_only_change_iogrp 1.09
    test_storwize_svc_snapshots 1.13
    test_storwize_svc_volume_migrate 1.13
    test_storwize_svc_volume_params 1.15
    test_storwize_terminate_connection 1.10
    test_storwize_vdisk_copy_ops 1.23
Ran 3083 tests in 148.093s
OK
No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy""
vagrant@devstack:/opt/stack/cinder$"
1,"the default value of ""quota_firewall_rule"" is ""-1"", and this means unlimited. There will be potential security issue if openstack admin do not modify this default value.
A bad tenant User can create unlimited firewall rules to ""attack"" network node, in the backend, we will have a large number of iptables rules. This will make the network node crash or very slow.
So I suggest we use another number but not ""-1"" here."
0,"ML2 plugin is not loaded properly in TestMl2SGServerRpcCallBack.
By grepping a result in .testrepository, test_extension_security_group.SecurityGroupTestPlugin is loaded as a core plugin in ML2 test. It means Ml2 plugin is not tested in this case.
$ OS_DEBUG=1 python setup.py testr --slowest --testr-args='neutron.tests.unit.ml2'
$ grep ""Loading Plugin"" .testrepository/235 | cut -d ']' -f 2 | cut -d : -f 2 | sort | uniq -c
    835 neutron.plugins.ml2.plugin.Ml2Plugin
     35 neutron.services.l3_router.l3_router_plugin.L3RouterPlugin
     14 neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin
    137 neutron.tests.unit.test_l3_plugin.TestL3NatServicePlugin"
1,ARNING nova.network.neutronv2 [req-eb54925d-c466-4069-be4e-691e155ea85d None None] Using neutron_admin_tenant_name for authentication is deprecated and will be removed in the next release. Use neutron_admin_tenant_id instead.
1,"When user passes only white spaces to flavor name, it creates flavor successfully. Since name is a mandatory parameter, it should restrict user from passing white spaces. Also leading and trailing white spaces should be removed before saving it to the backend similar to the instance name.
{
    ""flavor"": {
        ""name"": "" ""
        ""ram"": 1024,
        ""vcpus"": 2,
        ""disk"": 10,
        ""id"": ""10"",
        ""os-flavor-access:is_public"": false
    }
}
For example
name = "" "" #not allowed
name = ""extra large"" #allowed
name = "" extra large "" #allowed, but leading and trailing white spaces will be trimmed before saving it to the backend.
Actual output: HTTP/1.1 200 OK
Expected output: HTTP/1.1 400 Bad Request"
1,"Glance v2 api can not set or delete a property of a image when the value of the property is an empty string.
__delitem__ and __setitem__ methods of ExtraPropertiesProxy class in glance/api/property_protections.py may have some problems. When the value of a property is an empty string, the methods will raise KeyError."
0,The Big Switch plugin unnecessarily deletes individual ports from the controller when a network is being deleted. This isn't needed because the controller will automatically clean up the ports when their parent network is deleted.
1,"While running Neutron Tempest tests for Cisco N1KV plugin, XML tests fail with the following trace:
FAIL: tempest.api.network.test_networks.BulkNetworkOpsTestXML.test_bulk_create_delete_network[gate,smoke]
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-02-13 11:08:50,454 Request: POST http://192.168.255.184:9696/v2.0/networks
2014-02-13 11:08:50,454 Request Headers: {'Content-Type': 'application/xml', 'Accept': 'application/xml', 'X-Auth-Token': '<Token omitted>'}
2014-02-13 11:08:50,454 Request Body: <?xml version=""1.0"" encoding=""UTF-8""?>
<networks xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network ><name >network--331846832</name></network><network ><name >network--859697792</name></network></networks>
2014-02-13 11:08:51,051 Response Status: 201
2014-02-13 11:08:51,051 Glance request id req-b2eb0771-614b-4edd-a4f8-df8c9f60e0dd
2014-02-13 11:08:51,051 Response Headers: {'content-length': '962', 'date': 'Thu, 13 Feb 2014 19:08:51 GMT', 'content-type': 'application/xml; charset=UTF-8', 'connection': 'close'}
2014-02-13 11:08:51,051 Response Body: <?xml version='1.0' encoding='UTF-8'?>
<networks xmlns=""http://openstack.org/quantum/api/v2.0"" xmlns:quantum=""http://openstack.org/quantum/api/v2.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--331846832</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>908124c9-f76c-4861-a875-21f9793d09f7</id></network><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--859697792</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>50fff0cb-e592-4916-b7f2-1047c9174705</id></network></networks>
}}}
Traceback (most recent call last):
  File ""tempest/api/network/test_networks.py"", line 311, in test_bulk_create_delete_network
    resp, body = self.client.create_bulk_network(2, network_names)
  File ""tempest/services/network/network_client_base.py"", line 186, in create_bulk_network
    body = {'networks': self.deserialize_list(body)}
  File ""tempest/services/network/xml/network_client.py"", line 43, in deserialize_list
    return parse_array(etree.fromstring(body), self.PLURALS)
  File ""lxml.etree.pyx"", line 2754, in lxml.etree.fromstring (src/lxml/lxml.etree.c:54631)
  File ""parser.pxi"", line 1578, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:82748)
  File ""parser.pxi"", line 1457, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:81546)
  File ""parser.pxi"", line 965, in lxml.etree._BaseParser._parseDoc (src/lxml/lxml.etree.c:78216)
  File ""parser.pxi"", line 569, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:74472)
  File ""parser.pxi"", line 650, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:75363)
  File ""parser.pxi"", line 590, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:74696)
XMLSyntaxError: Namespace prefix n1kv on profile_id is not defined, line 2, column 211"
1,"The update quota sets API should check whether 'quota_set' is in body.
The code to update quota sets does the following:
for key in body['quota_set'].keys():"
0,"Currently there is one ML2 driver for cisco nexus in
    neutron/plugins/ml2/drivers/cisco
It needs to go down a level so other cisco drivers can live alongside it:
    neutron/plugins/ml2/drivers/cisco/apic
    neutron/plugins/ml2/drivers/cisco/nexus
    neutron/plugins/ml2/drivers/cisco/ucs
    neutron/plugins/ml2/drivers/cisco/..."
1,"Rescuing an instance seems to guess the image format at some point. This allows reading files from the compute host via the qcow2 backing file.
Requirements:
- instances spawned using libvirt
- use_cow_images = False in the config
To reproduce:
1. Create a qcow2 file backed by the path you want to read from the compute host. (qemu-img create -f qcow2 -b /path/to/the/file $((1024*1024)) evil.qcow2)
2. Spawn an instance, scp the file into it.
3. Overwrite the disk inside the instance (dd if=evil.qcow2 of=/dev/vda)
4. Shutdown the instance.
5. Rescue the instance
6. While in rescue mode, login and read /dev/vdb - beginning should be read from the qcow backing file
Libvirt description of the rescued instance will contain the entry for the second disk with attribute type=""qcow2"", even though it should be ""raw"" - same as the original instance.
Mitigating factors:
- files have to be readable by libvirt/kvm
- apparmor/selinux will limit the number of accessible files
- only full blocks of the file are visible in the rescued instance, so short files will not be available at all and long files are going to be truncated
Possible targets:
- private snapshots with known uuids, or instances of other tenants are a good target for this attack"
0,"Error 242 occurs fairly often in gate tests: http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkV4aXQgY29kZTogMjQyXCIgIiwiZmllbGRzIjpbImZpbGVuYW1lIl0sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNDMyMDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzg1MzE3OTg4NTQxLCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
This is actual an ALARM_CLOCK error [142] (rootwrap adds 100 to the error code), and means open vswitch times out; as the default timeout is 2 seconds there is a chance they could occur quite often in a rather stressful scenario as the one represented by parallel tests in tenant isolation.
It might be therefore advisable to allow for a configurable timeout on ovs commands, and increase this timeout for gate tests.
Kernel logs do not provide enough additional information.
It might be also worth adding open vswitch logs to the logs collected by devstack-gate"
1,"When we only install neutron-linuxbridge-agent package on RHEL6.5, the dhcp agent can not be started successfully. That is because there is hard code to use openvswitch.common plugin in nuetron.agent.linux.ovs_lib.py.
[root@osee15-control01 neutron]# /usr/bin/neutron-dhcp-agent --log-file /var/log/neutron/dhcp-agent.log --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini
Traceback (most recent call last):
  File ""/usr/bin/neutron-dhcp-agent"", line 6, in <module>
    from neutron.agent.dhcp_agent import main
  File ""/usr/lib/python2.6/site-packages/neutron/agent/dhcp_agent.py"", line 27, in <module>
    from neutron.agent.linux import interface
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/interface.py"", line 26, in <module>
    from neutron.agent.linux import ovs_lib
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ovs_lib.py"", line 31, in <module>
    from neutron.plugins.openvswitch.common import constants
ImportError: No module named openvswitch.common"
1,"I launch a vm with lvm-type image successfully, whose flavor includes ""OS-FLV-EXT-DATA:ephemeral"" flag, the size is 1G.
So, I can see two logical volumes have beed created by ""lvs"" command.
Then I call 鈥渘ova rebuild $SERVERID $IMAGEID鈥? try to rebuild it with the same image. Error occur, the vm'state get to ERROR, the two volumes disapear sametime.
nova-compute's log is:
""""""
2014-02-18 19:54:39.290 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvs -o lv_size --noheadings --units b --nosuffix /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:39.626 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): dd bs=1048576 if=/dev/zero of=/dev/cinder-volumes/instance-0000007c_disk.local seek=0 count=1024 oflag=direct execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:47.622 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:48.061 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:48.066 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:49.081 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:49.279 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:49.286 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:50.222 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:50.359 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:50.364 2368 ERROR nova.compute.manager [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Setting instance vm_state to ERROR
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Traceback (most recent call last):
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 4967, in _error_out_instance_on_exception
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] yield
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 2037, in rebuild_instance
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] block_device_info=block_device_info)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 835, in destroy
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] destroy_disks, context=context)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 935, in _cleanup
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] self._cleanup_lvm(instance)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 957, in _cleanup_lvm
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] libvirt_utils.remove_logical_volumes(*disks)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 392, in remove_logical_volumes
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] execute(*lvremove, attempts=3, run_as_root=True)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 50, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] return utils.execute(*args, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] return processutils.execute(*cmd, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] File ""/usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] cmd=' '.join(cmd))
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] ProcessExecutionError: Unexpected error while running command.
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Command: lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Exit code: 5
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stdout: ''
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stderr: ' One or more specified logical volume(s) not found.\n'
""""""
In nova.virt.libvirt.utils.py:
""""""
def remove_logical_volumes(*paths):
    """"""Remove one or more logical volume.""""""
    for path in paths:
        clear_logical_volume(path)
    if paths:
        lvremove = ('lvremove', '-f') + paths
        execute(*lvremove, attempts=3, run_as_root=True)
""""""
It will try 3 times to remove two volumes (instance-0000007c_disk and instance-0000007c_disk.local).
Then I trace the problem in another example, when it happed again, pdb result show below.
The 1st try:
""""""
2014-02-18 22:33:48.579 24156 DEBUG nova.openstack.common.processutils [req-a97ce6d7-ee76-4349-9d89-7c2e7325f2d5 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
> /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py(173)execute()
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
(' Logical volume ""instance-0000007e_disk"" successfully removed\n', ' Can\'t remove open logical volume ""instance-0000007e_disk.local""\n')
(Pdb) c
...
""""""
It shows that remove ephemeral disk fail, but image disk success.
2nd try begin:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
(' Logical volume ""instance-0000007e_disk.local"" successfully removed\n', ' One or more specified logical volume(s) not found.\n')
(Pdb) c
...
""""""
The ephemeral disk is remove in this time, but image disk have been removed in last step, so the whole command still fail.
3rd try:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('', ' One or more specified logical volume(s) not found.\n')
(Pdb)
""""""
Openstack think remove_logical_volumes fail, though two volumes have been removed in fact.
It seems that retry to remove open volume is useful, but if the situation that not all volumes were removed once time happen, the retry are useless, and remove_logical_volume must fail."
1,"See https://review.openstack.org/14139
This:
                    if not local_lock_path:
                        cleanup_dir = True
                        local_lock_path = tempfile.mkdtemp()
                    if not os.path.exists(local_lock_path):
                        cleanup_dir = True
                        ensure_tree(local_lock_path)
                    ...
                    finally:
                        # NOTE(vish): This removes the tempdir if we needed
                        # to create one. This is used to cleanup
                        # the locks left behind by unit tests.
                        if cleanup_dir:
                            shutil.rmtree(local_lock_path)
Why are we deleting the lock dir here? Does that even work? i.e. what if someone concurrently tries to take the lock, re-creates the dir and lock a new file?"
0,Fix error docstring and remove unused variable 'volume ' in VolumeTransferTestCase.
1,"ip_lib.IpRouteCommand.get_gateway() raises ValueError to default_route_line like
default via 192.168.99.1 proto static
since it wrongly assumes that the 5th word is a value metric (in the example above, it is ""static"")."
1,"http://logs.openstack.org/72/61972/27/gate/gate-tempest-dsvm-full/ed1ab55/logs/testr_results.html.gz

pythonlogging:'': {{{
2014-06-25 06:45:11,596 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 202 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers 0.295s
2014-06-25 06:45:11,674 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.077s
2014-06-25 06:45:12,977 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.300s
2014-06-25 06:45:12,978 25675 INFO [tempest.common.waiters] State transition ""BUILD/scheduling"" ==> ""BUILD/spawning"" after 1 second wait
2014-06-25 06:45:14,150 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.171s
2014-06-25 06:45:14,153 25675 INFO [tempest.common.waiters] State transition ""BUILD/spawning"" ==> ""ERROR/None"" after 3 second wait
2014-06-25 06:45:14,221 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 400 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f/action 0.066s
2014-06-25 06:45:14,404 25675 INFO [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 204 DELETE http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.182s
}}}

Traceback (most recent call last):
  File ""tempest/api/compute/servers/test_delete_server.py"", line 97, in test_delete_server_while_in_verify_resize_state
    resp, server = self.create_test_server(wait_until='ACTIVE')
  File ""tempest/api/compute/base.py"", line 247, in create_test_server
    raise ex
BadRequest: Bad request
Details: {'message': 'The server could not comply with the request since it is either malformed or otherwise incorrect.', 'code': '400'}"
1,"This code is incorrect: https://github.com/openstack/neutron/blob/master/neutron/db/l3_dvr_db.py#L297
The result is that no port is found by the query. Because of this, a new port gets created each time the L3 agent restarts and tries to find the external gw port for the DVR router."
1,"_copy_volume_from_snapshot appears to select its source file using _local_path_volume but needs to resolve source file by using the snap_info metadata.
Found this while filling in gaps in the unit test code, will add tests to cover this."
0,"depending on the sequence of tests, test_openvswitch_plugin sometimes ends up to kick
unexpected code via impl_fake rpc backend.
an example of the failure:
http://logs.openstack.org/91/71791/5/check/gate-neutron-python27/2317e6a/"
1,"Reproduce:
1. nova stop cirros
2.nova diagnostics cirros
[root@control-compute00 ~(keystone_admin)]# nova diagnostics cirros
ERROR: 'NoneType' object has no attribute 'iteritems'"
0,"The vmdk driver ignores the maintenanceMode property of a datastore and considers it as a potential candidate for volume creation. Rather, it should ignore datastores in maintenance mode for volume creation."
1,"2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
ram (MB): -1559
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
disk (GB): 29
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
VCPUS: -3

was showed in my compute node logs which make me confusing
we need to avoid this kind of audit logs and give more accurate info

discussions here:
http://lists.openstack.org/pipermail/openstack-dev/2014-May/034312.html"
0,"[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi: _call_method (session=52e6b7a6-4745-7303-17f1-52ed4da676a8)
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 817, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 197, in vim_request_handler
    for child in detail.getChildren():
AttributeError: 'NoneType' object has no attribute 'getChildren'"
1,When booting from a volume the config driver will not be mounted (if configured)
0,"Images v2 api allows you to update container and disk format after image upload
But does not allow you to upload an image without it being speficied"
0,"DHCP ChanceScheduler has _schedule_bind_network method which can detect duplicate entry.

Need to add the same ability to auto_schedule_networks.

That might be important for the case when api server and rpc_server run in different processes."
1,"When a volume is transferred from one project to another, this transferred volume cannot be attached to any instance due to wrong AUTH information. Nova-compute threw exception when trying to attach the volume:
2014-07-25 00:03:57,358.358 42302 ERROR nova.compute.manager [req-04e6cad9-0d3a-4c3c-a9b3-a9d3a0247872 6a08f3e43965436fb028eda1005ec77b b67a57cc0b9d443eac6d2ff8a494b088] [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Failed to attach volume c946565f-d87a-48df-a6f4-baa3bb9d2300 at /dev/vdc
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Traceback (most recent call last):
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/compute/manager.py"", line 3690, in _attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] encryption=encryption)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1083, in attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] disk_info)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1042, in volume_driver_method
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return method(connection_info, *args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return f(*args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 287, in connect_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] self._run_iscsiadm(iscsi_properties, (""--rescan"",))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 218, in _run_iscsiadm
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] check_exit_code=check_exit_code)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return processutils.execute(*cmd, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] cmd=' '.join(cmd))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] ProcessExecutionError: Unexpected error while running command.
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf iscsiadm -m node -T iqn.2010-01.com.SF:ogav.uuid-c946565f-d87a-48df-a6f4-baa3bb9d2300.200982 -p ISCSITARGET:3260 --rescan
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Exit code: 255
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stdout: ''
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stderr: 'iscsiadm: No portal found.\n'
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682]
How to reproduce:
1) Create a volume X on Solidfire with project A;
2) Transfer volume X to project B;
3) attach volume X to any instance;
In solidfire.py:accept_transfer(), the volume metadata in the backend is changed but the AUTH (CHAP username secret) remains unchanged, which causes the mismatch of the ownership and corresponding AUTH info. Cinder volume manager will have to be changed as well to adopt cases like this (volume info must be updated to DB once driver completes accept_transfer)."
1,"In migration f9263d6df56_remove_dhcp_lease there is mistake in DateTime type usage for mysql, used sa.DATETIME() instead of sa.DateTime()

   File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/f9263d6df56_remove_dhcp_lease.py"", line 46, in downgrade
    nullable=True))
  File ""<string>"", line 7, in add_column
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 363, in add_column
    schema=schema
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 127, in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
ProgrammingError: (ProgrammingError) type ""datetime"" does not exist
LINE 1: ALTER TABLE ipallocations ADD COLUMN expiration DATETIME
                                                        ^
 'ALTER TABLE ipallocations ADD COLUMN expiration DATETIME' {}"
0,"""class Port"" seems like a leftover from days when agents had direct database accesses."
1,"When review patch: https://review.openstack.org/#/c/64076/
I found that:
def db_sync(version=None, current_version=None):
    """"""
Place a database under migration control and perform an upgrade
:retval version number
""""""
    sql_connection = CONF.sql_connection
    try:
        _version_control(current_version or 0)
    except versioning_exceptions.DatabaseAlreadyControlledError as e:
        pass
    if current_version is None:
        current_version = int(db_version())
    if version is not None and int(version) < current_version:
        downgrade(version=version)
    elif version is None or int(version) > current_version:
        upgrade(version=version)
db_sync doesn't return anything, the doc string and code are broken. Since db_sync() accept empty arguments list, so a reasonable version number should be returned instead of remove the incorrect doc string."
1,"In v2, for some exceptions the exception is not converted to a string properly resulting in:
2014-07-09 15:36:38,714 INFO msg = _(""Image exceeds the storage quota: %s"") % e
2014-07-09 15:36:38,714 INFO File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-07-09 15:36:38,714 INFO raise UnicodeError(msg)
in some cases 'utils.exception_to_str' is used to cast the exception.
We should do this in all cases for consistency/to avoid the stack trace"
1,"Note: the reproduction case below has been fixed by not blocking migration on config drives. However, the underlying issue of NFS not being marked
as shared storage still stands, since the 'is_shared_block_storage' data
is used elsewhere as well.
---------------------------------------------------------------------------
To reproduce:
1. Set up shared instance storage via NFS and use one of the file-based image backends
2. Boot an instance with a config drive
3. Attempt to live migrate said instance w/o doing a block migration
The issue is caused by the following lines in nova/virt/libvirt/driver.py:
        if not (is_shared_instance_path and is_shared_block_storage):
            # NOTE(mikal): live migration of instances using config drive is
            # not supported because of a bug in libvirt (read only devices
            # are not copied by libvirt). See bug/1246201
            if configdrive.required_by(instance):
                raise exception.NoLiveMigrationForConfigDriveInLibVirt()
The issue, I believe, was caused by commit bc45c56f1, which separated checks for shared instance directories and shared block storage backends like Ceph. The issue is that if a deployer is not using Ceph, the call to self.image_backend.backend().is_shared_block_storage() returns False. However, is_shared_block_storage should not even be considered if the image backend is a file-based one."
1,"baremetal nodes are failing to boot for me:
2013-10-03 01:55:15,392.392 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpMxP1Xr power on execute /opt/stack/venvs/nova/local/lib/pyt
hon2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,573.573 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power Control: Up/On
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
2013-10-03 01:55:15,707.707 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpRYI8dn power status execute /opt/stack/venvs/nova/local/lib
/python2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,890.890 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power is off
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
Note the gap between up and status checking is only 200ms (from IPMI return to status check) but in that interval this hardware doesn't toggle to on.
After the deploy fails, a manual check with
ipmitool -I lanplus -H 10.10.16.40 -U Administrator -P <password> chassis power status
returns
Chassis Power is on"
1,The new consistency hash table seems to be causing deadlocks.
1,"In the code there are several comparisons using the ""in"" operator to check if a value occurs in a tuple with only one element, like:
    if a in (""foo""):
        do_something()
This comparison is wrong, since that is not a tuple with one element [1], therefore if the variable ""a"" had a value of ""f"" it will match.
[1] http://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences"
1,"In V2 API layer code, it would be better if we raise exception instead of return it
try:
            self.volume_api.delete_snapshot(context, id)
        except exception.NotFound:
            return exc.HTTPNotFound() ----> use raise instead of return"
0,"In the nova.virt.libvirt.driver.LibvirtDriver. _get_new_connection method two different libvirt event handlers are registered, one for lifecycle events (_event_lifecycle_callback) and one for connection events (_close_callback). These callbacks are called by a native thread that is continually calling libvirt.virEventRunDefaultImpl() in the _native_thread method; the latter method's Docstring contains the following note:
        This is a native thread which runs the default
        libvirt event loop implementation. This processes
        any incoming async events from libvirtd and queues
        them for later dispatch. This thread is only
        permitted to use libvirt python APIs, and the
        driver.queue_event method. In particular any use
        of logging is forbidden, since it will confuse
        eventlet's greenthread integration
while this rule is adhered to by the _event_lifecycle_callback method, it is violated by _close_callback (the other callback) because it calls the _set_host_enabled method which, among other things, writes to the log.
The _close_callback method needs to be altered so that it does not execute any logic that may interfere with eventlet's greenthread integration."
1,"1.Set the item ""security_group_api=nova"" in nova.conf
2.Restart nova
3.Create a security group
4.Update the security group
   PUT http://192.168.83.241:8774/v2/99a7b3d4bd6540aaaceae89ac74bfab6/os-security-groups/7
   {
    ""security_group"": {
        ""name"": ""huangtianhua"",
        ""description"":""for test""
        }
   }
5.The server raises exception as bellow:
   {
    ""computeFault"": {
        ""message"": ""The server has either erred or is incapable of performing the requested operation."",
        ""code"": 500
       }
   }
6.I think it's a bug.When traversal the rules of the group before returning throws error:
   ""DetachedInstanceError: Parent instance &lt;SecurityGroup at 0x789eed0&gt; is not bound to a Session; lazy load operation of attribute 'rules' cannot proceed."""
1,"The base= field is not using the correct value when a libvirt snapshot_delete is issued on the Nova side -- pass correct values for ""file_to_merge"" in the delete_info dict depending on whether or not other snapshots exist.
This fixes an issue where attached snapshot deletion results in an inconsistent qcow2 chain."
1,"When I tried to assign one of my successfully-allocated floating IPs to one of my instances, my nova-network.log showed this rather unhelpful traceback: http://paste.openstack.org/show/40674/
I'm running nova-network 1:2013.1.2-0ubuntu1~cloud0 from the cloud archive, on Ubuntu 12.04 LTS.
My searches through bugs while trying to find the *real* error turned up bug#1119817 which seems to be of a type with this one (although apparently more thoroughly researched)."
1,"1.Create metadata for a volume with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the volume, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the volume
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}"
0,"When the tests for FWaaS create a firewall and then delete it, the firewall object is still in the database in a PENDING_DELETE state waiting for the agent to remove it. Parent objects then can't be deleted because the firewall object is dependent on them.
Currently, all of the tests just leave the parent objects by using no_delete[1], which leaves things in the database and could lead to potential conflicts later.
1. https://github.com/openstack/neutron/blob/ac8c0c645de001a0d074cdfd9448f9680a5d5e34/neutron/tests/unit/db/firewall/test_db_firewall.py#L732"
1,"This stacktrace:
2014-04-17 09:37:06.460 10212 ERROR neutron.openstack.common.loopingcall [req-512e4936-b85a-4019-bc2c-e1b8faf4c8c5 None] in dynamic looping call
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall Traceback (most recent call last):
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall File ""/opt/stack/neutron/neutron/openstack/common/loopingcall.py"", line 123, in _inner
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall idle = self.f(*self.args, **self.kw)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 647, in _synchronize_state
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall scan_missing=scan_missing)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 401, in _synchronize_lrouters
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall filters=filters)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 195, in _get_collection
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall items = [dict_func(c, fields) for c in query]
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall File ""/opt/stack/neutron/neutron/db/l3_db.py"", line 106, in _make_router_dict
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall nw_id = router.gw_port['network_id']
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall TypeError: 'NoneType' object has no attribute '__getitem__'
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall
has been observed during this run:
http://208.91.1.172/logs/neutron/82709/9/412384/"
0,"The changes in commit:
  https://github.com/openstack/nova/commit/7161c62c22ebe609ecaf7e01d2feae473d01495a
Introduced version checking for boto 2.14; however string comparison is not effective for version checking as
'2.9.6' >= '2.14' == true
Resulting in older boto versions (as found in saucy) trying to test with the 2.14 code."
0,"Based on the discussion of drivers setting inappropriate permissions (https://wiki.openstack.org/wiki/OSSN/OSSN-0014), ibmnas driver inherits _set_rw_permissions_for_all() from nfs.py which sets 666 to the volumes.
Changes are expected to be made in nfs.py such that it sets 600 or 660 permissions, while doing so ibmnas driver fails performing the operation _resize_volume_file(). This can be fixed by performing resize operation using root user."
1,"The `user_id` passed in db.quota_get() should be a keyword argument.
The prototype of quota_get():
def quota_get(context, project_id, resource, user_id=None)
But In DbQuotaDriver::get_by_project_and_user():
return db.quota_get(context, project_id, user_id, resource)
which should be:
return db.quota_get(context, project_id, resource, user_id=user_id)"
1,"The Storwize driver mistakenly checks on startup if vdisk_count > 0 for io groups, and fails if this is not the case. This means that controllers with empty io groups cannot be used."
1,"If there are many networks when using metaplugin, net-list (GET networks API)
takes very long time.
For example: (showing hardware spec etc. is omitted since it is relative comparison.)
--- 200 networks, openvswitch plugin used natively
$ time neutron net-list
...snip
real 0m2.007s
user 0m0.428s
sys 0m0.100s
---
--- 200 openvswitch networks, under metaplugin
$ time neutron net-list
...snip
real 0m7.700s
user 0m0.472s
sys 0m0.072s
---
Note that the quantum-server wastes a lot of cpu usage too."
1,"After image-api partial refactor, some calls still use a glance service instance to call compute_utils.get_instance_metadata which expect the object to have a 'get()' method.
Since that method is not present in GlanceImageService, and exception is thrown and the image metadata cannot be retrieved.
Sample calling _cold_migration(..)
2014-06-05 15:45:13.138 WARNING nova.compute.utils [req-7a86365f-f01a-4d49-b1c3-595e8dc9bd24 admin admin] [instance: 290d3587-b69a-48d8-b5c0-307259e2f590] Can't access image 40c33532-0aed-4acc-8d7a-2a45698e1f2d: 'GlanceImageService' object has no attribute 'get'"
1,"The network namespace is not mandatory, but that makes
root_wrap non mandatory too, because you could
want to start non-privileged processes outside a
namespace through the same API, covering both
the namespace & non-namespace needs."
0,"Volume's status have update to 'available' in database when volume extended successfully, but in 'notification-info' it still being 'extending'.

2014-07-28 14:45:40.449 3580 INFO cinder.volume.manager [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] volume d514c152-890b-47ae-ba87-2cb47a34fdcb: extended successfully
2014-07-28 14:45:40.513 3580 INFO oslo.messaging._drivers.impl_zmq [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] 'notifications-info' {'event_type': 'volume.resize.end',
 'message_id': 'b59eeffa-cb58-4930-81b0-8086fa92eba3',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-15 03:57:03',
             'display_name': u'vol22',
             'launched_at': '2014-07-15 03:57:12',
             'size': 3,
             'snapshot_id': None,
             'status': u'extending',
             'tenant_id': u'236be19c86d7461295b122f3ef16cf27',
             'user_id': u'06d7b23a60ab4341bbc8701fb121de26',
             'volume_id': u'd514c152-890b-47ae-ba87-2cb47a34fdcb',
             'volume_type': None},
 'priority': 'INFO',
 'publisher_id': 'volume.controller1',
 'request_id': u'req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0',
 'timestamp': '2014-07-28 06:45:40.512994'}"
1,"Take a case, when there is no probe present. In this case if we try to clear probes using following CLI -

#neutron-debug probe-clear

it should ideally give a message like - ""Nothing to delete/clear"" or ""No probe is present to clear""

But when I tries above command following is traced -"
1,"Unlike 'set_db_attribute()' and 'clear_db_attribute' methods which don't restrict our choice of desired attributes, flow managment methods such as 'add_flow', 'mod_flow' and 'delete_flows' internally make use of '_build_flow_expr_arr()' method, which validates and limits flow parameters. Beside from obvious inconsistency I see the following problems:
- If we misspell flow parameter it just will be ignored, and instead of exception (this is what we would expect) wrong flow will be defined.
- To add more flow options we have to keep hardcoding them in '_build_flow_expr_arr()'
My proposition is to use similar to 'set_db_attribute()' method approach - don't restrict which options we can use, just pack them and execute the command."
1,"A network marked as external can be used as a gateway for tenant routers, even though it's not necessarily marked as shared.
If the 'shared' attribute is changed from True to False for such a network you get an error:
Unable to reconfigure sharing settings for network sharetest. Multiple tenants are using it
This is clearly not the intention of the 'shared' field, so if there are only service ports on the network there is no reason to block changing it from shared to not shared."
0,"In https://review.openstack.org/78880 NeutronDbPluginV2TestCase._do_side_effect() was renamed to _fail_second_call().
But some cisco nexus test cases still use the old name."
0," so as to avoid confusion and not be listed in providers."""
1,"The following warning appears when deleting VPNaaS' vpn-site-connection object:
2013-12-15 13:57:04.274 6899 WARNING neutron.agent.linux.iptables_manager [-] Tried to remove rule that was not there: 'POSTROUTING' u'-s 10.35.214.0/24 -d 10.35.7.0/24 -m policy --dir out --pol ipsec -j ACCEPT ' True False"
1,"If you create a volume from an image and launch an instance from it, the instance fails to be created.
To recreate this bug:
1. Create a volume from an image
2. Launch an instance from the volume
The following error is thrown in n-cpu.log:
2014-09-09 22:33:47.037 ERROR nova.compute.manager [req-e17654a6-a58b-4760-a383-643dd054c691 demo demo] [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Instance failed to spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Traceback (most recent call last):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/compute/manager.py"", line 2171, in _build_resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] yield resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/compute/manager.py"", line 2050, in _build_and_run_instance
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] block_device_info=block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 446, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] admin_password, network_info, block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 298, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] vi = self._get_vm_config_info(instance, image_info, instance_name)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 276, in _get_vm_config_info
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] image_info.file_size_in_gb > instance.root_gb):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 92, in file_size_in_gb
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] return self.file_size / units.Gi
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] TypeError: unsupported operand type(s) for /: 'unicode' and 'int'
It appears that a simple conversion of the file_size to an int in vmware_images.py should fix this."
1,"Not certain where the bug is yet, but filing here to have a recheck target.

Noticed two separate patches fail gating in different jobs but on the same test (tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern):

http://logs.openstack.org/30/104730/1/gate/gate-tempest-dsvm-neutron/975e16b/
http://logs.openstack.org/28/102628/4/check/check-grenade-dsvm-partial-ncpu/d2e1673/

Common error in n-api seems to be:

2014-07-17 23:33:31.537 ERROR nova.api.openstack [req-cd950aba-e19c-4c6b-918b-bdfd41d2afce TestVolumeBootPattern-908565911 TestVolumeBootPattern-1581069975] Caught error: 'image_id'

which just started popping up today:

http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiQ2F1Z2h0IGVycm9yOiAnaW1hZ2VfaWQnXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDU2NDE3OTMwMzl9"
0,"diff --git a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
index e90c291..7df8ebb 100644
--- a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
+++ b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
@@ -91,7 +91,7 @@ class ExtendedSnapshotAttributesTest(test.TestCase):
         res = self._make_request(url)
         self.assertEqual(res.status_int, 200)
- for i, snapshot in enumerate(self._get_snapshots(res.body)):
+ for snapshot in self._get_snapshots(res.body):
             self.assertSnapshotAttributes(snapshot,
                                           project_id='fake',
                                           progress='0%')"
0,"There are some places that a target plugin's method is called in
'with context.session.begin(subtransaction=True)'.
This causes 'lock wait timeout' error potentially.
--- error is like:
OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') ...snip
---
For example say ml2 is target plugin. Ml2's mechanism drivers separates precommit and postcommit
method so that 'lock wait timeout' error does not occur.
But it is meaningless if ml2 is used under metaplugin."
0,"diff --git a/cinder/tests/brick/test_brick_linuxscsi.py b/cinder/tests/brick/test_brick_linuxscsi.py
index 47b73dc..e0ec010 100644
--- a/cinder/tests/brick/test_brick_linuxscsi.py
+++ b/cinder/tests/brick/test_brick_linuxscsi.py
@@ -101,15 +101,6 @@ class LinuxSCSITestCase(test.TestCase):
                    )
             return out, None
- def fake_execute2(*cmd, **kwargs):
- out = (""350002ac20398383d dm-3 3PARdata,VV\n""
- ""size=2.0G features='0' hwhandler='0' wp=rw\n""
- ""`-+- policy='round-robin 0' prio=-1 status=active\n""
- "" |- 0:0:0:1 sde 8:64 active undef running\n""
- "" `- 2:0:0:1 sdf 8:80 active undef running\n""
- )
- return out, None
-
         self.stubs.Set(self.linuxscsi, '_execute', fake_execute)
         info = self.linuxscsi.find_multipath_device('/dev/sde')"
1,"Live migration disconnects attached volumes on target. The action itself does not fail and the instance appears online on the target host, but the underlying iSCSi connection results closed on the destination host, resulting in failures in the guest when accessing the volume(s).

The issue can be traced to this call to the following self.volume_api.terminate_connection(...) in the manager:

https://github.com/openstack/nova/blob/c594340359e3d148b6c349c388f10327ac7b8bfa/nova/compute/manager.py#L4868

And in turn to this Cinder patch that introduced the regression:
https://review.openstack.org/#/c/76471/7/cinder/volume/manager.py

Verified on Hyper-V using Devstack with both LVM and Windows Cinder drivers."
0,"The Linux bridge (qbr) created by the OVS hybrid vif driver sends usefulness IGMP queries.
By default, Linux bridge activates IGMP snooping onto a switch.
In the case of the Linux bridge was created by the OVS hybrid vif driver, is usefulness.
Because this bridge is only here to be able to apply security groups through netfilter (OVS bridge doesn't use netfilter hooks) and so it contains only 2 interfaces: the VM tap and a side of the veth patch port.
IGMP snooping on a switch of 2 ports is unnecessarily.
Furthermore, the Linux bridge could (I didn't find why and when) send some IGMP queries when the IGMP snooping is activated.
This IGMP queries could unnecessarily pollute the network and the network equipemnts log."
1,"While doing more thorough integration tests, we discovered two problems:

1. Floating IP is not working correctly. When the packet for a floating ip reaches the router, the router drops it due to a security rule wrongly set.

2. Last minute modification introduced typo in the code that made MD server unreachable"
1,"In the nova/virt/libvirt/driver.py file, the '_live_snapshot' and '_swap_volume' methods have the following code flow
  xml = dom.XMLDesc(0)
  dom.undefine()
  dom.blockRebase()
  dom.defineXML(xml)
The reason for this is that 'blockRebase' requires the guest to be transient, so we must temporarily delete the persistent config and then re-create it later.
Unfortunately this code is using the wrong XML document when re-creating the persistent config. 'dom.XMLDesc(0)' will return the guest XML document based on the current guest state. Since the guest is running in both these cases, it will get getting the *live* XML instead of the persistent XML.
So these methods are deleting the persistent XML and replacing it with the live XML. These two different XML documents are not guaranteed to contain the same information.
As a second problem, it is not requesting inclusion of security information, so any SPICE/VNC password set in the persistent XML is getting lost
The fix is to replace
  dom.XMLDesc(0)
with
  dom.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE |
                               libvirt.VIR_DOMAIN_XML_SECURE)
in the _live_snapshot and _swap_volume functions."
0,"nova.compute.api should return Aggregate Objects, and they should be converted into the REST API expected results in the aggregates API extensions."
0,"Provisioning a VM using Neutron, with RA's being broadcast by upstream switches, the VM was not getting the packets.
Changing the rules manually to:
ip6tables -I neutron-openvswi-ie90990dd-0 1 -p ipv6-icmp -j ACCEPT
Allowed RA packets through."
1,"when updating quota for a resource, with 'force_update=false', an exception will be raised when the new value is less than the used quota values(used and reserved), but the exception information is not correct, which should be fixed."
1,"Because of the instance object lazy loading its possible to get into situations where the API code is half way through assembling data to return to the client when the instance disappears underneath it. We really need to ensure everything we will need is retreived up front so we have a consistent snapshot view of the instance
[req-5ca39eb3-c1d2-433b-8dac-1bf5f338ce1f ServersAdminNegativeV3Test-1453501114 ServersAdminNegativeV3Test-364813115] Unexpected exception in API method
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 410, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions return self._view_builder.show(req, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 268, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions _inst_fault = self._get_fault(request, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 214, in _get_fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions fault = instance.fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/objects/base.py"", line 67, in getter
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions self.obj_load_attr(name)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/objects/instance.py"", line 520, in obj_load_attr
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions expected_attrs=[attrname])
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/objects/base.py"", line 153, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions result = fn(cls, context, *args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/objects/instance.py"", line 310, in get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/db/api.py"", line 676, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1715, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions columns_to_join=columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1727, in _instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions raise exception.InstanceNotFound(instance_id=uuid)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions InstanceNotFound: Instance fcff276a-d410-4760-9b98-4014024b1353 could not be found.
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions
  http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-nova-v3-full-master/a278802/logs/screen-n-api.txt"
0,"There is attempt to create a new table securitygroups by 49f5e553f61f_ml2_security_groups.py while table already exists (created by 3cb5d900c5de_security_groups.py)
INFO [alembic.migration] Context impl MySQLImpl.
INFO [alembic.migration] Will assume non-transactional DDL.
INFO [alembic.migration] Running upgrade havana -> e197124d4b9, add unique constraint to members
INFO [alembic.migration] Running upgrade e197124d4b9 -> 1fcfc149aca4, Add a unique constraint on (agent_type, host) columns to prevent a race
condition when an agent entry is 'upserted'.
INFO [alembic.migration] Running upgrade 1fcfc149aca4 -> 50e86cb2637a, nsx_mappings
INFO [alembic.migration] Running upgrade 50e86cb2637a -> ed93525fd003, bigswitch_quota
INFO [alembic.migration] Running upgrade ed93525fd003 -> 49f5e553f61f, security_groups
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 143, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 199, in load_python_file
    module = load_module(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 55, in load_module
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 652, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/versions/49f5e553f61f_ml2_security_groups.py"", line 53, in upgrade
    sa.PrimaryKeyConstraint('id')
  File ""<string>"", line 7, in create_table
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 647, in create_table
    self._table(name, *columns, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 149, in create_table
    self._exec(schema.CreateTable(table))
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.OperationalError: (OperationalError) (1050, ""Table 'securitygroups' already exists"") '\nCREATE TABLE securitygroups (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tdescription VARCHAR(255), \n\tPRIMARY KEY (id)\n)\n\n' ()"
1," Various performance related issues with block IO to the mapped devices on linux hosts it became clear that the choices that were made in how the driver does host creation are not in line with best practices for E-Series arrays. The driver creates a host that is of type 'linux' as it appears from querying the proxy and type ""MPP/RDAC"" when viewing the host within Santricity itself. At first glance ""linux"" would appear reasonable, but in fact the correct default choice should have been LnxALUA (from the proxy) or Linux (DM-MP)."
0,"When nova is deployed with neutron, nova boot fails if quota are activated but not for ports

""Failing"" usecase:
In neutron.conf
[quotas]
quota_items = network,subnet,router,floatingip

neutron quota-show
+---------------------+-------+
| Field | Value |
+---------------------+-------+
| floatingip | 2 |
| network | 5 |
| router | 1 |
| security_group | 10 |
| security_group_rule | 100 |
| subnet | 4 |
+---------------------+-------+

nova boot fails because neutronv2 api expect neutron.show_quota(tenant_id=context.project_id)['quota'].get('port') to return an int but instead None is returned"
1,"ci overcloud jobs started failing between 5 and 8 AM GMT
Error from http://logs.openstack.org/73/79873/5/check-tripleo/check-tripleo-overcloud-precise/859d4d4/
var/log/upstart/neutron-openvswitch-agent.log ( on contoller and 1 compute)
[-] Error while processing VIF ports
Traceback (most recent call last):
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1230, in rpc_loop
    sync = self.process_network_ports(port_info)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1084, in process_network_ports
    devices_added_updated)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 984, in treat_devices_added_or_updated
    details['admin_state_up'])
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 893, in treat_vif_port
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 593, in port_bound
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 459, in provision_local_vlan
    (segmentation_id, ofports))
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 190, in mod_flow
    flow_str = _build_flow_expr_str(kwargs, 'mod')
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 546, in _build_flow_expr_str
    raise exceptions.InvalidInput(error_message=msg)
InvalidInput: Invalid input for operation: Cannot match priority on flow deletion or modification.
2014-03-21 05:20:56.329 7601 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
merge times and traceback details seem to match up with
https://review.openstack.org/#/c/58533/19
currently I'm testing a revert to see if it fixes things"
0,"The exception ""VirtualInterfaceMacAddressException"" message says
""5 attempts to create virtual interface with unique mac address failed"" when the number of attempts is configurable
by ""create_unique_mac_address_attempts"".
As proposed fix change to a generic message."
0," ie adding workers=N to glance-registry.conf will increase the number of registry processes."""
0,"After Nexenta iSCSI volume driver migrate volume provider_location of volume doesn't updated.
And also volume migrate method doesn't delete temporal snapshot on destination host."
0,"Conductor will return new Instance objects to older clients when they do actions on the InstanceList object. This is because we only consider the version of their InstanceList object, and happily fill it with objects newer than they can handle."
1,"The libvirt driver currently does not set the machine type for a KVM guest by default. When not specified, libvirt will use the newest one it knows about. Unfortunately, that can result in live migrations failing if your environment is using different versions of the host OS on compute noes as the destination node may not be able to support the machine type used when the VM was originally started.
A simple solution to this is to provide a new option which allows you to specify the default machine type on a per compute node basis (nova.conf option). By using this option, you can ensure that VMs are started with a machine type that will allow it to be live migrated to other nodes in the deployment."
0,"After adding the 2.0 rpc API in consoleauth manager (See [1]), the method check_token in the new proxy class for v2 lacks the return statement and causes all calls to verify a token from other components to fail.
[1] https://review.openstack.org/#/c/51731/"
0,"$ find . -name '*.*' | xargs grep self.assertEqual | grep True
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(response['x-image-meta-is_public'], 'True')
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py: self.assertEqual(data['image']['is_public'], True)
...
./tests/unit/test_swift_store.py: self.assertEqual(connection.insecure, True)
./tests/unit/test_swift_store.py: self.assertEquals(connection.snet, True)
./tests/unit/test_swift_store.py: self.assertEquals(connection.snet, True)
$ find . -name '*.*' | xargs grep self.assertTrue
./tests/unit/test_context_middleware.py: self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py: self.assertTrue(req.context.is_admin)
...
./tests/unit/test_context_middleware.py: self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py: self.assertTrue(req.context.read_only)
./tests/unit/test_context_middleware.py: self.assertTrue(req.context.is_admin)
The tests assertions need to be consistent: use assertTrue/assertFalse instead of assertEqual to test boolean values."
0,"The oslo has changed the common policy for a long time, using a Enforer class to replace the old check function .In order to sync the common policy to nova, we have to rewrite the nova policy and the related unittests."
1,"Fwaas service can't run in operation system without namespace feature although use_namespaces equals False
$ grep -r ""^use_namespaces ="" /etc/neutron/l3_agent.ini
use_namespaces = False
Bellow is error log:
2013-11-18 14:52:09.782 INFO neutron.agent.l3_agent [-] L3 agent started
2013-11-18 14:52:14.553 ERROR neutron.services.firewall.agents.l3reference.firewall_l3_agent [-] FWaaS RPC info call failed for '770d54af-2bb6-4233-8a39-1d5ad36fea59'.
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 213, in process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent self._process_router_add(ri)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 193, in _process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent ri.router['tenant_id'])
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 92, in _get_router_info_list_for_tenant
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent local_ns_list = root_ip.get_namespaces(self.root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 174, in get_namespaces
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent output = cls._execute('', 'netns', ('list',), root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/bak/openstack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent raise RuntimeError(m)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent RuntimeError:
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Exit code: 255
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stdout: ''
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stderr: 'Object ""netns"" is unknown, try ""ip help"".\n'
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent"
0,"If SQLAlchemy >= 0.8.3 is used, running of unit tests fails. The following error is printed to stderr multiple times:

DBError: (IntegrityError) constraint failed u'UPDATE instance_system_metadata SET updated_at=?, deleted_at=?, deleted=? WHERE instance_system_metadata.id = ?' ('2013- 11-19 10:37:44.378444', '2013-11-19 10:37:44.377819', 11, 11)

A few of our migrations change the type of deleted column from boolean to int. MySQL and SQLite don't have native boolean data type. SQLAlchemy uses int columns (e.g. in case of MySQL - tinyint) + CHECK constraint (something like CHECK (deleted in (0, 1))) to emulate boolean data type.

In our migrations when the type of column `deleted` is changed from boolean to int, the corresponding CHECK constraint is dropped too. But starting from SQLAlchemy version 0.8.3, those CHECK constraints aren't dropped anymore. So despite the fact that column deleted is of type int now, we still restrict its values to be either 0 or 1.

Migrations changing the data type of deleted columns rely on SQL rendered for CHECK constraints (e.g. https://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/migrate_repo/versions/152_change_type_of_deleted_column.py#L172). There was a patch in SQLAlchemy 0.8.3 release that slightly changed the way DDL statements are rendered ((http://docs.sqlalchemy.org/en/latest/changelog/changelog_08.html#change-487183f04e6da9aa27d8817bca9906d1)). Unfortunately, due to the fact that nova migrations depend on such implementation details, this change to SQLAlchemy broke our code.

We must fix our migrations to work properly with new SQLAlchemy versions too (0.8.3+)."
1,"The BigSwitch consistency watchdog is launched too soon during initialization.
It's launched before the server pool has added the servers so by the time the greenthread initializes, the pool is not fully populated."
0,"The rpc code from Oslo is quite out of date, and missing at least one bugfix of interest (bug 1189711)."
0,Functional tests to verify that a host can support VXLAN are needed. These can also test the logic in ovs_lib and compare the results of the functional test to the ovs_lib result.
0,"For bigswitch plugin, networkdhcpagentbindings and subnetroutes tables are not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_PLUGIN/logs/screen/screen-q-svc.log.gz
FOr bigswitch ML2 mech driver, neutron_ml2.consistencyhashes is not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_ML2/logs/screen/screen-q-svc.log.gz"
0,"The dhcp_options_enabled is currently set to False because the neutron extra_dhcp_opts patches have not landed yet. This should land in the Havana release of Neutron, as such dhcp_options_enabled option can then be defaulted to True or the conditional in the code can be removed (the desired approach). This is explained in https://review.openstack.org/#/c/31061/, and in Neutron: https://review.openstack.org/#/c/30441/ and https://review.openstack.org/#/c/30447/, for the python-neutronclient. So for Icehouse release of Nova this option should be removed and the code modified to support the dhcp_options."
1,An administrator can delete VM's running omthe VC without knowing that they belong to OpenStack
0,"In Cisco network_plugin unit tests, supported_extension_aliases grows while running unit tests.
It is because cisco network plugin extends supported_extension_aliases.
supported_exntension_aliases is a class attribute, so it will not reset to the original even after each unit test finished.
Message like this is annoying when debugging unit tests.
Note that in a real environment the plugin is initialized only once and there is no negative impact.
2014-03-15 08:35:39,867 INFO [neutron.manager] Loading core plugin: neutron.plugins.cisco.network_plugin.PluginV2
2014-03-15 08:35:39,928 INFO [neutron.plugins.openvswitch.ovs_neutron_plugin] Network VLAN ranges: {'physnet1': [(1000, 1100)]}
2014-03-15 08:35:39,957 INFO [neutron.manager] supported_extension_aliases=['credential', 'Cisco qos', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'ag
ent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', '
agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute',
 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute
', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_schedule
r', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sched
uler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sch
eduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_
opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-addres
s-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs']
2014-03-15 08:35:39,957 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,957 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958 INFO [neutron.api.extensions] Initializing extension manager.
2014-03-15 08:35:39,959 ERROR [neutron.api.extensions] Extension path 'unit/extensions' doesn't exist!
2014-03-15 08:35:39,959 INFO [neutron.api.extensions] Loading extension file: __init__.py
2014-03-15 08:35:39,959 INFO [neutron.api.extensions] Loading extension file: __init__.pyc
2014-03-15 08:35:39,959 INFO [neutron.api.extensions] Loading extension file: _credential_view.py
2014-03-15 08:35:39,959 INFO [neutron.api.extensions] Loading extension file: _qos_view.py"
1,"In miration 63afba73813_ovs_tunnelendpoints_id_unique mistake in usage drop_constraint parameters tablename and type instead of table_name and type_.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/63afba73813_ovs_tunnelendpoints_id_unique.py"", line 63, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)"
0,"The fix for https://bugs.launchpad.net/nova/+bug/1259796 break nova unit tests. The original test nova.tests.virt.libvirt.test_libvirt.LibvirtConnTestCase.test_cpu_features_bug_1217630 and more tests that parse xml are broken. Details here:
http://paste.openstack.org/show/60471/"
1,"nova-compute traces on first volume attach if open-iscsi is not running:
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] Traceback (most recent call last):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/compute/manager.py"", line 4142, in _attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] do_check_attach=False, do_driver_attach=True)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] ret_val = method(obj, context, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 248, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] connector)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] six.reraise(self.type_, self.value, self.tb)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 239, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] device_type=self['device_type'], encryption=encryption)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1224, in attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] disk_info)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1183, in volume_driver_method
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] return method(connection_info, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/contextlib.py"", line 34, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] self.gen.throw(type, value, traceback)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 212, in lock
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] yield sem
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/volume.py"", line 285, in connect_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] for ip, iqn in self._get_target_portals_from_iscsiadm_output(out):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] ValueError: too many values to unpack
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]
Thats because it runs ""iscsiadm -m discovery -t sendtargets -p"", which prints upon first run:
Loading iscsi modules: done
Starting iSCSI initiator service: done
Setting up iSCSI targets: unused
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
aka there is some garbage before the actual output that it looks for. this is due to /etc/iscsid.conf having configured to start the daemon:
from /etc/iscsid.conf:
# Default for upstream open-iscsi scripts (uncomment to activate).
iscsid.startup = /usr/sbin/rcopen-iscsi start"
1,"When I start Openstack following below steps, Openstack services can't be started without db2 connection:
1, start openstack services;
2, start db2 service.
I checked codes in session.py under nova/openstack/common/db/sqlalchemy, the root cause is db2 connection error code ""-30081"" isn't in conn_err_codes in _is_db_connection_error function, connection retrying codes are skipped against db2, in order to enable connection retrying function against db2, we need add db2 support in _is_db_connection_error function"
0,"Currently the PCI device object includes a lot of function like alloc/free/claim etc. However, the NovaObject should not be used this way, and it makes the PCI device object really different with other NovaObject implementation.
We should keep the PCI device object as simple data access, and keep those method to separated functions."
1,There are a number of places that do not contain translation support in the virt drivers
1,"The fix for bug 1231263 ( https://bugs.launchpad.net/nova/+bug/1231263 ) addressed not logging the clear-text password in the nova wsgi.py module for the adminPass attribute for the Server Change Password REST API, but this only addressed that specific attribute. Since Nova has support for the ability to add REST API Extensions (in the contrib directory), there could any number of other password-related attributes in the request/response body for those additional extensions.
Although it would not be possible to know all of the various sensitive attributes that these API's would pass in the request/response (the only way to totally eliminate the exposure would be to not log the request/response which is useful for debugging), I would like to propose a change similar to the one that was made in keystone (under https://bugs.launchpad.net/keystone/+bug/1166697) to mask the password in the log statement for any attribute that contains the ""password"" sub-string in it.
The change would in essence be to update the _SANITIZE_KEYS / _SANITIZE_PATTERNS lists in the nova/api/openstack/wsgi.py module to include a pattern for the ""password"" sub-string.
Also, for a slight performance benefit, it may be useful to put a check in to see if debug logging level is enabled around the debug statement that does the sanitize call (since the request/response bodies could be fairly large and wouldn't want to take the hit to do the pattern matches if debug isn't on)."
1,"GPFS 'copy_on_write' mode works only when creating a volume from an image, with image format being 'raw'.
When the image format is other than 'raw' like for example 'qcow2', the GPFS driver copies the image file to the volume by converting it to 'raw' format.
But, currently during this operation as well, GPFS driver is snap'ing the glance image, making it a clone parent, with no child associated.
Though this does not break any functionality, this is a unnecessary operation and needs to be avoided."
0,"Need to add session persistence support for NVP advanced LBaaS, as the present session persistence implementation on Edge is set by default."
1,"I'm using latest git with devstack on Ubuntu 12.04.
When I update the availability zone the attached metadata gets deleted. Steps to reproduce the problem:
 1) nova aggregate-create testagg #assuming that this creates a new metadata entry with the Id 26
 2) nova aggregate-set-metadata 26 x=y
 3) nova aggregate-update 26 testagg zone1
Now the availability zone is set, but the x=y metadata is lost."
1,"When creating a volume from image using the RBD backend for cinder, the image will be copied directly and not converted even if the image is not raw (e.g. qcow2). This causes mounting and booting to fail as the volume is not raw.
The problem, I think, is in clone_image in the RBD volume driver, which doesn't check the type of the source image and doesn't convert it. It is called from CreateVolumeFromSpecTask#_create_from_image. Implementations of clone_image for other drivers seem to do the conversion correctly."
0,"************* Module neutron.plugins.plumgrid.plumgrid_plugin.plumgrid_plugin
W: 33, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.metaplugin.meta_neutron_plugin
W: 30, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.db.api
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.nec_plugin
W: 36, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nec.common.config
W: 21, 0: Unused import rpc (unused-import)
--
************* Module neutron.plugins.ryu.ryu_neutron_plugin
W: 43, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ryu.agent.ryu_neutron_agent
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.plugin
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.agent.midonet_driver
W: 24, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.db.mlnx_db_v2
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.agent.eswitch_neutron_agent
W: 39, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.common.comm_utils
W: 23, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.mlnx_plugin
W: 37, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.hyperv.hyperv_neutron_plugin
W: 29, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nicira.check_nvp_config
W: 28, 0: Unused import nvp_cfg (unused-import)
************* Module neutron.plugins.nicira.NeutronServicePlugin
W: 32, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nicira.vshield.vcns_driver
W: 22, 0: Unused import nicira_cfg (unused-import)
--
************* Module neutron.plugins.nicira.NeutronPlugin
W: 48, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.plugin
W: 31, 0: Unused import quota_db (unused-import)
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.mech_arista.mechanism_arista
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.l2pop.mech_driver
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.cisco.network_db_v2
W: 24, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.plugins.linuxbridge.db.l2network_db_v2
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.linuxbridge.lb_neutron_plugin
W: 40, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
W: 50, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.agent.ovs_neutron_agent
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.ovs_neutron_plugin
W: 49, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.embrane.base_plugin
W: 33, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.cisco.db.network_db_v2
W: 29, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.tests.unit.nec.test_db
W: 25, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.nec.test_security_group
W: 25, 0: Unused import ndb (unused-import)
--
************* Module neutron.tests.unit.nec.test_ofc_manager
W: 24, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.ryu.test_defaults
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_plugin
W: 19, 0: Unused import ryu_models_v2 (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_db
W: 24, 0: Unused import ryu_models_v2 (unused-import)
W: 22, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_defaults
W: 19, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_mlnx_comm_utils
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.nicira.test_nvplib
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.openvswitch.test_ovs_defaults
W: 18, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.embrane.test_embrane_l3_plugin
W: 26, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_defaults
W: 25, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_neutron_plugin
W: 26, 0: Unused import config (unused-import)
--
************* Module install_venv
W: 29, 0: Unused import subprocess (unused-import)"
0,"Exceptions in nova/exception.py that get returned via REST api should have more user friendly (non-admin and non-dev) messages.
For example
 class InstanceRecreateNotSupported(Invalid):
     msg_fmt = _('Instance recreate is not implemented by this virt driver.')
Assuming that exception gets returned via the REST API, a user shouldn't have to know what a 'virt driver' is, that is a backend concept that we should be hiding.
Instead this exception should say something like 'Instance recreate is not supported'"
0,"Currently the VMware Nova Driver relies on the VM name in vCenter/ESX to match the UUID in Nova. The name can be easily edited by vCenter administrators and break Nova administration of VMs. A better solution should be found allowing the Nova Compute Driver for vSphere to look up VMs by a less volatile and publicly visible mechanism.
EDIT:
A fix would make the link between vSphere and Nova more solid and involve using a vSphere metadata value that cannot be easily edited. Currently the UUID is stored as an extra config metadata property inside vSphere (associated with the instance's virtual-machine) and
this value is not easy to accidentally change. That would make the link much more robust."
1,"Lines 132 & 133 of cinder/cinder/volume/drivers/solidfire.py are currently the following:
payload = json.dumps(command, ensure_ascii=False)
payload.encode('utf-8')
The string.encode() method used on line 133 returns an encoded string, but does not modify the payload string itself. Because of this, payload is never actually encoded to UTF-8. A suggested fix might be to modify the two lines to be the following:
payload = json.dumps(command, ensure_ascii=False).encode('utf-8')"
0,"For filesystem backed Cinder drivers like GPFS, a new volume file is created before the swap operation is invoked as part of the online volume migration sequence.

For this reason, the call to blockRebase from . _swap_volume (in nova.virt.libvirt.driver) should include the flag VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT to allow the newly created volume file to be written to."
0,LoopingCall has been renamed to FixedIntervalLoopingCall in https://review.openstack.org/#/c/26345/
1,"The minimum disk size of a backing VM is set to 1KB, but the VIM APIs fail the operation if the disk size is less than 1MB."
0,"I'm running Ubuntu 12.04 LTS x64 + OpenStack Havana with the following neutron package versions:

neutron-common 2013.2~rc3-0ubuntu1~cloud0
neutron-dhcp-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-l3-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-metadata-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-server 2013.2~rc3-0ubuntu1~cloud0
python-neutron 2013.2~rc3-0ubuntu1~cloud0
python-neutronclient 2.3.0-0ubuntu1~cloud0

When adding a router interface the following error message in /var/log/neutron/server.log:

2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp **args)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp port = self.get_port_from_device.get_port(device)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp AttributeError: 'function' object has no attribute 'get_port'
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp
2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.common [-] Returning exception 'function' object has no attribute 'get_port' to caller
2013-10-18 15:35:14.863 15675 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n **args)\n', ' File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch\n neutron_ctxt, version, method, namespace, **kwargs)\n', ' File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n result = getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up\n port = self.get_port_from_device.get_port(device)\n', ""AttributeError: 'function' object has no attribute 'get_port'\n""]"
1,"There are quite a few review comments on the EMC VNX Direct Driver:
https://review.openstack.org/#/c/73672
These issues need to be addressed right after the code is merged."
0,"Ex.
  NG: raise webob.exc.HTTPBadRequest('What?')
  OK: raise webob.exc.HTTPBadRequest(explanation='What?')
If there is no ``explanation=``, the message of the argument is
recognized as ``detail``."
1,"The ML2 bigswitch mechanism driver's create_port_postcommit() and update_port_postcommit() methods pass the PortContext to a _prepare_port_for_controller() method that modifies the port dictionary accessed as PortContext.current. This modified port dictionary is currently returned by ML2 as the result of the port create or update operation, but the changes do not get persisted in the DB. The fix for bug 1276391 is likely to result in these changes no longer being returned to the client. Mechanism drivers are not supposed to modify this port dictionary, except by calling PortContext.set_binding() from within bind_port(). It does not appear that this mechanism driver actually binds ports. If the port dictionary needs to be modified before being passed to the bigswitch controller, it should be copied first.
Also, the TestBigSwitchMechDriverPortsV2.test_port_vif_details() unit test asserts that the returned binding:vif_type is the modified value, so this test will also need to be changed or eliminated as part of this fix."
0,"libvirt.imagebackend.cache needs to be refactored to eliminate the callback to call_if_not_exists(). The only way it could meet all the requirements while remaining a callback is if the os.path.exists() check were removed from the function, which would then require path checks in each of the backend classes where prepare_template() is called. Ultimately, this is a lot more code than simply adding a backend-specific call to each class.

There's too much risk associated with such a refactor at such a late stage for Grizzly, so we'll look to tackle this for Havana.y"
1,"We had a temporary outage of Neutron, and many instances got stuck in this state. 'nova delete' on them does not work until nova-compute is forcibly restarted.
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | ci-overcloud-novacompute1-4q2dbhdklrkq |
| OS-EXT-SRV-ATTR:hypervisor_hostname | ci-overcloud-novacompute1-4q2dbhdklrkq.novalocal |
| OS-EXT-SRV-ATTR:instance_name | instance-00003f80 |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | deleting |
| OS-EXT-STS:vm_state | error |
| OS-SRV-USG:launched_at | 2014-03-05T03:54:49.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | |
| created | 2014-03-05T03:46:25Z |
| default-net network | 10.0.58.225 |
| fault | {""message"": ""Connection to neutron failed: Maximum attempts reached"", ""code"": 500, ""details"": "" File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 253, in decorated_function |
| | return function(self, context, *args, **kwargs) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2038, in terminate_instance |
| | do_terminate_instance(instance, bdms) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/lockutils.py\"", line 249, in inner |
| | return f(*args, **kwargs) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2036, in do_terminate_instance |
| | self._set_instance_error_state(context, instance['uuid']) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__ |
| | six.reraise(self.type_, self.value, self.tb) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2026, in do_terminate_instance |
| | reservations=reservations) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/hooks.py\"", line 103, in inner |
| | rv = f(*args, **kwargs) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2005, in _delete_instance |
| | user_id=user_id) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__ |
| | six.reraise(self.type_, self.value, self.tb) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1975, in _delete_instance |
| | self._shutdown_instance(context, db_inst, bdms) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1884, in _shutdown_instance |
| | network_info = self._get_instance_nw_info(context, instance) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 902, in _get_instance_nw_info |
| | instance) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/api.py\"", line 48, in wrapper |
| | res = f(self, context, *args, **kwargs) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 445, in get_instance_nw_info |
| | result = self._get_instance_nw_info(context, instance, networks) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 452, in _get_instance_nw_info |
| | nw_info = self._build_network_info_model(context, instance, networks) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 1010, in _build_network_info_model |
| | data = client.list_ports(**search_opts) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 112, in with_params |
| | ret = self.function(instance, *args, **kwargs) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 307, in list_ports |
| | **_params) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1251, in list |
| | for r in self._pagination(collection, path, **params): |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1264, in _pagination |
| | res = self.get(path, params=params) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1237, in get |
| | headers=headers, params=params) |
| | File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1229, in retry_request |
| | raise exceptions.ConnectionFailed(reason=_(\""Maximum attempts reached\"")) |
| | "", ""created"": ""2014-03-05T04:22:04Z""} |
| flavor | h1.large (872d8f61-c45a-45c3-87da-466d9f0f241b) |
| hostId | 0bab209cc6f26a8d5c4bc76e3da39d4fa68e5fefc6e5c0eada7a90d2 |
| id | ae9c75e3-51d2-43a3-8b20-34375b4c72d3 |
| image | tripleo-precise-1393812840.template.openstack.org (114e4b92-567e-4348-9ed8-e88281104208) |
| key_name | - |
| metadata | {} |
| name | tripleo-precise-tripleo-test-cloud-2125650.slave.openstack.org |
| os-extended-volumes:volumes_attached | [] |
| security_groups | default, default |
| status | ERROR |
| tenant_id | 64d2d3bc07084ef1accd4e3502909c77 |
| tripleo-bm-test network | 192.168.1.78 |
| updated | 2014-03-05T04:22:04Z |
| user_id | 35ef3ce265cb4a25b5303f3daa143f4e |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
0,"Hi! This is mostly just a wishlist request related to operational usability.

Recently I was investigating a report of changes to security group rules for a tenant and found that the AUDIT log messages captured during security group rule changes was, well, less than useful :)

Example:

2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Authorize security group ingress default

What would be more useful to know in this particular AUDIT log message would be something like this:

2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default added TCP ingress (22:22)

or:

2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default removed ICMP ingress (-1:-1)

Best,
-jay"
1,"The consistency watchdog in eventlet currently calls time.sleep which will block other greenthreads who are members of the same pool.
https://github.com/openstack/neutron/blob/288e3127440158f177beaae1972236def4916251/neutron/plugins/bigswitch/servermanager.py#L554
It should use eventlet.sleep so it yields to other members of the same pool."
0,"""tox -e docs"" shows the following error
2014-07-31 22:20:04.961 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] Exception in string format operation
2014-07-31 22:20:04.961 23265 TRACE nova.exception Traceback (most recent call last):
2014-07-31 22:20:04.961 23265 TRACE nova.exception File ""/Users/dims/openstack/nova/nova/exception.py"", line 118, in __init__
2014-07-31 22:20:04.961 23265 TRACE nova.exception message = self.msg_fmt % kwargs
2014-07-31 22:20:04.961 23265 TRACE nova.exception KeyError: u'flavor_id'
2014-07-31 22:20:04.961 23265 TRACE nova.exception
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] reason:
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] code: 404"
0,"HTTPS errors occur when attaching volumes with Huawei HVS iSCSI driver.
If the iSCSI initiator is not added to a host, the error will occurs.
But if the iSCSI initiator was added to a host already, no error occurs when attaching volumes to this host."
1,"Tonight I created a new router and attached an new subnet to it, then I tried to detach this subnet but failed. I debug the code and found ExtraRoute_db_mixin._get_extra_routes_by_router_id always returns all routes, because of the third line:
1. def _get_extra_routes_by_router_id(self, context, id):
2. query = context.session.query(RouterRoute)
3. query.filter(RouterRoute.router_id == id)
4. return self._make_extra_route_list(query)
   It should be:
          query = query.filter(RouterRoute.router_id == id)"
1,"I enable GPFS driver on one cinder volume node. After I unmount GPFS from this node, the GPFS driver can continue to handle creating volume request, so that the volume file is created on local disk, instead of GPFS NSD.
[root@zhaoqin-RHEL-GPFS1 gpfs]# mmumount fs1
Sun Oct 20 11:35:53 CDT 2013: mmumount: Unmounting file systems ...
[root@zhaoqin-RHEL-GPFS1 gpfs]# mount | grep gpfs
[root@zhaoqin-RHEL-GPFS1 gpfs]#
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder create 1
+---------------------+--------------------------------------+
| Property | Value |
+---------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| created_at | 2013-10-20T16:36:11.236108 |
| display_description | None |
| display_name | None |
| id | 78906f80-4809-49e9-aceb-b21d6fdf226b |
| metadata | {} |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | creating |
| volume_type | None |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 gpfs]# ls /gpfs/fs1
volume-78906f80-4809-49e9-aceb-b21d6fdf226b
Then, I attempt to remove this volume file using cinder command, but fails.
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder delete 78906f80-4809-49e9-aceb-b21d6fdf226b
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| 78906f80-4809-49e9-aceb-b21d6fdf226b | error_deleting | None | 1 | None | false | |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+"
0,"Currently, custom disk_bus configuration will be defaulted to virtio when
a user will try to power off + power on or hard reboot his instance.

It is happening since hard_reboot() doesn't consider the image_meta when constructing
the disk_info"
1,"In NEC Plugin, if a resource is in ERROR status and there is no corresponding resource on OpenFlow controller, the resource cannot be deleted by an API request. Unless critical reasons that resources cannot be deleted, resources should be able to be deleted from API."
0,"when boot vm, it can use '--nic <net-id=net-uuid,v4-fixed-ip=ip-addr, port-id=port-uuid>' to set network info, if it want to use ipv6,it hase to use port-id which hase a ipv6 address, I think it should can use '--nic net-id=net-uuid, fixed-ip=ip-addr' which include ipv4 and ipv6 address.Currently,nova already prevent that:
if address is not None and not utils.is_valid_ipv4(address):
   msg = _(""Invalid fixed IP address (%s)"") % address
    raise exc.HTTPBadRequest(explanation=msg)"
0,"Some class uses __metaclass__ for abc.ABCMeta.
six be used in general for python 3 compatibility.

For example

import abc
import six

six.add_metaclass(abc.ABCMeta)
class FooDriver:

    @abc.abstractmethod
    def bar():
        pass"
1,"When enabling notification with notification_driver = messaging, I get the following:
2014-02-03 14:20:41.152 ERROR oslo.messaging.notify._impl_messaging [-] Could not send notification to notifications. Payload={'priority': 'INFO', '_unique_id': 'da748b32fd144c25adc45ba5b393339d', 'event_type': 'compute.instance.create.end', 'timestamp': '2014-02-03 14:20:41.151419', 'publisher_id': 'compute.devstack', 'payload': {'node': u'devstack', 'state_description': '', 'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', 'created_at': '2014-02-03 14:20:33+00:00', 'access_ip_v6': None, 'disk_gb': 0, 'availability_zone': u'nova', 'terminated_at': '', 'ephemeral_gb': 0, 'instance_type_id': 6, 'instance_flavor_id': '42', 'image_name': u'cirros-0.3.1-x86_64-uec', 'host': u'devstack', 'fixed_ips': [FixedIP({'version': 4, 'floating_ips': [], 'label': u'private', 'meta': {}, 'address': u'10.0.0.2', 'type': u'fixed'})], 'user_id': u'6bcbc8f54d65473c9a0c4a55f64fb580', 'message': u'Success', 'deleted_at': '', 'reservation_id': u'r-jycyyveh', 'image_ref_url': u'http://162.209.87.220:9292/images/7b8d712a-fb31-43b8-8a05-a74d70fd8a11', 'memory_mb': 64, 'root_gb': 0, 'display_name': u'dwq', 'instance_type': 'm1.nano', 'tenant_id': u'cda1741ff4ef47f48fb3d9d76e302add', 'access_ip_v4': None, 'hostname': u'dwq', 'vcpus': 1, 'instance_id': '272c2ec6-bb98-4e84-9377-84c63c7a9ce9', 'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', 'state': u'active', 'image_meta': {u'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', u'container_format': u'ami', u'min_ram': u'0', u'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', u'disk_format': u'ami', u'min_disk': u'0', u'base_image_ref': u'7b8d712a-fb31-43b8-8a05-a74d70fd8a11'}, 'architecture': None, 'os_type': None, 'launched_at': '2014-02-03T14:20:41.070490', 'metadata': {}}, 'message_id': '03b2985a-6bcd-44ff-8303-29618d3c2b01'}
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/opt/stack/oslo.messaging/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging version=self.version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 93, in _send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging self._driver.send_notification(target, ctxt, message, version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging return self._send(target, ctxt, message, envelope=(version == 2.0))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 362, in _send
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging rpc_amqp.pack_context(msg, context)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py"", line 299, in pack_context
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging context_d = six.iteritems(context.to_dict())
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 484, in iteritems
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging return iter(getattr(d, _iteritems)(**kw))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'RequestContext' object has no attribute 'iteritems'
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging"
1,"when a cinder create using an --image-id as source for the volume happens in conjunction with inadequate space free on /tmp the create fails due to the conversion to bare format running out of disk space.
converting in /tmp seems to me to be a reasonable choice at first but not when you consider that image files in glance can be quite large. Doesn't cinder need some way to allow defining the conversion space to be somewhere other than /tmp. Many conversions will happen on VM's with small memory and disk footprints. Perhaps the conversion would better take place somewhere defined by the admin?"
1,"sha1 Nova at: 106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71
sha1 ironic at: 036c79e38f994121022a69a0bc76917e0048fd63
The ironic driver passes stats to nova's resource tracker in get_available_resources(). Sometimes these appear to get through to the database without modification, sometimes they seem to be replaced entirely by other stats generated by the resource tracker. The correct behaviour should be to combine the two.
As an example, the following query on the compute_nodes table in nova's database shows the contents for a tripleo system (all nodes are ironic):
mysql> select hypervisor_hostname, stats from compute_nodes;
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hypervisor_hostname | stats |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""} |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 0588e463-748a-4248-9110-6e18988cfa4e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""} |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""} |
| 4615c72a-9ea0-433e-8c52-308163112f89 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""} |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""} |
| 18794409-10d0-4946-9356-66cd5ab8472e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ed281d00-c16a-474d-8adb-ef525a9045fa | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
29 rows in set (0.00 sec)
The nodes with ironic stats have not had any instances created on them, the ones with resource tracker stats but no ironic stats have instances on them. This is shown by the following:
ironic node-list
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| uuid | instance_uuid | power_state | provision_state | maintenance |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | None | power off | None | False |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | a703ae98-2398-445b-91bf-48f368c5b82a | power on | active | False |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | None | power off | None | False |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | 308b1375-f8b8-42e5-bf20-143303975135 | power on | active | False |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | 4e8c16c4-d0f6-43b8-8c8f-d110d89ac16f | power on | active | False |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | 01cbe85a-1b12-40ca-ac99-5e7b062b1b50 | power on | active | False |
| 18794409-10d0-4946-9356-66cd5ab8472e | 4d764e7e-a123-4390-9c73-0c05a52f5f23 | power on | active | False |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | 87636b3b-b7b0-4e79-bd5f-5189eb5b1134 | power on | active | False |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | 03c06c2f-b606-4ddd-a205-573ea036b5b8 | power on | active | False |
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | 0189aa0e-5d05-4f6d-8a77-d5869b5c79f2 | power on | active | False |
| ed281d00-c16a-474d-8adb-ef525a9045fa | 11e03336-fb12-4448-a8de-b38f82d8b282 | power on | active | False |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | None | power off | None | False |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | ab08556b-ce7f-46f3-bb10-91771426d977 | power on | active | False |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | 4051cc97-bb28-4dbf-b018-d9a61e73a269 | power on | active | False |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | 855e24b6-f9ae-441b-8520-42d4df9f8703 | power on | active | False |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | 141e6055-3e2c-4376-aa8b-39ad5c63e8bc | power on | active | False |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | a62cab3f-56f7-47f2-813b-23a3255cad15 | power on | active | False |
| 0588e463-748a-4248-9110-6e18988cfa4e | 8fe18a1b-d753-4558-a9e1-b24f552f8e12 | power on | active | False |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | 839a77e1-2a9e-4db4-94d9-d68903fe028c | power on | active | False |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | 205b6dbf-5f75-4d2c-a3b0-1be45d93d493 | power on | active | False |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | 09d2ad8c-f813-4b1a-9501-530462240657 | power on | active | False |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | aa6c8024-ba80-4dc8-810c-c6fae57218c7 | power on | active | False |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | a76d7b2f-cf8c-4673-96e4-dcd9f2ea3bb5 | power on | active | False |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | 9a2904d2-d364-4084-a40a-3fbc65d90059 | power on | active | False |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | a86316c0-bdf0-4d6e-81ba-f44da63c906c | power on | active | False |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | 03b0d70d-42c4-426e-adab-09df927f30bb | power on | active | False |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | 808dbf2e-8f40-4d6d-9d7b-2c74e3194a6d | power on | active | False |
| 4615c72a-9ea0-433e-8c52-308163112f89 | None | power off | None | False |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | None | power off | None | False |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+"
1,"OFANeutronAgent.__init__() calls setup_rpc(), which spawns a thread to consume RPCs.
self.updated_ports, which is accessed by an RPC handler, port_update(), has not been initialized at this point."
1,"In nuage plugin when a router delete operation is performed, the user and group association is not deleted. This is a bug which is caused by a check for a nuage zone attached to the router even after the router is deleted.
This commit associated with this bug will fix the above issue."
1,mtu setting for network interface not vlan.
1,"On subnet deletion ml2 plugin deletes all the ports associated with this subnet and does not check if a port is associated with other subnets.
Steps to reproduce:
1) create a network with two subnets
2) create dhcp port for the network, port is associated with both subnets
3) delete one of the subnets
4) dhcp port is getting deleted
Though new dhcp port is created shortly I think it's not ok to delete existing dhcp port."
0,"""The Volumes API controller for the OpenStack API."" is inappropriate for SnapshotController."
0,"The following methods contained in glance/tests/functional/store_utils.py are not used anywhere:
. setup_swift,
. teardown_swift,
. get_swift_uri,
. setup_s3,
. teardown_s3,
. get_s3_uri
Let's get rid of it."
0,"PortLimit thrown when specifying used fixed ip
The nova/network/neutronv2/api.py _create_port will catch errors thrown from the neutron client. The code asserts that a 409 error is an over-quota error. However, this will hide other errors that may be occurring within the system.
The code is currently taking a calculated risk by assuming that all 409 errors that come through this code path will be over quota's. However, another high traffic code path is specifying a fixed_ip. If a user specifies a fixed ip address, this code will now incorrectly throw a PortLimitExceeded error. This leads the users to believe that they have run out of their quota limit.
Example exception (then wrapped exception):
NV-6FC38FD Neutron error creating port on network 7d360984-e12c-4bb3-819d-4b93c4ca4269
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 182, in _create_port
    port_id = port_client.create_port(port_req_body)['port']['id']
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
    ret = self.function(instance, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 308, in create_port
    return self.post(self.ports_path, body=body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1188, in post
    headers=headers, params=params)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1111, in do_request
    self._handle_fault_response(status_code, replybody)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1081, in _handle_fault_response
    exception_handler_v20(status_code, des_error_body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 93, in exception_handler_v20
    message=msg)
NeutronClientException: 409-{u'NeutronError': {u'message': u'Unable to complete operation for network 7d360984-e12c-4bb3-819d-4b93c4ca4269. The IP address 10.0.0.2 is in use.', u'type': u'IpAddressInUse', u'detail': u''}}
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1236, in _allocate_network_async
    dhcp_options=dhcp_options)
  File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 49, in wrapper
    res = f(self, context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 358, in allocate_for_instance
    LOG.exception(msg, port_id)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 335, in allocate_for_instance
    security_group_ids, available_macs, dhcp_opts))
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 191, in _create_port
    raise exception.PortLimitExceeded()
PortLimitExceeded: Maximum number of ports exceeded"
0,"When a quota-show for a different tenant is requested without admin permissions, you get the following message that is confusing and could be improved.
""Non-admin is not authorised to access quotas for another tenant"""
1,"In Ubuntu when you don't specify the hostname of the machine in /etc/hosts , and you try to run sudo <whatever> you get this message shown in your terminal:
sudo: unable to resolve host ....
But this shouldn't be causing nova compute to fail, but it does because of this function (nova/network/linux_net.py):
def device_exists(device):
    (_out, err) = _execute('ip', 'link', 'show', 'dev', device,
                                             check_exit_code=False, run_as_root=True)
    return not err
As you can see this function instead of checking the return code of the command ""ip link show"", it try to check whether anything was displayed to stderr or no, and guess what ? the message ""sudo: unable to resolve host ...."" is sent to stderr !!
I will go as far as to tell (And maybe i am wrong) that there is only one way and one preferable way to check wether a command succeed or failed is by checking the return code of this latter, and if there is some special case where a command is returning wrong return code, than it should be treated as it is, i.e. special case. Remember ""Special cases aren't special enough to break the rules."""
1,"If you suspend a rescued instance, resume returns it to the ACTIVE state rather than the RESCUED state."
0,"When neutron automatically deploys the database schema it does not ""stamp"" the version. According to neutron/db/migration/README this means that I have to stamp it to manually.

Neutron should automatically stamp the version to head when deploys the schema the first time. That way I don't have to determine what version I'm running so that I can do a schema migration."
0,"Followin stacktraces appeared after merging https://review.openstack.org/#/c/40381:
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp **args)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp context, obj_id['monitor_id'], obj_id['pool_id'], status)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp assoc = self._get_pool_health_monitor(context, id, pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp monitor_id=id, pool_id=pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] Returning exception Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c to caller
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] ['Traceback (most recent call last):\n', ' File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n **args)\n', ' File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch\n neutron_ctxt, version, method, namespace, **kwargs)\n', ' File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n result = getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status\n context, obj_id[\'monitor_id\'], obj_id[\'pool_id\'], status)\n', ' File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor\n assoc = self._get_pool_health_monitor(context, id, pool_id)\n', ' File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor\n monitor_id=id, pool_id=pool_id)\n', 'PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c\n']
2013-12-12 14:34:40.265 6522 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp **args)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 174, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp context, model_mapping[obj_type], obj_id, status)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 194, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp v_db = self._get_resource(context, model, id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 212, in _get_resource
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp raise loadbalancer.MemberNotFound(member_id=id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp MemberNotFound: Member 21f6ce73-fff8-4a7e-b798-527a4bb64b92 could not be found
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp"
1,"When creating an image in glance v1 and specifyig the location with a bad scheme you receive an HTTP 500. In this case 'http+swift"" is a bad scheme.
glance image-create --name bad-location --disk-format=vhd --container-format=ovf --location=""http+swift://bah""
Request returned failure status.
HTTPInternalServerError (HTTP 500)
2013-12-03 21:24:32.009 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866e
a33fa52945f0 54bc4959075343ff80f460b77e783a49] Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 389, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
    return self.app(env, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/paste/urlmap.py"", line 203, in __call__
    return app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 599, in __call__
    request, **action_args)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 618, in dispatch
    return method(*args, **kwargs)
  File ""/opt/stack/glance/glance/common/utils.py"", line 422, in wrapped
    return func(self, req, *args, **kwargs)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 754, in create
    image_meta = self._reserve(req, image_meta)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 488, in _reserve
    store = get_store_from_location(location)
  File ""/opt/stack/glance/glance/store/__init__.py"", line 263, in get_store_from_location
    loc = location.get_location_from_uri(uri)
  File ""/opt/stack/glance/glance/store/location.py"", line 73, in get_location_from_uri
    raise exception.UnknownScheme(scheme=pieces.scheme)
UnknownScheme: Unknown scheme 'http+swift' found in URI
2013-12-03 21:24:32.011 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866ea33fa52945f0 54bc4959075343ff80f460b77e783a49] localhost - - [03/Dec/2013 21:24:32] ""POST /v1/images HTTP/1.1"" 500 139 0.216952"
1,"In the event that an end user sets resize_confirm_window to something small (say 1 in this example) there is a possibility that the periodic task can run in nova/compute/manager.py:ComputeManager._finish_resize() after the migration has been updated but before the instances has been updated.
http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n3570
One possible solution to this would be to reverse the order, and update the instance before updating the migration, in which case the migration will get updated in _confirm_resize: http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n5018"
1,"It is no longer possible to create a volume from an image that does not have a checksum set.

https://github.com/openstack/cinder/commit/da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2 - this patch removes the default of None from the getattr call.

If this is intended it would be nice to see something more informative in the logs."
0,"Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues."
0,"The body validation is redundant for os-volume_upload_image, should remove the redundant KeyError for 'os-volume_upload_image'. In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
1,"If a volume gets deleted in the Ceph backend, and Cinder still knows about it in a 'deleting' state, cinder-volume on startup will try to clean it up. rbd.Image() will however raise a rbd.ImageNotFound exception in these cases. These should be disregarded so cinder can continue the delete if it's no longer in the backend.
Stack trace:
http://paste.openstack.org/show/53858/"
1,"when using iSCSI protocol to conenct IBM v7000, starting cinder-volume service maybe occur KeyError.
log:
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] CN-18C9F33 Error encountered during initialization of driver: StorwizeSVCDriver
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] 'license_compression_enclosures'
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 207, in init_host
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager self.driver.do_setup(ctxt)
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 153, in do_setup
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager self._helpers.compression_enabled()
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 54, in compression_enabled
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager if resp[key] != '0':
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager KeyError: 'license_compression_enclosures'
it should lack of license_compression_enclosures when compression is not enabled for the system. so need add checking in the code."
1,"patch 552693e4ad51291c8bfd28cd1939ed3609f6eeac (https://review.openstack.org/#/c/37933/) which added compute scheduler rpcapi version 2.9 does not provide any backwards compatibility, this should be done using 'self.can_send_version'
The result of this is, if compute is running grizzly and trunk is running with
[upgrade_levels]
scheduler=grizzly
in nova.conf
nova-conductor logs:
1e5a084a4838b5d258532645eacc'} from (pid=861) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
2013-09-03 04:32:36.347 DEBUG qpid.messaging.io.ops [-] SENT[4a35cb0]: SessionCompleted(commands=[0-16]) from (pid=861) write_op /usr/lib/python2.7/site-packages/qpid/messaging/driver.py:671
2013-09-03 04:32:36.351 ERROR nova.openstack.common.rpc.amqp [req-b596a82a-0be1-4b90-a93a-d05ae7011e0b demo demo] Exception during message handling
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp **args)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/conductor/manager.py"", line 745, in build_instances
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp legacy_bdm_in_spec=legacy_bdm)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/rpcapi.py"", line 112, in run_instance
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp legacy_bdm_in_spec=legacy_bdm_in_spec), version='2.9')
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 169, in cast
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp self._set_version(msg, version)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 72, in _set_version
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp raise rpc_common.RpcVersionCapError(version_cap=self.version_cap)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp RpcVersionCapError: Specified RPC version cap, 2.6, is too low
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp"
0,"Both the Big Switch Plugin and ML2 driver allow a server_timeout param in the initialization methods that can never be set by a user so it doesn't serve a purpose [1][2]. With those removed, the entire __init__ method of the base class can be removed as well."
1,"I was playing around with cloud-init. I wanted to use cloud init as substitute for the missing environment variables feature.
The basic idea is to define variables in the user data and inject the them before starting the service.
Following:
docker run -e ""MY_Variable=MyValue"" -d centos
would look like in openstack:
nova boot --image centos:latest --user-data ""MY_Variable=MyValue"" myinstance
Unfortunately does the metadata service not work inside of a docker container. After some testing I figured out that the reason for that is that a container uses as default gateway the docker network (docker ip address). The metadata service simple rejects the call since the IP address of docker container is not associated with the nova instance.
Note: The metadata service itself can be accessed (http://169.254.169.254) but it is not possible to access the actual data (http://169.254.169.254/2009-04-04 - Status 404)
I was able to work around the issue by simply changing the route inside the container:
# Hack: In order to receive data from the metadata service we must make sure we resolve the data via our nova network.
#
# A docker container in openstack has two NICs.
# - eth0 has a IP address on the docker0 bridge which is usually an e.g. 172.0.0.0 IP address.
# - pvnetXXXX is a IP address assigned by nova.
#
# Extract the NIC name of the nova network.
#
NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
while [ ""$NOVA_NIC"" == """" ] ; do
   echo ""Find nova NIC...""
   sleep 1
   NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
done
echo ""Device $NOVA_NIC found. Wait until ready.""
sleep 3
# Setup a network route to insure we use the nova network.
#
echo ""[INFO] Create default route for $NOVA_NIC. Gateway 10.0.0.1""
ip r r default via 10.0.0.1 dev $NOVA_NIC
# Shutdown eth0 since icps will fetch enabled enterface for streaming.
ip l set down dev eth0
This approach is obviously a poor solution since it has certain expectation of the network.
Another solution might be extend the docker driver to add a firewall rule which will masquerade requests on 169.254.169.254 with the actual nova network IP address
I third solution would need improvements in docker. If docker would have a network mode which allows to assign the IP from outside this issue would be solved. That of course is a just which must be accepted by the docker community.
Network Threads:
- https://groups.google.com/forum/#!topic/docker-dev/YfCeX8TBweA
Simple script to test metadata inside the container:
#!/bin/bash
status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
while [ $status != '200' ]; do
   echo ""Cannot access metadata, status: '$status', try again...""
   date # easier to see in docker logs that loop is still running.
   status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
   sleep 1
done
echo ""Yes we got some user data:""
curl http://169.254.169.254/2009-04-04"
1,"I try to create a image with the min_ram and min_disk as negative number, and create image operation is successful.
I use the glance api V2 to create image.
curl -i -X POST -H ""X-Auth-Token: $token"" -H ""content-type: application/json"" -d '{""name"": ""image-7"", ""type"": ""kernel"", ""foo"": ""bar"", ""disk_format"": ""aki"", ""container_format"": ""aki"", ""protected"": false, ""tags"": [""test"",""image""], ""visibility"": ""public"", ""min_ram"":-1, ""min_disk"":-1' \
http://192.168.0.100:9292/v2/images
glance image-show image-7
+------------------+--------------------------------------+
| Property | Value |
+------------------+--------------------------------------+
| Property 'foo' | bar |
| Property 'type' | kernel |
| container_format | aki |
| created_at | 2014-01-03T06:01:14 |
| deleted | False |
| disk_format | aki |
| id | aaa8e463-10aa-4518-a590-52feebaabcb5 |
| is_public | True |
| min_disk | -1 |
| min_ram | -1 |
| name | image-7 |
| owner | 25adaa8f93ee4199b6a362c45745231d |
| protected | False |
| status | queued |
| updated_at | 2014-01-03T06:01:14 |
+------------------+--------------------------------------+
I think glance need to check the value of min_ram and min_disk. They need to be greater than or equal 0."
0,The git repository oslo.vmware is now available: https://github.com/openstack/oslo.vmware/. It is time to consume it and get rid of the vmware folder in the glance folder.
0,"WMI root\virtualization namespace v1 (in Hyper-V) has been removed from Windows Server / Hyper-V Server 2012 R2, according to:
http://technet.microsoft.com/en-us/library/dn303411.aspx

Because of this, setting the force_hyperv_utils_v1 option on the Windows Server 2012 R2 nova compute agent's nova.conf will cause exceptions, since it will try to use the removed root\virtualization namespace v1.

Logs:
http://paste.openstack.org/show/87125/"
0,"When operating ""nova --os-compute-api-version 3 list-extensions"" with a patch https://review.openstack.org/#/c/91942/ ,
the output is the following.
So 'extensions', 'flavors' and 'ips' are not camelcase. This seems inconsistent.

$ nova --os-compute-api-version 3 list-extensions"
1,"I'm trying to set a custom policy.json for Neutron based on new roles I have defined.
In this task, I changed the ""default"" policy from ""rule: admin_or_owner"" to ""rule:admin_only"". After that, a bunch of operations stopped working, including, for instance, a regular user deleting a network or a router of his/her own project. Even with the policy for ""delete_network"" unchanged -- rule:admin_or_owner --, only the admin could delete a network.
I put a print statement in neutron.openstack.common.policy.check method to investigate what was happening. On the following lines you can compare the debug message in the logs with the actual content of the ""rule"" parameter passed to ""check"".
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_network'
(((rule:delete_network and rule:delete_network:provider:physical_network) and rule:delete_network:provider:network_type) and rule:delete_network:provider:segmentation_id)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_port'
(((((((rule:delete_port and rule:delete_port:binding:host_id) and rule:delete_port:allowed_address_pairs) and rule:delete_port:binding:vif_details) and rule:delete_port:binding:vif_ty
pe) and rule:delete_port:mac_address) and rule:delete_port:binding:profile) and rule:delete_port:fixed_ips)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_router'
(rule:delete_router and rule:delete_router:distributed)
- - -
DEBUG neutron.policy [...] Failed policy check for 'update_subnet'
(rule:update_subnet and rule:update_subnet:shared)
- in this case, there is no ""update_subnet:shared"" rule, but there is a ""subnets:shared:write"" rule (which doesn't seem to be used).
- - -
These are the tests I've implemented that got broken after changing the default rule. The update tests simply try to rename the resource.
test_delete_network_of_own_project
test_delete_port_own_project
test_add_router_interface_to_router_of_own_project*
test_delete_router_of_own_project
test_remove_router_interface_from_router_of_own_project*
test_update_router_of_own_project
test_update_shared_subnet_of_own_project
* these tests got broken because of this bug: https://bugs.launchpad.net/neutron/+bug/1356678."
1,"A mistake verification for 'readonly' .
Because the param of 'readonly' is bool , can not be checked in that way:
 readonly_flag = body['os-update_readonly_flag'].get('readonly')
        if not readonly_flag:
            msg = _(""Must specify readonly in request."")
            raise webob.exc.HTTPBadRequest(explanation=msg)
when the readonly == false , it will be throw HTTPBadRequest"
1,"When using the copy_from option, readers and writers can have different
speeds to respectively read and write.
A reader timeout will happen if the writer is slow and the writer is
being asked to write a lot. This is currently happening when using
the VMware store and copying from an HTTP server. The reader is reading
16MB which takes too long to upload to vCenter which is causing a
timeout from the HTTP server. The writer should be able to control the
size of the chunks being read when using copy_from: this way the writer
will write fast enough to not make the reader timeout.
This can be reproduced with the filesystem store and copying from http by adding sleep of 30/40 seconds in
https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L433 between each chunk read.
The chunk will become smaller and then zero.
This is simulating the effect of a slow writer."
0,"The unit test test_iptables_firewall.py has invalid MAC addresses in the code - 'ff:ff:ff:ff', an actual MAC has 6 octets and should be 'ff:ff:ff:ff:ff:ff'. The test still seems to run fine, but it should be cleaned-up."
1,"L2populationRpcCallBackTunnelMixin get_agent_ports yields (None, {}) for unknown networks.
it's useless for consumers."
0,"There are several unused reservation methods are not called directly, so it could be removed from the db API.
Here are the methods:
* reservation_create https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L695
* reservation_get https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L702
* reservation_get_all_by_project https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L707
* reservation_destroy https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L712"
0,"This function never used.
grep -r ""stub_out_rate_limiting"" *
cinder/tests/api/fakes.py:def stub_out_rate_limiting(stubs):"
0,"Based on the data in this wiki
https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
and discussions on the dev & operator mailing lists
  http://lists.openstack.org/pipermail/openstack-dev/2013-November/019767.html
  http://lists.openstack.org/pipermail/openstack-operators/2013-November/003748.html
We are able to increase the min required libvirt to 0.9.11
This will allow us to switch to using the (soon to be released) standalone libvirt python binding from PyPI, as well as removing some old compat code."
0,The XenAPI driver simply parses the Xen hypervisor capabilities to report the architecture type in the supported instances list. Unfortunately the Xen hypervisor uses a architecture name of 'x86_32' for i686 platforms which means it won't match the standard OS 'uname' reported architecture used by other drivers.
1,"nova add-fixed-ip <server> <network> fails with following in compute log:
2014-07-31 06:01:48.697 ERROR oslo.messaging.rpc.dispatcher [req-6e04dd42-1ebe-4aa3-a37b-e84bb60b3413 admin demo] Exception during message handling: 'dict' object has no attribute 'get_meta'
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 414, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher payload)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher return f(self, context, *args, **kw)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 327, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher kwargs['instance'], e, sys.exc_info())
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 315, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 3737, in add_fixed_ip_to_instance
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher self._inject_network_info(context, instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 4091, in _inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5383, in inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher self.firewall_driver.setup_basic_filtering(instance, nw_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 286, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher self.nwfilter.setup_basic_filtering(instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 123, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher if subnet.get_meta('dhcp_server'):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute 'get_meta'
same happens with remove-fixed-ip call"
1,"The Neutron metadata agent supports multiple workers to increase scalability. By default we create a single worker, and the various deployment tools (Should) increase to 4/8/16/x. Any number other than 1 would be a better default."
1,"I switched to use v2 api and noticed ""cinder list"" does not return value of bootable flag on volumes.

v2 api does show bootable.

I believe the error is that bootable attribute is missing from:
cinder.api.v2.views.volume.ViewBuilder.detail"
1,GPFS Driver does not limit clone depth for snapshots - there is a potential for snapshot clones to grow without bound regardless of the setting of gpfs_max_clone_depth config flag.
0,"Import StringIO
StringIO.StringIO()

should be :
Import six
six.StringIO() or six.BytesIO()

StringIO works for unicode
BytesIO works for bytes

For Python3 compatible."
0,"httplib.HTTPConnection is being assigned to a module-level variable in the Big Switch server manager module. This was a work-around to mock not correctly stopping patches targeting the normal library path.

This should be removed once the root cause of the patching issue is found.

https://github.com/openstack/neutron/blob/f64eacfd27220c180f6afc979087b35aa1385550/neutron/plugins/bigswitch/servermanager.py#L74"
1,"The init method in type_tunnel.TunnelRpcCallbackMixin expects extra parameters[1] not passed to the rest of the RPC mixins so it requires a custom __init__ method in the ML2 plugin[2].
1. https://github.com/openstack/neutron/blob/caf60442247ef0a13595db2691733f3b7589d24f/neutron/plugins/ml2/rpc.py#L54
2. https://github.com/openstack/neutron/blob/0755e7b379232285e434d827eeb854260a1db595/neutron/plugins/ml2/drivers/type_tunnel.py#L90"
1,"'DescribeInstances' in ec2 shows the wrong image message('ami-00000002') to the instance booting from a volume:
n781dba539a84:~ # euca-describe-instances i-0000003a
RESERVATION r-mdcwadip c1c092e1c88f4027aeb203d50d63135b
INSTANCE i-0000003a ami-00000002 wjvm1 running None (c1c092e1c88f4027aeb203d50d63135b, n781dba57c996) 0 m1.small 2014-02-10T07:00:40.000Z nova 20.20.16.12 ebs
BLOCKDEVICE /dev/vda vol-0000000d false
More info can be found here: http://paste.openstack.org/show/64087/
--------------
The codes in ec2utils.py:
def glance_id_to_id(context, glance_id):
    """"""Convert a glance id to an internal (db) id.""""""
    if glance_id is None:
        return
    try:
        return db.s3_image_get_by_uuid(context, glance_id)['id']
    except exception.NotFound:
        return db.s3_image_create(context, glance_id)['id']
------
The image_ref (as glance_id in this function) of the instance booting from a volume, will be """", not None.
The protected codes above can't take efforts."
0,"I found the code in l3_db.py is redundant,the code is below:
in function _update_router_gw_info:
        if gw_port and gw_port['network_id'] != network_id:
            fip_count = self.get_floatingips_count(context.elevated(),
                                                   {'router_id': [router_id]})
            if fip_count:
                raise l3.RouterExternalGatewayInUseByFloatingIp(
                    router_id=router_id, net_id=gw_port['network_id'])
            if gw_port and gw_port['network_id'] != network_id:
                with context.session.begin(subtransactions=True):
                    router.gw_port = None
                    context.session.add(router)
                self._core_plugin.delete_port(context.elevated(),
                                              gw_port['id'],
                                              l3_port_check=False)
it does not need the third if!"
1,"Current metaplugin didn't implements get_ports or get_port to proxy request for actual plugins.
so extension such as VIF extension will not be added the result of port."
1,"If you're trying to delete an instance which is out-of-sync such that it's missing a InstanceInfoCache entry, you'll will receive a traceback: http://paste.openstack.org/show/60685/
Delete in this case should be allowed so that you can cleanup these 'broken' instances.
The solution is to catch the InstanceInfoCacheNotFound exception (like we do with other NotFound exceptions around this code), and continue on."
1,"port_bound forgets to normalize port name.
it causes port_unbound fail to find Port for non ""tap"" prefixed ports."
0,"Server actions should raise ""Instance is locked"" while instance is locked by admin, the actions include:
1. reboot in v2 and v3
2. delete in v2 and v3
3. resize/confirm resize/revert resize in v2 and v3
4. shelve/unshelve/shelve_offload in v2 and v3
5. attach_volume/swap_volume/detach_volume in v2 and v3
6. rebuild in v2 and v3"
1,"target codes:

def _log_progress_if_required(left, last_log_time, virtual_size):
    if timeutils.is_older_than(last_log_time, PROGRESS_INTERVAL_SECONDS):
        last_log_time = timeutils.utcnow()
        complete_pct = float(virtual_size - left) / virtual_size * 100
        LOG.debug(_(""Sparse copy in progress, ""
                    ""%(complete_pct).2f%% complete. ""
                    ""%(left) bytes left to copy""), <====== here miss a conversion type like 's'
            {""complete_pct"": complete_pct, ""left"": left})
    return last_log_time"
0,"when instance resize, it will call finish_migration at last to create new instance and destroy old instance
if driver layer has problem in create new instance, the instance will be set to 'ERROR' state
we are able to use reset-state --active to reset the instance and use it
but the instance information is set to new instance and not reverted to old one"
0,"Input parameter 'context' of Check_*** methods(check_attach, check_detach, _check_metadata_properties, _check_volume_availability) in cinder/volume/api.py are not being used, so remove this 'context' parameter."
1,"In nova/virt/images.py, if qemu-img command fails or the image is missing, instead of returning en empty QemuImgInfo it should probably throw some exception to inform the caller of the situation instead of hiding the problem like it does today."
1,"metadata definition property show should handle type specific prefix
The metadata definitions API supports listing namespaces by resource type. For example, you can list only namespaces applicable to images by specifying OS::Glance::Image
The API also support showing namespace properties for a specific resource type. The API will automatically prepend any prefix specific to that resource type. For example, in the OS::Compute::VirtCPUTopology namespace, the properties will come back with ""hw_"" prepended.
However, if you then ask the API to show the property with the prefix, it will return a ""not found"" error. To actually see the details of the property, you have to know the base property (without the prefix). It would be nice if the API would attempt to auto-resolve any automatically prefixed properties when showing a property.
This is evident from the command line. If you look at the below interactions, you will see the namespaces listed, then limited to a particular resource type, then the properties shown for the namespace, and then a failure to show the property using the automatically prepended prefix.
* Apologize for formatting.
$ glance --os-image-api-version 2 md-namespace-list
+------------------------------------+
| namespace |
+------------------------------------+
| OS::Compute::VMware |
| OS::Compute::XenAPI |
| OS::Compute::Quota |
| OS::Compute::Libvirt |
| OS::Compute::Hypervisor |
| OS::Compute::Watchdog |
| OS::Compute::HostCapabilities |
| OS::Compute::Trust |
| OS::Compute::VirtCPUTopology |
| OS::Glance:CommonImageProperties |
| OS::Compute::RandomNumberGenerator |
+------------------------------------+
$ glance --os-image-api-version 2 md-namespace-list --resource-type OS::Glance::Image
+------------------------------+
| namespace |
+------------------------------+
| OS::Compute::VMware |
| OS::Compute::XenAPI |
| OS::Compute::Libvirt |
| OS::Compute::Hypervisor |
| OS::Compute::Watchdog |
| OS::Compute::VirtCPUTopology |
+------------------------------+
$ glance --os-image-api-version 2 md-namespace-show OS::Compute::VirtCPUTopology --resource-type OS::Glance::Image
+----------------------------+----------------------------------------------------------------------------------+
| Property | Value |
+----------------------------+----------------------------------------------------------------------------------+
| created_at | 2014-09-10T02:55:40Z |
| description | This provides the preferred socket/core/thread counts for the virtual CPU |
| | instance exposed to guests. This enables the ability to avoid hitting |
| | limitations on vCPU topologies that OS vendors place on their products. See |
| | also: http://git.openstack.org/cgit/openstack/nova-specs/tree/specs/juno/virt- |
| | driver-vcpu-topology.rst |
| display_name | Virtual CPU Topology |
| namespace | OS::Compute::VirtCPUTopology |
| owner | admin |
| properties | [""hw_cpu_cores"", ""hw_cpu_sockets"", ""hw_cpu_maxsockets"", ""hw_cpu_threads"", |
| | ""hw_cpu_maxcores"", ""hw_cpu_maxthreads""] |
| protected | True |
| resource_type_associations | [""OS::Glance::Image"", ""OS::Cinder::Volume"", ""OS::Nova::Flavor""] |
| schema | /v2/schemas/metadefs/namespace |
| visibility | public |
+----------------------------+----------------------------------------------------------------------------------+
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology hw_cpu_cores
Request returned failure status 404.
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  Could not find property hw_cpu_cores<br /><br />
 </body>
</html> (HTTP 404)
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology cpu_cores
+-------------+---------------------------------------------------+
| Property | Value |
+-------------+---------------------------------------------------+
| description | Preferred number of cores to expose to the guest. |
| name | cpu_cores |
| title | vCPU Cores |
| type | integer |
+-------------+---------------------------------------------------+"
0,"In keystone/openstack/common/db/sqlalchemy/test_migrations.py, line 30 imports `test` what is not exists. Oslo project contains appropriate file. Solution 颅鈥?synchronize keystone.openstack.common with corresponding file in Oslo.
The bug reproduces under nosetests."
1,"On a network, ""neutron port-list --network_id <net-id> --device_owner 'network:dhcp'"" shows there are two ports. This is checked from the mysql database:
mysql> select * from ports where tenant_id='abcd' and device_owner='network:dhcp' and network_id='7d2e3d47-396d-4867-a2b0-0311465a8454';
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| tenant_id | id | name | network_id | mac_address | admin_state_up | status | device_id | device_owner |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| abcd | 3d6a7627-6af9-4fb6-9cf6-591c1373d349 | | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:60:83:3f | 1 | ACTIVE | dhcp4fff1f08-9922-5c44-b6f8-fd9780f48512-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
| abcd | a4c0eb19-407e-4970-90a8-0128259fb048 | | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:e1:1b:8f | 1 | ACTIVE | dhcpce80c236-6a89-571d-970b-a1d4bb787827-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
2 rows in set (0.00 sec)
However, the ""neutron dhcp-agent-list-hosting-net 7d2e3d47-396d-4867-a2b0-0311465a8454 shows only one DHCP-server running.
This problem is observed in an environment with 4 nodes running dhcp-agents. The neutron API server and the DHCP agents are NOT running on the same node.
What happened is that error occurred when the DHCP server is being ""moved"" from DHCP-agentA running on nodeA to DHCP-agentB running on nodeB. The sequence is
  neutron dhcp-agent-network-remove <agentA> <net-id> (1)
  neutron dhcp-agent-network-add <agentB> <net-id> (2)
Right before or during the time step 1 is done, nodeA was rebooted. So the DHCP-port ws never removed. When nodeA came back and the DHCP-agent restarted, it didn't do the unplug of the dhcp port device. THe DHCP agent also failed to make the release_dhcp_port RPC call to the API-server to have the port deleted from mysql."
1,"The nova-networks extension is improperly converting cidr values to strings:
$ nova network-list
shows a list of ips for cidr:
[u'192.168.50.0', u'192.168.50.1', u'192.168.50.2', u'192.168.50.3',...]
This is possibly due to the extension being updated to use objects, but I don't recall seeing it previously, so it is possible something changed the way an ipnetwork is converted to json so that it now iterates through the object isntead of printing it as a string."
1,"In the following commit:

commit 46de2d1e2d0abd6fdcd4da13facaf3225c721f5e
Author: Rafi Khardalian <email address hidden>
Date: Sat Jan 26 09:02:19 2013 +0000

    Libvirt: Add support for live snapshots

    blueprint libvirt-live-snapshots

There was the following chunk of code

         snapshot_directory = CONF.libvirt_snapshots_directory
         fileutils.ensure_tree(snapshot_directory)
         with utils.tempdir(dir=snapshot_directory) as tmpdir:
             try:
                 out_path = os.path.join(tmpdir, snapshot_name)
- snapshot.extract(out_path, image_format)
+ if live_snapshot:
+ # NOTE (rmk): libvirt needs to be able to write to the
+ # temp directory, which is owned nova.
+ utils.execute('chmod', '777', tmpdir, run_as_root=True)
+ self._live_snapshot(virt_dom, disk_path, out_path,
+ image_format)
+ else:
+ snapshot.extract(out_path, image_format)

Making the temporary directory 777 does indeed give QEMU and libvirt permission to write there, because it gives every user on the whole system permission to write there. Yes, the directory name is unpredictable since it uses 'tempdir', this does not eliminate the security risk of making it world writable though.

This flaw is highlighted by the following public commit which makes the mode configurable, but still defaults to insecure 777."
0,wsgi.run_server no longer used
1,"This reported to me today by Maru. When an rpc worker is spawned as a sub-process, that happens after the nova notifier thread has already started.
eventlet.hubs.use_hub() is the call in neutron/openstack/common/service.py that causes all thread execution to stop.
From the event let documentation: ""Make sure to do this before the application starts doing any I/O! Calling use_hub completely eliminates the old hub, and any file descriptors or timers that it had been managing will be forgotten.""
Maru's observation is that this means that thread should not spawn before forking the process if they need to run in the child process. I agree.
The reason that threads spawn is that the plugin gets loaded prior to forking and the thread for the nova notifier is started in the __init__ method of a sub-class of the plugin."
1,"With DVR routers for FIP to work, there has to be a FIP Namespace that should be created and an IR Namespace.
But when a LBaaS VIP port is created on a Subnet that is part of the DVR router, the IR Namespace is not created for the VIP port since that port is not a Compute Port.
There are some corner cases here, when there are VMs in a node then the IR's for that subnet is already created and so the FIP namespace also will be created.
The issue will only be seen if the compute and the VIP ports are separated apart."
1,"Steps to reproduce:

1) create a security group that is referencing itself, for example
euca-create-group test2
euca-authorize test2 -P tcp -p 22 -s 0.0.0.0/0
euca-authorize test2 -P tcp -p 6666 -o test2

2) create any instance in this security group

euca-run-instance .. -g test2 ..

Expected result:
no stackstrace to be thrown
Actual result:
stacktrace with KeyError appears in the log. The iptable rules are created correctly and instance ends up in running state."
1,"    def add_extension(self, ext):
        # Do nothing if the extension doesn't check out
        if not self._check_extension(ext):
            return
        alias = ext.get_alias()
        LOG.info(_('Loaded extension: %s'), alias)
        if alias in self.extensions:
            raise exceptions.Error(_(""Found duplicate extension: %s"") %
                                   alias)
        self.extensions[alias] = ext"
1,"The zfssa driver is configured by giving a list of initiators and an initiator group
zfssa_initiator = iqn1,iqn2
zfssa_initiator_group = group1
The driver creates the initiator group on the zfssa storage appliance and adds the initiators to the group.
But currently the driver creates the initiators but will only add one initiator to the group. So the others will not be able to
do IO with the volume.
The problem seems to be in the zfssarest.py file add_to_initiatorgroup() function which replaces the existing initiators in the group with the new one instead of adding."
1,"When the L3 agent restarts, it does not preserve the link local addresses used for each router. For this reason, it has to reassign them and rewire everything. This is very disruptive to network connectivity. Connectivity should be preserved as much as possible.
This was an expected backlog item for the new DVR feature."
0,"http://logs.openstack.org/62/104262/7/gate/gate-nova-python27/3adf0e2/console.html
2014-07-25 16:27:18.188 | Traceback (most recent call last):
2014-07-25 16:27:18.188 | File ""nova/tests/db/test_db_api.py"", line 1236, in test_security_group_get_no_instances
2014-07-25 16:27:18.188 | security_group = db.security_group_get(self.ctxt, sid)
2014-07-25 16:27:18.188 | File ""nova/db/api.py"", line 1269, in security_group_get
2014-07-25 16:27:18.188 | columns_to_join)
2014-07-25 16:27:18.188 | File ""nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-07-25 16:27:18.188 | return f(*args, **kwargs)
2014-07-25 16:27:18.188 | File ""nova/db/sqlalchemy/api.py"", line 3668, in security_group_get
2014-07-25 16:27:18.188 | query = _security_group_get_query(context, project_only=True).\
2014-07-25 16:27:18.188 | File ""nova/db/sqlalchemy/api.py"", line 3635, in _security_group_get_query
2014-07-25 16:27:18.188 | read_deleted=read_deleted, project_only=project_only)
2014-07-25 16:27:18.189 | File ""nova/db/sqlalchemy/api.py"", line 237, in model_query
2014-07-25 16:27:18.189 | session = kwargs.get('session') or get_session(use_slave=use_slave)
2014-07-25 16:27:18.189 | File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 765, in __call__
2014-07-25 16:27:18.189 | return mock_method(*params, **named_params)
2014-07-25 16:27:18.189 | File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1002, in __call__
2014-07-25 16:27:18.189 | expected_method = self._VerifyMethodCall()
2014-07-25 16:27:18.189 | File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1049, in _VerifyMethodCall
2014-07-25 16:27:18.189 | expected = self._PopNextMethod()
2014-07-25 16:27:18.189 | File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1035, in _PopNextMethod
2014-07-25 16:27:18.189 | raise UnexpectedMethodCallError(self, None)
2014-07-25 16:27:18.189 | UnexpectedMethodCallError: Unexpected method call get_session.__call__(use_slave=False) -> None
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5leHBlY3RlZE1ldGhvZENhbGxFcnJvcjogVW5leHBlY3RlZCBtZXRob2QgY2FsbCBnZXRfc2Vzc2lvbi5fX2NhbGxfXyh1c2Vfc2xhdmU9RmFsc2UpIC0+IE5vbmVcIiBBTkQgcHJvamVjdDpcIm9wZW5zdGFjay9ub3ZhXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImN1c3RvbSIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJmcm9tIjoiMjAxNC0wNy0xM1QxNjo0MDo1NiswMDowMCIsInRvIjoiMjAxNC0wNy0yN1QxNjo0MDo1NiswMDowMCIsInVzZXJfaW50ZXJ2YWwiOiIwIn0sInN0YW1wIjoxNDA2NDc5MzkzMjc0LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
8 hits in 2 weeks, check and gate, all failures, looks like it started around 7/21."
0,"The body validation is redundant for createBackup, should remove the redundant KeyError for 'createBackup'.
 In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
1,"Intended operation:
---
1. create network and subnet
  neutron net-create net
  neutron subnet-create --name sub net 20.0.0.0/24
2. create port for dhcp server. It is intended to assign specific ip address.
 neutron port-create --name dhcp --device-id reserved_dhcp_port --fixed-ip ip_address=20.0.0.10,subnet_id=sub net -- --device_owner network:dhcp
3. then schedule dhcp-agent manually
 neutron dhcp-agent-network-add 275f7a3f-0251-485c-aea1-9913e173dd1e net
---
But now force scheduling occurs at port creation (2.). So it is not available to schedule manually (3.)."
1,When attempting a boot-from-volume with a volume size that doesn't match the disk size the compute manager will attempt to resize the volume (which fails). It's fine to press on with the given size.
1,"When trying to connect to a console with internal_access_path if the server does not respond by 200 we should raise an exception but the current code does not insure this case.
https://github.com/openstack/nova/blob/master/nova/console/websocketproxy.py#L68
The method 'find' return -1 on failure not False or 0"
1,"Nuage plugin stores a mapping of neutron and VSD id's for every neutron resource.
This bug is to remove the mapping to avoid storing redundant data and also avoid the
upgrade and out of sync issues."
0," which is set to true by default. It can be removed."""
1,"See https://bugs.launchpad.net/glance/+bug/1336168

Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and missed a variable in the RBD store."
1,"For some reason, when Solidfire driver tried to accept_transfer(), the backend failed to find the volume (the volume is there, but that is another issue to dig into), then the driver puked:
2014-08-14 14:54:48.350 36969 ERROR cinder.volume.drivers.solidfire [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Volume cbb95e17-f66e-4081-b3aa-521889ed436d, not found on SF Cluster.
2014-08-14 14:54:48.351 36969 ERROR cinder.openstack.common.rpc.amqp [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Exception during message handling
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp **args)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/manager.py"", line 731, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp new_project)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/drivers/solidfire.py"", line 750, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp 'volumeID': sf_vol['volumeID'],
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp TypeError: 'NoneType' object has no attribute '__getitem__'
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp
accept_transfer() should handle Volume Not Found like attach/detach_volume() do."
1,"When port flavor_access tempest tests into v3, I found if the remove_tenant_access and add_tenant_access called as non admin user an Unexpected API Error arose.
I look into code, find out the issue is that flavors.add_flavor_access and flavors.remove_flavor_access require admin privilege in DB level but the policy doesn't require it. and the exception is not catched. I think there is the same issue in nova v2 api.
I also think we should remove the privilege check in DB level, but it need more tests, and can be remove in another patch or blue-print.
the tempest log is:
2013-08-28 11:56:08.154 2220 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors/155214353/action
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-28 11:56:08.169 2220 INFO tempest.common.rest_client [-] Response Status: 500
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Wed, 28 Aug 2013 03:56:08 GMT', 'content-length': '202', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Body: {""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<class 'nova.exception.AdminRequired'>"", ""code"": 500}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314
the nova log is:
2013-08-28 11:56:08.165 DEBUG routes.middleware [-] Matched POST /flavors/155214353/action from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Route path: '/flavors/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>, 'id': u'155214353'} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Action: 'action', body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:927
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Calling method <bound method FlavorActionController._add_tenant_access of <nova.api.openstack.compute.plugins.v3.flavor_access.FlavorActionController object at 0x50f5f50>> from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:928
2013-08-28 11:56:08.167 ERROR nova.api.openstack.extensions [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Unexpected exception in API method
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/flavor_access.py"", line 176, in _add_tenant_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions flavors.add_flavor_access(id, tenant, context)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/flavors.py"", line 245, in add_flavor_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions return db.flavor_access_add(ctxt, flavorid, projectid)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/db/api.py"", line 1424, in flavor_access_add
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions return IMPL.flavor_access_add(context, flavor_id, project_id)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 106, in wrapper
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions nova.context.require_admin_context(args[0])
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/context.py"", line 195, in require_admin_context
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions raise exception.AdminRequired()
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions AdminRequired: User does not have admin privileges
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions
2013-08-28 11:56:08.167 INFO nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] HTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'>
2013-08-28 11:56:08.168 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Returning 500 to user: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'> from (pid=12748) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1188
2013-08-28 11:56:08.168 INFO nova.osapi_compute.wsgi.server [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] 192.168.1.101 ""POST /v3/flavors/155214353/action HTTP/1.1"" status: 500 len: 409 time: 0.0126910"
1,"When you try to update IPSec Policy lifetime, you get an error:
(neutron) vpn-ipsecpolicy-update ipsecpolicy --lifetime units=seconds,value=36001
Request Failed: internal server error while processing your request.
Meanwhile updating IKE Policy lifetime works well:
(neutron) vpn-ikepolicy-update ikepolicy --lifetime units=seconds,value=36001
Updated ikepolicy: ikepolicy"
0,"excutils.save_and_reraise_exception should be used when reraising an exception, as described in openstack.common.excutils.

I don't see any issues due to this but it is better to be fixed."
1,"cinder.openstack.common.exception has already been deleted.
It seems to be a leak in 'Change-Id:
I6f46f90bd74cc26fc01667e467e3dab38037eec3'."
1,"2013-10-09 18:43:35.867 5236 ERROR nova.compute.manager [req-a8a83c04-03b4-4579-897f-a9825d17231e 657da0d8cf4440eca4a4ebad6fa1248a 0f7a0b0633c64e8888e6324bfe88dc16] [instance: a278d507-2112-40c2-912b-84b104300ed7] Error: Cannot resize a VHD to a smaller size

It would be helpful to debug the original and newer size in this case."
1,The conductor object_action() method does a shallow copy of the instance in order to do change tracking after the method is called. This is not sufficient as complex types like dicts and lists will not be copied and then the change detection logic will think those fields have not changed.
1,Typo in the metering agent when there is an exception when it invokes a driver method.
1,"HTTP 413 is supposed to mean (per RFC2616) that the request entity was too large. E.g., if you send an enormous body with the request. That is not at all how it is being used in the server resize request example below. The nova/api/openstack/compute/servers.py is coded to return 413 for QuotaError and PortLimitExceeded on create as well as for QuotaError on resize, and there may be other places 413 is being returned inappropriately.
POST /v2/6ce8fae0510349dcbf9b3965f7a20061/servers/8ebaabfc-9018-4ac1-afc6-630aee8a8ae3/action
Request body: { ""resize"": {
            ""flavor"": {
              ""vcpus"": 1,
              ""ram"": 9999999999,
              ""disk"": 20
          }}}
Response: HTTP 413 (Request Entity Too Large)
Response body:
{
overLimit: {
message: ""Quota exceeded for ram: Requested 1410063359, but already used 6144 of 8000000 ram""
code: 413
retryAfter: ""0""
}
-
}"
0,"    I have successfully compiled libvirt 1.2.7 and qemu 2.1.0 but had some troubles with nova-compute. It appears like libvirt is throwing back an error if a nwfilter is already present.

Here is my debug log:
2014-08-22 08:22:25.032 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-nodhcp' already exists with uuid 59970732-ca52-4521-ba0c-d001049d8460)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.033 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-base' already exists with uuid b5aa80ad-ea4a-4633-84ac-442c9270a143)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.034 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-vpn' already exists with uuid b61eb708-a9a5-4a16-8787-cdc58310babc)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239

Here is the original function:
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        self._conn.nwfilterDefineXML(xml)

And here is the ""patched"" function"":
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        try:
            self._conn.nwfilterDefineXML(xml)
        except Exception, e:
            LOG.debug(_('nwfilterDefineXML may have failed with (%s)!'), e)

I'm not a python expert but I think that patch could be adapted to raise an error ONLY if the nwfilter rule doesn't already exist."
1,"The SDN-VE controller expects the port-id information for removing router interfaces. Furthermore, the controller requires the use of string 'null' when the external gateway info is present and set to {} in Neutron update_router requests. The controller cannot accept "":"" as part of the incoming requests and therefore the requests need to be manipulated to replace the "":"" with ""_"" before sending requests to the controller."
0,"The test_n1kv_plugin unit tests have segment IDs that overlap with segment IDs in test_n1kv_db.
https://github.com/openstack/neutron/blob/ba242d91c54b36628e03332a91adfef40e32ac32/neutron/tests/unit/cisco/n1kv/test_n1kv_plugin.py#L259
https://github.com/openstack/neutron/blob/64c0d5d272fd48c84b044a771ca6122700b69a29/neutron/tests/unit/cisco/n1kv/test_n1kv_db.py#L47
This causes a unit test for segment uniqueness to fail when they end up executing simultaneously. (e.g. http://logs.openstack.org/85/54485/10/check/gate-neutron-python27/e18c12f/console.html)"
0,In PLUMgrid plugin drivers are not dynamically loaded. They should be loaded by configuration as the rest of the plugins
0,"The test_disabled test for the IPv6 utilities leaves a module level flag set to disabled, which will break any following tests that depend on IPv6 being enabled (e.g. the iptables tests).
http://logs.openstack.org/49/113749/8/check/gate-neutron-python27/8787946/console.html
To reproduce, you can run these two tests serially.
neutron.tests.unit.test_ipv6.TestIsEnabled.test_disabled
neutron.tests.unit.test_security_groups_rpc.TestSecurityGroupAgentEnhancedRpcWithIptables.test_prepare_remove_port"
1,"OVS lib propose a deferred apply methods to save system calls to 'ovs-ofctl' binaries ('ovs-ofctl' can apply flow from file or stdin if file is '-').
This method use a dict for 'add', mod' or 'del' flow actions that contain a concatenated string flows. This dict is purge after all flows are applied at the end of 'deferred_apply_off' method.
If another call is made on that dict during the 'deferred_apply_off', some flows could be deleted a the end when they have not been applied.
I can see that on ML2 plugin with l2-pop mechanism driver. If I delete more than one port at a time, some flooding flow rules could be not deleted on the br-tun bridge."
0,"Currently, when a volume is created on the 3PAR, metadata is added onto 鈥渃omment鈥?section onto the 3PAR which can be modified by users directly from management console. Hence for any new driver related tasks, to avoid manipulation from user, the 3PAR driver should use optional key value pairs for adding metadata. Comments section would be mainly used for informational purposes. All metadata keys should start with a custom prefix for ease of use."
1,"The Cinder storage driver is currently depending on Hyper-V WMI features for VHD conversions.
This must be urgently removed and replaced with Win32 calls available on any supported Windows SKU as otherwise CI tests cannot be performed and the Windows Storage Server edition cannot be supported."
1,"After an instance has been in the rescue state for some time, a periodic task triggers to unrescue the instances:
_poll_rescued_instances
File nova/notifications.py info_from_instance
  instance_type = flavors.extract_flavor(instance_ref)
File ""nova/compute/flavors.py"" in extract_flavor
  instance_type[key] = type_fn(sys_meta[type_key])
KeyError: 'instance_type_memory_mb'
This then continues to happen on every run of the periodic task, and starts to fill up the DB with instance faults."
1,"As the title says, if there are 2 different ovs ports with the same external_ids:iface-id field (which is the port_id), when at least one of them is managed by the ovs agent, it might fail finding the correct one if they are not connected to the same bridge.
Steps to reproduce:
1. Create a router with an internal port to some Neutron network
2. Find the port in 'ovs-vsctl show'
3. Use the following command to find the port_id in ovs: sudo ovs-vsctl --columns=external_ids list Interface <port_name>
4. Use the following commands to create a new port with the same field in a new bridge:
 sudo ovs-vsctl add-br a
 sudo ip link add dummy12312312 type dummy
 sudo ovs-vsctl add-port br-a dummy12312312
 sudo ovs-vsctl set Interface dummy12312312 external_ids:iface-id=""<port_id>"" # port_id was obtained in point 3.
5. Restart the ovs agent.
At this point the ovs agent's log should show ""Port: dummy12312312 is on br-a, not on br-int"".
Expected result: ovs agent should know to iterate though the options and find the correct port in the correct bridge."
1,There is a missing RPC call from Mellanox neutron agent to neutron server for update device_up once device configuration is applied.
1,"correct the message string: %(volumeid) -->%(volumeid)s in cinder/volume/drivers/prophetstor/dpl_fc.py
msg = _('Volume %(volumeid) failed to send assign command, '
                    'ret: %(status)s output: %(output)s') % \
                {'volumeid': volumeid, 'status': ret, 'output': output}"
1,"I found the following error in q-lbaas log after creating a vip on a pool.
2013-09-04 21:41:59.830 10678 DEBUG neutron.openstack.common.periodic_task [-] Running periodic task LbaasAgentManager.collect_stats run_periodic_tasks /opt/stack/neutron/neutron/openstack/common/periodic_task.py:176
2013-09-04 21:41:59.831 10678 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Error upating stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 137, in collect_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager stats = driver.get_stats(pool_id)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 168, in get_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager pool_stats['members'] = self._get_servers_stats(parsed_stats)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 188, in _get_servers_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager if stats['type'] == TYPE_SERVER_RESPONSE:
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager KeyError: 'type'
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager"
1,"Set vmware_volume_folder=/cinder-volumes in cinder.conf
> cinder create 1
fails with following error:
Unable to find suitable datastore for volume of size: 1 GB under host: (obj){
   value = ""host-361""
   _type = ""HostSystem""
 }. More details: Server raised fault: 'The name '/cinder-volumes' already exists"
0,"I'm using the nova docker driver on RHEL 3.10.11-1.el6.x86_64 (rebuilt kernel). Based on the format of /proc/mounts on RHEL, the cgroup devices path cannot be found.
On my box the line in /proc/mounts for cgroups looks like this:
none /sys/fs/cgroup cgroup rw,relatime,perf_event,blkio,net_cls,freezer,devices,memory,cpuacct,cpu,cpuset,clone_children 0 0
In the docker driver the method which searches /proc/mounts for the cgroup path looks like this:
    def _find_cgroup_devices_path(self):
        for ln in open('/proc/mounts'):
            if ln.startswith('cgroup ') and 'devices' in ln:
                return ln.split(' ')[1]
Therefore the method cannot find my cgroup path. I hacked around this with a 1 LOC change to the _find_cgroup_devices_path method which looks for 'cgroup' as the 3rd item in the line split:
if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
The update method in its entirety looks like:
169 def _find_cgroup_devices_path(self):
170 for ln in open('/proc/mounts'):
171 if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
172 return ln.split(' ')[1]
Based on the format in /proc/mounts on my ubuntu box, this change *should* work on ubuntu as well as rhel.
I did read that docker is only supported with devstack + unbuntu, so I realize this defect may get deferred or even closed. However I wanted to surface it as I believe future efforts of openstack + docker will need to consider non-ubuntu + devstack envs."
1,"(NEC Neutron third party CI blocking issue)
When multiple delete_port are run in parallel, nec plugin detects ResourceClosedError from sqlalchemy. The error itself can occur when a port row is deleted from ports table, but the problem is it occurs even after retrying db_plugin.delete_port by restarting a new transaction in plugin.delete_port().
At now it seems only nec plugin hits this problem, but potentially other plugins can hit this issue.
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.
After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout.
One example is : http://133.242.19.163:8000/neutron-ci-logs/Neutron_Master/917/logs/devstack/q-svc-filtered.txt.gz
2014-02-18 18:38:41.771 16025 ERROR neutron.api.v2.resource [-] delete failed
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/common/log.py"", line 43, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource data['exc'] = unicode(e)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/openstack/common/excutils.py"", line 68, in __exit__
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource six.reraise(self.type_, self.value, self.tb)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/common/log.py"", line 40, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource ret = method(*args, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 419, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource super(NECPluginV2, self).delete_network(context, id)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 968, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource for p in ports)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 967, in <genexpr>
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource only_auto_del = all(p['device_owner'] in AUTO_DELETE_PORT_OWNERS
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/loading.py"", line 65, in instances
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource fetch = cursor.fetchall()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 752, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource self.cursor, self.context)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1027, in _handle_dbapi_exception
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource util.reraise(*exc_info)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 746, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource l = self.process_rows(self._fetchall_impl())
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 715, in _fetchall_impl
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource self._non_result()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 720, in _non_result
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource ""This result object does not return rows. ""
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource ResourceClosedError: This result object does not return rows. It has been closed automatically.
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource
2014-02-18 18:38:41.772 16025 DEBUG neutron.wsgi [-] Data Request Failed: internal server error while processing your request. type is <type 'unicode'> _to_xml_node /opt/stack/neutron/neutron/wsgi.py:533
2014-02-18 18:38:41.772 16025 INFO neutron.wsgi [-] 10.56.45.201 - - [18/Feb/2014 18:38:41] ""DELETE /v2.0/networks/451384e4-a4f2-4558-8da0-40dc60f1a17f HTTP/1.1"" 500 517 103.278109"
1,"When deleting a volume that was snapshot, the volume is not properly deleted.
Steps to recreate bug:
1. Create a volume
2. Attach volume to a running instance.
3. Take an online snapshot of the volume.
Note that the active volume used by the instance is now switched to volume-<uuid>.<snapshot-uuid>.
4. Delete the snapshot.
The snapshot volume will be rebased, and the ""active"" volume as seen in volume-<uuid>.info is now set to
volume-<uuid>.<snapshot-uuid>.
Under the glusterfs mount point, there are now only 2 files related to the volume:
  - volume-<uuid>.info
  - volume-<uuid>.<snapshot-uuid>
The original volume base volume-<uuid> is deleted after the rebase.
5. Detach the volume from the running instance.
6. Delete the volume.
Under the glusterfs mount point, the volume-<uuid>.<snapshot-uuid> file is not deleted as expected."
0,"The method create() in /nova/virt/libvirt/ imagebackend.py has two unused
variable: old_format and features.These should be removed."
1,"When neutron Openswitch agent runs with l2pop ON, it is not removing flows from table 0 of br-tun, those flow sthat carry cleaned up tunnel ports by l2-pop.
Impact of this bug: Explosion of flow rules in br-tun table 0 as tunnel ports come-in and go-away in a scaled environment"
0,The sync code that syncs the operational status to the db calls out to nsx within a db transaction which can cause deadlock if another request comes in and eventlet context switches it.
0, it would raise FirewallRuleInUse excpetion in DB ops
0,"neutron/agent/linux/ovs_lib:ofctl_arg_supported executes an ovs-ofctl command and catches the general Exception, concluding that the feature is not supported. However, unexpected exceptions may be raised, swallowed and never logged. So, for example, if an OVS agent starts up and arp_responder is True, then the capabilities test for it may fail in an unexpected way. The result is that arp_responder will not be supported, the agent will start and the exception will not be printed, masking a potential bug.

Additionally, the capability test should be moved to the offline sanity command."
0,"In order to support NVP advanced lbaas/firewall UI, it is needed to return the router_id attribute when getting vip/firewall.
Also, it is needed to refactor duplicate code for delete_vip/delete_pool/delete_health_monitor"
0,"The Hyper-V agent raises:

2014-08-06 10:42:37.096 2052 ERROR neutron.openstack.common.rpc.amqp [req-46340a1a-9143-45c9-b645-2612d41f20a6 None] Exception during message handling
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\amqp.py"", line 462, in _process_data
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp **args)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\dispatcher.py"", line 178, in dispatch
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp raise rpc_common.UnsupportedRpcVersion(version=version)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp UnsupportedRpcVersion: Specified RPC version, 1.1, not supported by this endpoint.
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp

The issue does not affect functionality, but it creates a lot of noise in the logs since the error is logged at each iteration."
0,"https://review.openstack.org/65034
commit c04785e0ced18ebab6bada1d3961c1394c541a69
Author: Itsuro Oda <email address hidden>
Date: Mon Jan 6 15:03:14 2014 +0900
    Make metaplugin be used with a router service plugin
    ""l3_plugin_list"" configuration parameter of the metaplugin is permitted
    blank now.
    If ""l3_plugin_list"" is blank, router extension and extensions which extend
    the router extension don't be included in ""supported-extension-aliases"" of
    the metaplugin.
    This makes the metaplugin be able to be used with a router service plugin.
    Note that if ""l3_plugin_list"" is not blank, a router service plugin must
    not be specified, otherwise the error of the bug report still occurs.
    This patch removes some router extension related meaningless codes also.
    (e.g. external-net extension belongs to L2 functionality and be handled
     by core plugins properly.)
    Closes-bug: 1266347
    DocImpact
    Change-Id: I0454bc0a4bd7eda5dad18b0538fb7baebe0b9f91"
1,"SNAT name space is being hosted on all nodes though agent_mode is set as dvr

Here are the steps followed.

1. one service node with agent_mode=snat
2. Two compute nodes with agent_mode=dvr
3. Create an external network. Subnet within it.
4. Create a private network & subnet(sub1) within it.
5 .Create a Distributed Router(DVR)
6. Set the external network as gateway to the DVR.
7. Add the ptivate subnet (sub1) as interface to the DVR.
8. Since the service node has the agent_mode as DVR-snat, the snat namespace will sit on this service node. But the snat namespace is hosted on all 3 nodes(service node with agent_mode=dvr_snat & Compute nodes with agent_mode=dvr)

Expected behavior:
snat name space should sit only on the node with agent_mode=dvr_snat."
0,The name of the synchronized queue class is queue instead of Queue in Python3.
0,"The guru-meditation report fails on Hyper-V due to missing signal handling.
This is a blocking issue on Windows.
http://64.119.130.115/74060/3/Hyper-V_logs/hv-compute1/nova-console.log.gz
Traceback (most recent call last):
  File ""c:\OpenStack\virtualenv\Scripts\nova-compute-script.py"", line 9, in <module>
    load_entry_point('nova==2014.1.dev954.g3a611cc', 'console_scripts', 'nova-compute')()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 353, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2321, in load_entry_point
    return ep.load()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2048, in load
    entry = __import__(self.module_name, globals(),globals(), ['__name__'])
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\cmd\compute.py"", line 32, in <module>
    from nova.openstack.common.report import guru_meditation_report as gmr
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 63, in <module>
    class GuruMeditation(object):
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 100, in GuruMeditation
    def setup_autorun(cls, version, signum=signal.SIGUSR1):
AttributeError: 'module' object has no attribute 'SIGUSR1'
Nova patch that introduced the issue:
https://github.com/openstack/nova/commit/cec532848f569afb4832029bce4969578472a57a
Review link:
https://review.openstack.org/#/c/69058/"
1,"2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 217, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent kwargs.get('use_root_namespace', False))
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 70, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent namespace)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 81, in _execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent root_helper=root_helper)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent raise RuntimeError(m)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent RuntimeError:
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qdhcp-e1eaed6e-ef91-4741-acb2-62daba3ffede', 'ip', 'route', 'replace', 'default', 'via', '10.2.0.1', 'dev', 'tapb1e4235c-4e']
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Exit code: 2
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stdout: ''
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stderr: 'RTNETLINK answers: Network is unreachable\n'
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent"
1,"I yanked a disk from my system by mistake .Then I execute the command ""curl -v http://proxy-ip:6000/recon/diskusage"" and 500 reported.
It is unreasonable that the recon daemon crash when the only one disk removed while the others disks works fine.
Thanks."
0,"In order to eliminate the need to have the hplefthandclient in the global-requirements project, we need to remove the hplefthandclient from being imported in all hplefthand driver unit tests in cinder.
It should _at least_ be optional for the tests."
1,"If anti-affinity policy is set for two different servers, and only one host is available, a stack trace is shown instead of a error message. Failure is expected, but a message about why it failed would be helpful.
On a single node devstack setup:
nova server-group-create --policy anti-affinity aagroup
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server1
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server2
server1 boots fine
server2 has Status Error and Power State No State
Horizon shows:
Fault
Message
unsupported operand type(s) for |: 'list' and 'set'
Code
500
Details
File ""/opt/stack/nova/nova/scheduler/manager.py"", line 140, in run_instance legacy_bdm_in_spec) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 86, in schedule_run_instance filter_properties, instance_uuids) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 289, in _schedule filter_properties) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 275, in _setup_instance_group filter_properties['group_hosts'] = user_hosts | group_hosts
Created
April 3, 2014, 1:58 p.m."
0,"when we snapshot an instance, we will use @delete_image_on_error to delete any failed snapshot
however, the image will not be removed by backup code flow, it will be an issue if too many backup failed
at last ,all useful image will be removed and we have only 'error' image left in host"
0,"On the libvirt driver the driver.finish_migration method is called with an extra
parameter 'resize_instance'. It should be used to know if it is
necessary or not to resize the disks."
1,"QoS should be changed after retyping the volume.
So far for storwzie driver, if the new type does not have QoS configurations, the old QoS configurations remain in the volume."
1,"To insert data into DB 'glance-manage db load_metadefs' uses IDs for namespaces which are generated by built-in function in Python - enumerate:
for namespace_id, json_schema_file in enumerate(json_schema_files, start=1):
For empty database it works fine, but this causes problems when there are already metadata namespaces in database. The problem is that when there are already metadata definitions in DB then every invoke of glance-manage db load_metadefs leads to IntegrityErrors because of duplicated IDs.
There are two approaches to fix this:
1. Ask for a namespace just after inserting it. Unfortunately in current implementation we need to do one more query.
2. When this go live - https://review.openstack.org/#/c/120414/ - then we won't need to do another query, because ID is available just after inserting a namespace to DB (namespace.save(session=session))."
0,"Trace ends with:

TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3] File ""/opt/stack/nova/nova/virt/libvirt/rbd.py"", line 238, in exists
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3] except rbd.ImageNotFound:
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3] AttributeError: 'module' object has no attribute 'ImageNotFound'

It looks like the above module tries to do a ""import rbd"" and ends up importing itself again instead of the global library module.

A quick fix would be renaming the file to rbd2.py and changing the references in driver.py and imagebackend.py, but maybe there is a better solution?"
1,"I have a private repo that is a few weeks behind the upstream, and was having problems stacking. Initially it was failing in one of the migrations, so I changed the SQLAlchemy version to 0.8.4 and alembic to 0.6.5 and retried (clean database).

What I saw was that the migration worked, but later a router create failed. It was complaining that the enable_snat field was not specified and did not have a default value. Checking in the migrations, I saw that 128e042a2b68_ext_gw_mode.py adds this field and sets the default value to True. However, it was not doing that.

I changed the migration file to use server_default=sa.text(""true"") and now the migration works. BTW, it was the only migration file using 'default=True'.

Not sure if we should just apply this change to the existing migration file or if some other action is needed."
0,"1) Send dns nameserver and dhcp info to the VSM
2) Update subnet properties to VSM"
0,Looking at the lbaas code it's not very obvious that constants.ACTIVE_PENDING is actually a list of statuses that contain states that are ACTIVE or PENDING. This patch renames ACTIVE_PENDING to ACTIVE_PENDING_STATUSES so it's obvious that this is a list and not just a string called ACTIVE_PENDING.
1,"since commit b50e66f the router interfaces will not be automatically delete anymore when a network is deleted.
Instead a 409 response code will be returned.

The NSX plugin still has logic to ensure correct backend state in case router interfaces are present on network_delete. This logic is useless and can be removed.
Also it performs some unnecessary queries, so it affects also plugin performance."
0,"https://blueprints.launchpad.net/cinder/+spec/volume-retype
Volume retype is a new driver API that is introduced in IceHouse release. This allows the user to manually change the volume type associated with a volume.
The vmdk driver uses a volume-type to associate vSphere storage profile with the volume. The vmdk driver needs to handle a volume type change."
1,The domain name gets added to the initiator name used by the host when it's an AD member. The method which automatically gets the initiator name when this is not set in the registry does not take this into account. Trying to use a wrong initiator name will lead to an exception when trying to log in to the according iSCSI target.
1,"Using Nova trunk (Juno). I'm seeing the following nova-api.log errors when unauthenticated /versions controller POST requests are made with a request body:
-----
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 ERROR nova.api.openstack.wsgi [-] Exception handling resource: index() got an unexpected keyword argument 'body'
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi Traceback (most recent call last):
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 983, in _process_stack
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi action_result = self.dispatch(meth, request, action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 1070, in dispatch
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi return method(req=request, **action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi TypeError: index() got an unexpected keyword argument 'body'
-----
Both the index() and multi() actions in the versions controller are susceptible to this behavior. Ideally we wouldn't be logging stack traces when this happens."
0,"After https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension, nova has the feature of creating instance groups with affinity or anti-affinity policy and creating vm instance with affinity/anti-affinity group.
If did not enable ServerGroupAffinityFilter and ServerGroupAntiAffinityFilter, then the instance group will not able to leverage affinity/anti-affinity.
Take the following case:
1) Create a group with affinity
2) Create two vms with this group
3) The result is that those two vms was not created on the same host.
We should throw exception if using server group with no configured affinity filter"
1,"Currently Nova logs an error if a libvirt domain disappears during while waiting for it to be destroyed, but the code actually treats this (correctly) as a recoverable situation since the end result is the required one. Hence this should be logged as a warning not an error.
This may help wit some of the gate failures: https://bugs.launchpad.net/nova/+bug/1300279"
0,"There have been quite a few warning log in cinder scheduler log, something like:

2014-06-11 09:41:39.441 3074 WARNING cinder.context [-] Arguments dropped when creating context: {'user': u'1284c7ced03d4b17830b0c1911b0cfb2', 'tenant': u'a28cee075f3f49528afd48a79646533a', 'user_identity': u'1284c7ced03d4b17830b0c1911b0cfb2 a28cee075f3f49528afd48a79646533a - - -'}

http://logs.openstack.org/25/77125/8/check/check-tempest-dsvm-full/bd92be7/logs/screen-c-sch.txt.gz?level=WARNING#_2014-06-11_09_26_45_485

It's quite annoy and not really helpful."
0,"Improve unit test coverage for ...
quantum/plugins/cisco/nexus/cisco_nexus_network_driver_v2 100 24 0 26 1 72%
quantum/plugins/cisco/nexus/cisco_nexus_plugin_v2 117 23 0 12 2 79%"
1,"$ git grep \\.NotImplementedError
neutron/extensions/l3.py:288: raise qexception.NotImplementedError()
neutron/extensions/l3.py:291: raise qexception.NotImplementedError()
neutron/plugins/nicira/NeutronPlugin.py:1091: raise q_exc.NotImplementedError(_(""admin_state_up=False """
0,"Improve unit test coverage for ...
quantum/plugins/cisco/network_plugin 199 131 0 17 2 34%"
0,"There is unused arguments in a mock method, and duplication of constants, in the Cisco VPNaaS unit test files."
1,"The get_instance_disk_info method in the nova.virt.libvirt.driver logs a None value if a disk path is undefined. The log message is intended to show a path that does not represent a path, but in the case the path does not actually exist, it gives the following uninformative message:
2013-10-14 06:25:21.404 45722 DEBUG nova.virt.libvirt.driver [-] skipping None since it looks like volume get_instance_disk_info /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py:4299
When the path is not defined it should log the instance it is looking at instead."
1,"2013-09-30 05:40:58.352 ERROR nova.openstack.common.periodic_task [req-126d36dc-15d2-4525-a2cd-9f4015987140 None None] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task task(self, context)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/compute/manager.py"", line 4861, in update_available_resource
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task nodenames = set(self.driver.get_available_nodes())
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 599, in get_available_nodes
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task CONF.vmware.cluster_name)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 1008, in get_all_cluster_refs_by_name
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task cls_mor = find_entity_mor(cls, entity_path)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 992, in find_entity_mor
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task return [mor for mor in entity_list if mor.propSet[0].val == entity_name]
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task"
1,"In the following commands, 'vmtest' is a freshly created virtual machine.
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state | -
| status | ACTIVE
$ nova rescue vmtest
+-----------+--------------+
| Property | Value
+-----------+--------------+
| adminPass | 2ZxvzZULT4sr
+-----------+--------------+
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state | -
| status | RESCUE
$ nova pause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state | -
| status | PAUSED
$ nova unpause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state | -
| status | ACTIVE
Here, we would want the vm to be in the 'RESCUE' state, as it was before being paused.
$ nova unrescue vmtest
ERROR (Conflict): Cannot 'unrescue' while instance is in vm_state active (HTTP 409) (Request-ID: req-34b8004d-b072-4328-bbf9-29152bd4c34f)
The 'unrescue' command fails, which seems to confirm that the VM was no longer being rescued.
So, two possibilities:
1) When unpausing, the vm should go back to 'rescued' state
2) Rescued vms should not be allowed to be paused, as is indicated by this graph: http://docs.openstack.org/developer/nova/devref/vmstates.html
Note that the same issue can be observed with suspend/resume instead of pause/unpause, and probably other commands as well.
WDYT ?"
0,"Remove the redundant KeyError check for 'os-migrate_volume_completion'
The fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
0,"tests/unit/test_db_plugin.py line 585
(https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_db_plugin.py#L585)
sorted() function is used within assertEqual() function.
This breaks tests objective which is to test sorting
Every unit test using this function (_test_list_with_sort) for sorting tests will always succeed.
sorted() should not be used for proper test results"
0,The _heal_instance_info cache() is used to heal the cache access instance['info_cache'] so we should always load the info_cache so we can avoid the extra query to get it when accessed.
0,"I have python-glance-2014.1.2-1.el7ost.noarch

when configuring

default_store=vmware_datastore
known_stores = glance.store.vmware_datastore.Store
vmware_server_host = 10.34.69.76
vmware_server_username=root
vmware_server_password=qum5net
vmware_datacenter_path=""New Datacenter""
vmware_datastore_name=shared

glance-api doesn't seem to come up at all.
glance image-list
Error communicating with http://172.16.40.9:9292 [Errno 111] Connection refused

there seems to be nothing interesing in the logs. After changing to the

  default_store=file

  glance image-create --disk-format vmdk --container-format bare --copy-from 'http://str-02.rhev/OpenStack/cirros-0.3.1-x86_64-disk.vmdk' --name cirros-0.3.1-x86_64-disk.vmdk --is-public true --property vmware_disktype=""sparse"" --property vmware_adaptertype=""ide"" --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore

or

  glance image-create --disk-format vmdk --container-format bare --file 'cirros-0.3.1-x86_64-disk.vmdk' --name cirros-0.3.1-x86_64-disk.vmdk --is-public true --property vmware_disktype=""sparse"" --property vmware_adaptertype=""ide"" --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore

the image remains in queued state

I can see log lines
2014-08-15 12:38:55.885 24732 DEBUG glance.store [-] Registering store <class 'glance.store.vmware_datastore.Store'> with schemes ('vsphere',) create_stores /usr/lib/python2.7/site-packages/glance/store/__init__.py:208
2014-08-15 12:39:54.119 24764 DEBUG glance.api.v1.images [-] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057
2014-08-15 12:43:31.408 24764 DEBUG glance.api.v1.images [eac2ff8d-d55a-4e2c-8006-95beef8a0d7b caffabe3f56e4e5cb5cbeb040224fe69 77e18ad8a31e4de2ab26f52fb15b3cc1 - - -] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057

so it looks like there is inconsistency on the scheme that should be used. After hardcoding

  STORE_SCHEME = 'vmware_datastore'

in the

  /usr/lib/python2.7/site-packages/glance/store/vmware_datastore.py

the behaviour changed, but did not improve very much:

  glance image-create --disk-format vmdk --container-format bare --file 'cirros-0.3.1-x86_64-disk.vmdk' --name cirros-0.3.1-x86_64-disk.vmdk --is-public true --property vmware_disktype=""sparse"" --property vmware_adaptertype=""ide"" --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
400 Bad Request
Store for image_id not found: 7edc22ae-f229-4f21-8f7d-fa19a03410be
    (HTTP 400)"
0,"There are four methods in nova/nova/volume/cinder.py which are NotImplemented. They are as follows:
1. get_volume_metadata
2. delete_volume_metadata
3. update_volume_metadata
4. get_volume_metadata_value
These methods are required in cases where nova needs to modify a cinder volume's metadata, e.g. attach and detach time.
The latest code in nova's master branch shows these methods as NotImplemented."
1,"I'm using live-migration with devstack and ML2 plugin (the same error occurs with the OVS plugin)
https://wiki.openstack.org/wiki/Devstack/LiveMigration
First of all, I think there is a bug in nova :
https://bugs.launchpad.net/nova/+bug/1224960
I've proposed a patch attached to this bug to resolv it quickly.
it seems that there is also something that goes wrong in neutron.
after live-migrating a VM, the port is correctly created on the new host and the dataplane seems correct. But the port is still down in the port table, and the vif-type is ""binding_failed"" in ml2_port_bindings table"
0,"when using at least two compute nodes using KVM, and use NFS share_storage to test resize an instance.
The configuration of NFS used the introduction about live-migration using NFS in community doc.
when executed command ""nova resize ae6f9472-3080-4e86-8a52-f8e642081d15"", can work well, and the instance's state will change to ""VERIFY_RESIZE', Then I resize-confirm it, nova met the issue as follow:
{u'message': u""[Errno 39] Directory not empty: '/KVM/stack/data/nova/instances/ae6f9472-3080-4e86-8a52-f8e642081d15_resize'"", u'code': 500, u'details': u' File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 263, in decorated_function |
| | return function(self, context, *args, **kwargs) |
| | File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2700, in confirm_resize |
| | do_confirm_resize(context, instance, migration_id) |
| | File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner |
| | return f(*args, **kwargs) |
| | File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2697, in do_confirm_resize |
| | migration=migration) |
| | File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2724, in _confirm_resize |
| | network_info) |
| | File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4623, in confirm_migration |
| | self._cleanup_resize(instance, network_info) |
| | File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1018, in _cleanup_resize |
| | shutil.rmtree(target) |
| | File ""/usr/lib64/python2.6/shutil.py"", line 221, in rmtree |
| | onerror(os.rmdir, path, sys.exc_info()) |
| | File ""/usr/lib64/python2.6/shutil.py"", line 219, in rmtree |
| | os.rmdir(path) |
| | ', u'created': u'2013-10-22T15:10:50Z'}
cd /KVM/stack/data/nova/instances/be962096-a539-46c7-ae66-9ea383809e9b_resize
[root@cc be962096-a539-46c7-ae66-9ea383809e9b_resize]# ls -al
total 24340
drwxr-xr-x 2 nobody nobody 4096 Oct 18 2013 .
drwxrwxrwx 14 root root 4096 Oct 18 2013 ..
-rw-r--r-- 1 nobody nobody 25034752 Oct 18 2013 .nfs000000000714002e00000001"
0,"According to the oslo.messaging documentation, when a RPC request is made to a given topic, and there are multiple servers for that topic, only _one_ server should service that RPC request. See http://docs.openstack.org/developer/oslo.messaging/target.html
""topic (str) 鈥?A name which identifies the set of interfaces exposed by a server. Multiple servers may listen on a topic and messages will be dispatched to one of the servers in a round-robin fashion.""
In the case of a QPID-based deployment using topology version 2, this is not the case. Instead, each listening server gets a copy of the RPC and will process it.
For more detail, see
https://bugs.launchpad.net/oslo/+bug/1178375/comments/26"
1,"Started roughly 1800 UTC this evening

2014-08-01 19:36:06.878 | error: can't copy 'etc/neutron/plugins/cisco/cisco_cfg_agent.ini': doesn't exist or not a regular file"
0,"Since nova.virt.driver:LibvirtDriver.get_guest_config prepends instance root_device_name with 'dev' prefix, root_device_name may not coincide with device_name in block device mapping structure.
In this case describe instances operation reports wrong instance type: instance-store instead of ebs.

Environment: DevStack

Steps to reproduce:
1 Create volume backed instance passing vda as root device name
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --block-device-mapping vda=yyy:::1 inst
Note. I used cirros ami image.

2 Describe instances
$ euca-describe-instances
Look on instace type. It must be ebs, but it is instance-store in the output.

Note. If euca-describe-instance crashes on ebs instnce, apply https://review.openstack.org/#/c/95580/"
1,"Currently on success, DELETE /v2/image/{imageId}/members/{memberId} returns 200 with no response body.
According to the Glance API contract, a 200 means you should expect a response body. Success with no response body should be 204."
1,"Create snapshot from bootable volume.
Let's say cinder process down suddenly, when snapshot status is changed available.
Restarting cinder process, there is available snapshot that doesn't have snapshot metadata.
When the snapshot doesn't have the snapshot metadata, the status is available is not good.(data and status are mismatched)"
0,"stub_out_key_pair_funcs was useless, remove it.
-def stub_out_key_pair_funcs(stubs, have_key_pair=True):
- def key_pair(context, user_id):
- return [dict(name='key', public_key='public_key')]
-
- def one_key_pair(context, user_id, name):
- if name == 'key':
- return dict(name='key', public_key='public_key')
- else:
- raise exc.KeypairNotFound(user_id=user_id, name=name)
-
- def no_key_pair(context, user_id):
- return []"
1,"""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp **args)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 808, in wrapper
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 779, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.db.volume_update(ctxt, volume_ref['id'], updates)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 772, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self._migrate_volume_generic(ctxt, volume_ref, host)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 710, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp new_volume['migration_status'] = None
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 690, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote='dest')
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 293, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp {'status': dest_orig_status})
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 287, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote=dest_remote)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 378, in _attach_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 798, in brick_get_connector
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 114, in factory
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 802, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/remotefs/remotefs.py"", line 41, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp err=_('nfs_mount_point_base required'))
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp InvalidParameterValue: An unknown exception occurred."
0,"Not sure if this is intentional or an oversight, but nova.tests.db.test_migrations.BaseWalkMigrationTestCase._test_postgresql_opportunistically calls get_mysql_connection_info even though there is a get_pgsql_connection_info method that is slightly different and used elsewhere.
Obviously not breaking anything, but should probably be cleaned up."
1,"When deleting/updating a port on a shared network, the plugin is currently using the tenant ID of the port owner in the URI rather than the tenant ID of the network owner. This should always be the tenant ID of the network the port is being attached to."
0,UnboundLocalError: local variable 'mode' referenced before assignment. This most likely is due to a recent commit: https://bugs.launchpad.net/neutron/+bug/1316190
0,"There are various places in the unit tests where non-existent ""assert"" methods are called on mock objects.
e.g. assert_called_once(), assert_called_twice(), assert_called_one_with(), assert_called_twice()
To reproduce, put the mock.py file in the base neutron directory and edit the __getattr__ methods to check if the name begins with 'assert' and raise an exception if true."
1,"The current implementation can potentially lead to a timeout when the invoke_api is not triggered (for example PUT/GET: direct HTTP access to the datastore).
We need to recreate the session responses and retry when getting 401 HTTP responses."
1,"Unrecognized Content-Type provided in request get_body /opt/stack/new/cinder/cinder/api/openstack/wsgi.py:796
Cinder api service logs the above entry, on every request made by the python-cinderclient, for example ""cinder list"".
 'application/json' should be an accepted content type, without any additional log.
You can see the above issue as the result of all tempest-devstack gate job as well."
1,"When I deploy nova with a shared storage and with neutron/ML2/linuxbridge, I have an error when I want to to use ""nova evacuate""
command :
# nova evacuate 1fa486f3-259c-4e1e-ae82-8b52606f1efd devstack2 --on-shared-storage
here are the logs on the compute node (devstack2) which will host the VM after the evacuation:
2014-02-25 16:08:39.918 ERROR nova.compute.manager [req-8307b6e1-b6ee-423e-9baf-d05a0ac5e91d admin admin] [ins
tance:
1fa486f3-259c-4e1e-ae82-8b52606f1efd] Setting instance vm_state to ERROR
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/compute/manager.py"", line 5261, in _error_out_instance_on_exception
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] yield
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/compute/manager.py"", line 2267, in rebuild_instance
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] extra_usage_info=extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/conductor/api.py"", line 271, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 428, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] extra_usage_info=extra_usage_info_p)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/client.py"", line 150, in call
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] wait_for_reply=True, timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 87, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 386, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] raise result
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] incoming.message))
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/conductor/manager.py"", line 502, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/compute/utils.py"", line 276, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] ignore_missing_network_data)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] File ""/opt/stack/nova/nova/notifications.py"", line 288, in bandwidth_usage
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] macs = [vif['address'] for vif in nw_info]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]"
0,"Some work in Juno around adding a new router processing queue to the l3_agent.py obsoleted much of the logic in the _process_routers method. The following can be simplified.

1. No loop is necessary since the list passed always has exactly one router in it.
2. No thread pool is necessary because there is only one thread active and the method waits for it to complete at the end.
3. The set logic is no longer needed."
0,"The neutron.common.exceptions.AlreadyAttached exception is not used. It looks like it's replaced by this exception in the nicira plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/nicira/common/exceptions.py#L51
Either common.exceptions.AlreadyAttached should be removed or NvpPortAlreadyAttached should extend the common AlreadyAttached exception class."
1,The vmdk cinder drivers retry the VIM APIs including login API even during errors which are unrelated to connection or session overload problems. The VIM APIs need to retried only during session overload or a stale session scenario.
1,"1. Create snapshot metadata with a too long key('a'*260)
    On the _update_snapshot_metadata() in snapshot_metadata.py, it will raise HTTPRequestEntityTooLarge(413) exception in this case. So the 413 exception expected to raise. But we got the server fault(500)
2. The KeyError exception raised while generating a WSGI response based on the exception, because headers no 'Retry-After' attribute in this case.
   if code == 413:
            retry = self.wrapped_exc.headers['Retry-After']
            fault_data[fault_name]['retryAfter'] = retry"
1,"1.create a dvr with name say dvr1
2.add router gateway
3.now perform router update neutron router-update dvr1 --name dvr2
Actual Results:
Request Failed: internal server error while processing your request"" error is seen,trace related to L3RouterPlugin logs are seen
Expected Results:
router name should be updated"
1,"In cleaning up VDIs we call destroy_vdi.
However, should the destroy fail, it masks the real error, for example:
Unable to destroy VDI OpaqueRef:f53f35a9-31e1-af85-87a8-aa0dc7eecd43
  nova/compute/manager.py"", _build_instance
...
  nova/virt/xenapi/vmops.py"", line, in _attach_disks
    DEVICE_SWAP, name_label, swap_mb)
  nova/virt/xenapi/vm_utils.py"", line, in generate_swap
    'swap', swap_mb, fs_type)
 nova/virt/xenapi/vm_utils.py"", line, in _generate_disk
    destroy_vdi(session, vdi_ref)
  nova/virt/xenapi/vm_utils.py"", line, in destroy_vdi
    _('Unable to destroy VDI %s') % vdi_ref)
We should instead use the safe_destroy_vdi call."
0,The Big Switch ML2 driver references the deprecated portbindings_db in the port location tracking code. This needs to be removed because it's resulting in the entire portbinding_db schema for one small function.
1,"the code at: https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L73
will trigger a rebind of the security group of the port, as well as instruct the plugin to notify the agent if the port update request has the 'security_groups' attribute.
Actually these operations are not needed if there's no change from the current value for security groups.
This condition is causing a fair amount of port_update notification to be sent to the agent; the DHCP port is updated quite often, and since the plugin function is called from the RPC dispatcher, the security groups attribute is not missing, but is an empty list.
For this reason the notification is then sent every time a DHCP port is updated."
1," but the exception msg only contain info about """"available"""". So fix it."""
1,"In accept_transfer() workflow (https://github.com/openstack/cinder/blob/master/cinder/transfer/api.py#L193)
         try:
             # Transfer ownership of the volume now, must use an elevated
             # context.
             self.volume_api.accept_transfer(context,
                                             vol_ref,
                                             context.user_id,
                                             context.project_id)
             self.db.transfer_accept(context.elevated(),
                                     transfer_id,
                                     context.user_id,
                                     context.project_id)
   self.volume_api.accept_transfer() sends out a RPC request (cast) to volume manager and volume driver may do something in backend (e.g. modify account for the volume), but since it's a unblocking RPC cast, this call returns pretty fast and the volume record in DB will be updated. There can be cases where DB update finishes even before volume manager / driver does their job. Unfortunately some driver(s) relies on original DB record to do their stuff (SolidFire for example). Such situation will turn volume into inconsistent state (between backend and cinder DB) and become unusable.
   We may either change accept_transfer() in volume RPC API from CAST to blocking CALL, or we can pass enough original volume state to volume manger so that they don't have to rely on unreliable DB state."
1,"failure to mount gluster storage appears in the cinder volume log as a warning and not an error
since we would basically not be able to create/delete/attach when we fail to mount I think that this should be logged as ERROR and if possible print the trace in the log.
2014-05-08 18:54:45.306 3121 WARNING cinder.volume.drivers.glusterfs [-] Exception during mounting Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf mount -t glusterfs 10.35.64.104:/gluster2-cinder-tshefi /var/lib/cinder/mnt/8f2d3277bf483ae9bae356bd866fed5e
Exit code: 1
Stdout: 'Mount failed. Please check the log file for more details.\n'
Stderr: ''"
0,"During Juno we had plans to clean up the iSCSI and target code. Sadly the work rotted in gerrit without reviews and didn't make it into this release. In the process there was a commit that merged that layed some foundation work:
Change-Id: Iaa55e31e3dadc7dcb58112302c3807a8f92bcada
Without the follow up work however this is basicly ""dead code"" so it should be removed from the Juno release and added back in with Kilo when we're ready to go again."
1,"It seems that when you allocate a floating-ip in a tenant with nova-network, its quota is never returned after calling 'nova floating-ip-delete' ecen though 'nova floating-ip-list' shows it gone. This behavior applies to each tenant individually. The gate tests are passing because they all run with tenant isolation. But this problem shows in the nightly run without tenant isolation:
http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-full-non-isolated-master/2bc5ead/console.html"
1,"This happens on master.
Steps to reproduce:
- usual devstack config will do
- boot a VM
- ssh to the VM
- curl http://169.254.169.254 from the console
- 404 Not Found error is reported
I would expect the metadata server response to come across correctly."
0,"Nova api ""show"" do not fetch the error message of vm in havana.
In /nova/nova/api/openstack/compute/views/servers.py, fault = instance.get(""fault"", None), cannot get the fault message.
I tried instance[""fault""], If the vm in error state, it get the error message.
So i think it's some lazy load problem of the instance object and instance fault."
1,"working with gluster backend for cinder, after failure to delete a snapshot on timeout, I reset the snapshot status to try and delete it again, in which time I immediately fail to delete the snapshot with the following error:
2014-04-22 18:25:48.499 2233 DEBUG cinder.openstack.common.processutils [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Running cmd (subprocess): sudo cinder-rootwrap /etc/cind
er/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa execute /usr/lib/python2.6/site-packages/cinder/open
stack/common/processutils.py:142
2014-04-22 18:25:48.507 2233 DEBUG qpid.messaging.io.raw [-] SENT[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x01\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' writeable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:480
2014-04-22 18:25:48.508 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x02\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionAttached(name='e20240e0-4cca-44eb-8847-cc486fa240fa:357') write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionCommandPoint(command_id=serial(0), command_offset=0) write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.741 2233 ERROR cinder.openstack.common.rpc.amqp [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Exception during message handling
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp **args)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 435, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp {'status': 'error_deleting'})
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 423, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp self.driver.delete_snapshot(snapshot_ref)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 247, in inner
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 557, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp self._delete_snapshot(snapshot)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 621, in _delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp raise exception.GlusterfsException(msg)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp GlusterfsException: No base file found for /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa.
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp
2014-04-22 18:26:03.171 2233 DEBUG qpid.messaging.io.raw [-] READ[3b385a8]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:03.172 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3b385a8]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:07.577 2233 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.6/site-packages/cinder/openstack/common/periodic_task.py:176
this is the ls output showing that we see the mount:
[root@hostXX ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
-rw-r--r--. 1 qemu qemu 10739843072 Apr 22 18:21 /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
here is the snapshot in the storage:
[root@storage Dafna]# ls -l |grep c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d
-rw-r--r-- 2 qemu qemu 10739843072 Apr 22 18:21 volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa"
0,An OVA package is a tar archive usually containing an OVF directory inside it. Nova needs to be able to differentiate OVF and OVA based on the container format in order to extract the relevant information from it.
1,"Location: cinder.api.contrib.volume_type_encryption
Method: create
Bug:
When a volume type is made an encrypted volume type, through the volume_type_encryption API extension, the extension does not confirm no volumes exist with the volume type before making the volume type encrypted. If volumes exist with the volume type before the volume type is made an encrypted volume type, these volumes will not be encrypted, although the user may think they are encrypted because they have an encrypted volume type.
Proposed Fix:
Add a check in the volume_type_encryption extension to stop the creation of an encrypted volume type if there are currently volumes with the volume type. Also add a unit test confirming functionality."
1,"When live-migrating an instance that has a Cinder volume (stored on NFS) attached, the operation fails if the volume size is bigger than the space left on the destination node. This should not happen, since this volume does not have to be migrated. Here is how to reproduce the bug on a cluster with one control node and two compute nodes, using the NFS backend of Cinder.
$ nova boot --flavor m1.tiny --image 173241e-babb-45c7-a35f-b9b62e8ced78 test_vm
...
$ nova volume-create --display-name test_volume 100
...
| id | 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 |
...
$ nova volume-attach test_vm 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 auto
...
$ nova show test_vm | grep OS-EXT-SRV-ATTR:host
| OS-EXT-SRV-ATTR:host | t1-cpunode0 |
$ nova service-list | grep nova-compute
| nova-compute | t1-cpunode0 | nova | enabled | up | 2014-08-13T19:14:40.000000 | - |
| nova-compute | t1-cpunode1 | nova | enabled | up | 2014-08-13T19:14:41.000000 | - |
Now, let's say I want to live-migrate test_vm to t1-cpunode1:
$ nova live-migration --block-migrate test_vm t1-cpunode1
ERROR: Migration pre-check error: Unable to migrate a0d9c991-7931-4710-8684-282b1df4cca6: Disk of instance is too large(available on destination host:46170898432 < need:108447924224) (HTTP 400) (Request-ID: req-b4f00867-df51-44be-8f97-577be385d536)
In nova/virt/libvirt/driver.py, _assert_dest_node_has_enough_disk() calls get_instance_disk_info(), which in turn, calls _get_instance_disk_info(). In this method, we see that volume devices are not taken into account when computing the amount of space needed to migrate an instance:
...
            if disk_type != 'file':
                LOG.debug('skipping %s since it looks like volume', path)
                continue
            if target in volume_devices:
                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
                          'volume', {'path': path, 'target': target})
                continue
...
But for some reason, we never get into these conditions.
If we ssh the compute where the instance currently lies, we can get more information about it:
$ virsh dumpxml 11
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/nova/mnt/84751739e625d0ea9609a65dd9c0a6f1/volume-6b9e1d03-3f53-4454-add9-a8c32d82c7e6'/>
      <target dev='vdb' bus='virtio'/>
      <serial>6b9e1d03-3f53-4454-add9-a8c32d82c7e6</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
...
The disk type is ""file"", which might explain why this volume is not skipped in the code snippet shown above. When we use the default Cinder backend, we get something such as:
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/disk/by-path/ip-192.168.200.250:3260-iscsi-iqn.2010-10.org.openstack:volume-47ecc6a6-8af9-4011-a53f-14a71d14f50b-lun-1'/>
      <target dev='vdb' bus='virtio'/>
      <serial>47ecc6a6-8af9-4011-a53f-14a71d14f50b</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </disk>
I think that the code in LibvirtNFSVolumeDriver.connect_volume() might be wrong: conf.source_type should be set to something else than ""file"" (and some other changes might be needed), but I must admit I'm not a libvirt expert.
Any thoughts ?"
1,"I am working with gluster as cinder's backend on 2 computes for Havana.
when I try to boot an instance from a cloned volume (cinder create 10 --source-volid f7416ba6-af45-47d3-a333-478447a1ab54 --display-name from_vol1) we get a rootwrap error in volumes log and the instance moves to status ERROR.
more info that might be helpful, I tried booting an instance from a newly created volume and a volume created from image and instance is started correctly.
This error is for cloned from volume only.
[root@cougar06 ~(keystone_admin)]# nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| 4dc50e69-9d84-4b19-b7ec-4bf0628d751b | na | ERROR | None | NOSTATE | |
+--------------------------------------+------+--------+------------+-------------+----------+
[root@cougar06 ~(keystone_admin)]#
2013-10-24 16:44:00.413 2483 ERROR cinder.openstack.common.rpc.common [req-26521926-e9cb-4308-aacd-425ba2a1932a a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n **args)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n return getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper\n return func(self, *args, **kwargs)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection\n conn_info = self.driver.initialize_connection(volume, connector)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection\n info = self._qemu_img_info(path)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info\n info = image_utils.qemu_img_info(path)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info\n out, err = utils.execute(*cmd, run_as_root=True)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute\n return processutils.execute(*cmd, **kwargs)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute\n cmd=\' \'.join(cmd))\n', 'ProcessExecutionError: Unexpected error while running command.\nCommand: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\nExit code: 1\nStdout: \'\'\nStderr: ""Could not open \'/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\': No such file or directory\\n""\n']
2013-10-24 16:44:04.763 2483 ERROR cinder.openstack.common.rpc.amqp [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Exception during message handling
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp **args)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp conn_info = self.driver.initialize_connection(volume, connector)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp info = self._qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp info = image_utils.qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp out, err = utils.execute(*cmd, run_as_root=True)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp return processutils.execute(*cmd, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp cmd=' '.join(cmd))
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Exit code: 1
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stdout: ''
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n""
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp
2013-10-24 16:44:04.765 2483 ERROR cinder.openstack.common.rpc.common [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Returning exception Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
Exit code: 1
Stdout: ''
Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n"" to caller
the volume does appear as if it exists and available in cinder list
[root@cougar06 ~(keystone_admin)]# cinder list
/usr/lib/python2.6/site-packages/babel/__init__.py:33: UserWarning: Module backports was already imported from /usr/lib64/python2.6/site-packages/backports/__init__.pyc, but /usr/lib/python2.6/site-packages is being added to sys.path
  from pkg_resources import get_distribution, ResolutionError
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 0466799b-0810-4c69-a894-0f395fe89452 | available | from_vol | 10 | None | true | |
| 1e36f3ac-27ef-46ea-b5fa-686c4da9f449 | available | test | 10 | None | false | |
| 5d658297-5037-4203-9482-b072a2bc7526 | available | from_vol1 | 10 | None | true | |
| f7416ba6-af45-47d3-a333-478447a1ab54 | available | from_img | 10 | None | true | |
| f9d6b98f-8394-4a01-9424-f23897382d87 | available | dafna | 10 | None | false | |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@cougar06 ~(keystone_admin)]#
but if I look under mnt its not there:
root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/
conversion/ mnt/ .novaclient/ tmp/
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-
volume-1e36f3ac-27ef-46ea-b5fa-686c4da9f449 volume-f9d6b98f-8394-4a01-9424-f23897382d87
volume-f7416ba6-af45-47d3-a333-478447a1ab54 volume-f9d6b98f-8394-4a01-9424-f23897382d87.24804bb3-3846-45f8-8f24-5320e6d57184
volume-f7416ba6-af45-47d3-a333-478447a1ab54-clone volume-f9d6b98f-8394-4a01-9424-f23897382d87.73262c7c-e762-44cf-85d4-14693ab0dd31
volume-f7416ba6-af45-47d3-a333-478447a1ab54.info volume-f9d6b98f-8394-4a01-9424-f23897382d87.info
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-^C"
0,"the following two failures appeared after upgrading jsonschema to 2.4.0. downgrading to 2.3.0 returned the tests to passing.
======================================================================
FAIL: nova.tests.test_api_validation.TcpUdpPortTestCase.test_validate_tcp_udp_port_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  pythonlogging:''
  stderr
  stdout
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 602, in test_validate_tcp_udp_port_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 65536. 65536 is greater than the maximum of 65535'}
actual = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 65536. 65536.0 is greater than the maximum of 65535'}
======================================================================
FAIL: nova.tests.test_api_validation.IntegerRangeTestCase.test_validate_integer_range_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 244 -> 245...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 245 -> 246...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 246 -> 247...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 247 -> 248...
INFO [248_add_expire_reservations_index] Skipped adding reservations_deleted_expire_idx because an equivalent index already exists.
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 248 -> 249...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 249 -> 250...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 361, in test_validate_integer_range_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 0. 0 is less than the minimum of 1'}
actual = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 0. 0.0 is less than the minimum of 1'}"
1,"2014-09-26 03:38:57.259 | Traceback (most recent call last):
2014-09-26 03:38:57.259 | File ""tempest/scenario/manager.py"", line 142, in delete_wrapper
2014-09-26 03:38:57.259 | delete_thing(*args, **kwargs)
2014-09-26 03:38:57.259 | File ""tempest/services/volume/json/volumes_client.py"", line 108, in delete_volume
2014-09-26 03:38:57.259 | resp, body = self.delete(""volumes/%s"" % str(volume_id))
2014-09-26 03:38:57.259 | File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.259 | return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.259 | File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.259 | resp, resp_body)
2014-09-26 03:38:57.259 | File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.259 | raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.259 | BadRequest: Bad request
2014-09-26 03:38:57.260 | Details: {u'message': u'Invalid volume: Volume status must be available or error, but current status is: in-use', u'code': 400}
2014-09-26 03:38:57.260 | }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.260 | traceback-2: {{{
2014-09-26 03:38:57.260 | Traceback (most recent call last):
2014-09-26 03:38:57.260 | File ""tempest/common/rest_client.py"", line 561, in wait_for_resource_deletion
2014-09-26 03:38:57.260 | raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.260 | TimeoutException: Request timed out
2014-09-26 03:38:57.260 | Details: (TestEncryptedCinderVolumes:_run_cleanups) Failed to delete resource 704461b6-3421-4959-8113-a011e6410ede within the required time (196 s).
2014-09-26 03:38:57.260 | }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.261 | traceback-3: {{{
2014-09-26 03:38:57.261 | Traceback (most recent call last):
2014-09-26 03:38:57.261 | File ""tempest/services/volume/json/admin/volume_types_client.py"", line 97, in delete_volume_type
2014-09-26 03:38:57.261 | resp, body = self.delete(""types/%s"" % str(volume_id))
2014-09-26 03:38:57.261 | File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.261 | return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.261 | File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.261 | resp, resp_body)
2014-09-26 03:38:57.261 | File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.261 | raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.261 | BadRequest: Bad request
2014-09-26 03:38:57.261 | Details: {u'message': u'Target volume type is still in use.', u'code': 400}
2014-09-26 03:38:57.262 | }}}
2014-09-26 03:38:57.262 |
2014-09-26 03:38:57.262 | Traceback (most recent call last):
2014-09-26 03:38:57.262 | File ""tempest/test.py"", line 142, in wrapper
2014-09-26 03:38:57.262 | return f(self, *func_args, **func_kwargs)
2014-09-26 03:38:57.262 | File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 56, in test_encrypted_cinder_volumes_luks
2014-09-26 03:38:57.262 | self.attach_detach_volume()
2014-09-26 03:38:57.262 | File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 49, in attach_detach_volume
2014-09-26 03:38:57.262 | self.nova_volume_detach()
2014-09-26 03:38:57.262 | File ""tempest/scenario/manager.py"", line 439, in nova_volume_detach
2014-09-26 03:38:57.262 | 'available')
2014-09-26 03:38:57.262 | File ""tempest/services/volume/json/volumes_client.py"", line 181, in wait_for_volume_status
2014-09-26 03:38:57.263 | raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.263 | TimeoutException: Request timed out
2014-09-26 03:38:57.263 | Details: Volume 704461b6-3421-4959-8113-a011e6410ede failed to reach available status within the required time (196 s).

http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogKFRlc3RFbmNyeXB0ZWRDaW5kZXJWb2x1bWVzOl9ydW5fY2xlYW51cHMpIEZhaWxlZCB0byBkZWxldGUgcmVzb3VyY2VcIiBBTkQgbWVzc2FnZTpcIndpdGhpbiB0aGUgcmVxdWlyZWQgdGltZVwiIEFORCB0YWdzOlwiY29uc29sZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDExNzM4OTc0MTMwfQ==

130 hits in 7 days, check and gate, all failures."
0,"@wsgi.action('os-getVNCConsole')
    def get_vnc_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)
@wsgi.action('os-getSPICEConsole')
    def get_spice_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)"
1,"The capabilities check in the BigSwitch server manager tries to call json.loads on an object that has already been decoded and fails. This
causes the servers to have an empty capability list so none of the newer features are leveraged."
0,"Some things need cleanup in the ML2Manager.
1.) In the current ML2 Manager, we are using sys.exit(1) if the network_type isn't found in self.drivers:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L70
Here we should probably throw an exception. When running unit test, if we hit this condition the unit tests will exit as well.
2.) We should also be mindful of using the reserved keyword 'type' and rename type in this case to something else:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L49"
0,"The method is huge and has lots of conditional blocks.
We should break the big conditional blocks out into private methods so the top-level quota_reserve logic can be unit tested on it's own.
This became an issue in this review for a separate bug fix in the logic:
https://review.openstack.org/#/c/121259/"
0,"The Big Switch consistency DB throws an exception if read_for_update() is called multiple times without closing the transaction in between. This was originally because there was a DB lock in place and a single thread could deadlock if it tried twice. However,
there is no longer a point to this protection because the DB lock is gone and certain response failures result in the DB being read twice (the second time for a retry)."
1,"For instance booted from volume with legacy bdm and image (this method is documented as workaround in http://docs.openstack.org/grizzly/openstack-ops/content/attach_block_storage.html) admin user creates instance snapshot in the image tenant rather than current tenant.
Created snapshot cannot be used.
Environment: DevStack
Steps to reproduce:
1 Create bootable volume from public image from not current tenant.
For example, use demo tenant in DevStack.
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.
2 Boot an instance from the volume passing the original image.
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy inst
3 Create instance snapshot under admin user
$ nova image-create inst snap
4 List images and make sure there is no the created snapshot.
$ glance image-list
5 List images from the original image tenant and found the snapshot.
$ glance --os-tenant-name nnn image-list
snapshot_volume_backed in nova/compute/api.py receives image in image_meta parameter, cleans some attributes, but forgets to deal something with owner attribute."
0,"Now there is not any API sample file of ""unshelve a server"" API,
and OpenStack API documentation[1] also does not describe the API.
[1]: http://api.openstack.org/api-ref-compute-ext.html"
1,Change https://review.openstack.org/#/c/62314/ mistakenly made the conversion of the instance object in compute/rpcapi.py rescue_image() unconditional on the RPC version. From 3.9 onwards this should be passed as an object
1,"The description of the option ""compute_driver"" should include hyperv.HyperVDriver along with the other supported drivers
https://github.com/openstack/nova/blob/aa018a718654b5f868c1226a6db7630751613d92/nova/virt/driver.py#L35-L38"
1,"I observed this stacktrace:
http://paste.openstack.org/show/85277/
It looks like there may be circumstances where routers do not get initialized. It's better to be more defensive in this regard."
1,"When you do a normal volume attach to an existing VM and then detach it, the connection_info contains the following
connection_info['data']['device_path'] at libvirt volume driver disconnect_volume(self, connection_info, mount_device) time.
When you boot a VM from a volume, not an image, and then terminate the VM, the libvirt volume driver disconnect_volume's
connection_info['data'] doesn't contain the 'device_path' key. The libvirt volume driver's need this information to correctly disconnect the LUN from the kernel."
1,"After seeing that the following bug was fixed
https://bugs.launchpad.net/nova/+bug/1296913
executed the cmd:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
now the server-foo is stuck in scheduling state forever.
see attached logs."
1,Fix typo in lbaas agent exception message
1,"This is a stacktrace experienced during a test on trunk:
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Matched DELETE /ports/a42a9719-8416-4c44-a4f3-b3861648281f __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Route path: '/ports/{id}{.format}', defaults: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Match dict: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>, 'id': u'a42a9719-8416-4c44-a4f3-b3861648281f', 'format': None} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-30 13:42:39.606 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Sending port.delete.start on notifications.info notify /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:598
2013-12-30 13:42:39.607 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] UNIQUE_ID is 3f460839b3144eaa911c04de8725eb06. _add_unique_id /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:339
2013-12-30 13:42:39.610 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Acquired connection https://192.168.1.8:443. 9 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Issuing - request POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.647 29118 DEBUG neutron.plugins.nicira.nvplib [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Looking for port with q_port_id tag 'a42a9719-8416-4c44-a4f3-b3861648281f' on: '6d795954-a836-47f2-b2e3-2438e0da7f80' get_port_by_neutron_tag /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:743
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Acquired connection https://192.168.1.8:443. 8 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Issuing - request GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', u'_context_tenant_name': None, u'_context_project_name': None, u'_context_read_deleted': u'no', u'args': {u'agent_state': {u'agent_state': {u'topic': u'dhcp_agent', u'binary': u'neutron-dhcp-agent', u'host': u'cidevstack', u'agent_type': u'DHCP agent', u'configurations': {u'subnets': 3, u'use_namespaces': True, u'dhcp_lease_duration': 86400, u'dhcp_driver': u'neutron.agent.linux.dhcp.Dnsmasq', u'networks': 3, u'ports': 5}}}, u'time': u'2013-12-30T21:42:39.665454'}, u'_context_tenant': None, u'method': u'report_state', u'_unique_id': u'49ca1db3e75247e594c8c4005b4b262a', u'_context_timestamp': u'2013-12-30 21:42:39.665074', u'_context_is_admin': True, u'version': u'1.0', u'_context_project_id': None, u'_context_tenant_id': None, u'_context_user': None, u'_context_user_id': None, u'namespace': None, u'_context_user_name': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'project_name': None, 'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', 'is_admin': True, 'tenant': None, 'timestamp': u'2013-12-30 21:42:39.665074', 'tenant_name': None, 'project_id': None, 'user_name': None, 'read_deleted': u'no', 'user': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.671 29118 DEBUG neutron.context [req-0f858449-8d08-4c71-af5c-3fa7b4470f20 None None] Arguments dropped when creating context: {'project_name': None, 'tenant': None} __init__ /opt/stack/neutron/neutron/context.py:84
2013-12-30 13:42:39.681 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Completed request 'POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 (0.07 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Reading X-Nvp-config-Generation response header: '96231' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:146
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Released connection https://192.168.1.8:443. 9 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Completed request 'GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 (0.04 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Released connection https://192.168.1.8:443. 10 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3681] Completed request 'POST /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3682] Completed request 'GET /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.686 29118 DEBUG neutron.plugins.nicira.nvplib [-] Created logical port d612afd7-339d-4f25-97d2-e6f6c966551e on logical switch 6d795954-a836-47f2-b2e3-2438e0da7f80 create_lport /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:856
2013-12-30 13:42:39.696 29118 ERROR neutron.api.v2.resource [-] delete failed
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1342, in delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource port_delete_func(context, neutron_db_port)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 498, in _nvp_delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource port_data)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 750, in _nvp_get_port_id
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource nvp_port['uuid'])
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_db.py"", line 54, in add_neutron_nvp_port_mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource return mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource self.commit()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource self._prepare_impl()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource self.session.flush()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 541, in _wrap
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource _raise_if_duplicate_entry_error(e, get_engine().name)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 492, in _raise_if_duplicate_entry_error
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource raise exception.DBDuplicateEntry(columns, integrity_error)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry 'a42a9719-8416-4c44-a4f3-b3861648281f' for key 'PRIMARY'"") 'INSERT INTO quantum_nvp_port_mapping (quantum_id, nvp_id) VALUES (%s, %s)' ('a42a9719-8416-4c44-a4f3-b3861648281f', 'd612afd7-339d-4f25-97d2-e6f6c966551e')
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource
2013-12-30 13:42:39.700 29118 INFO neutron.wsgi [-] 127.0.0.1 - - [30/Dec/2013 13:42:39] ""DELETE //v2.0/ports/a42a9719-8416-4c44-a4f3-b3861648281f HTTP/1.1"" 500 249 0.097381
It is unclear what the root cause is at the moment."
1,"In l3 agent is method ns_name() that concatenates constant NS_PREFIX with router.id that doesn't change during router's lifecycle. It's ineffective, namespace name can be an instance attribute."
0,"The current cache implementation found in cinder.api.openstack.wsgi is too simple.

Only a bunch of generic methods are provided:
- cache_resource(self, resource_to_cache, id_attribute='id', name=None)
- cached_resource(self, name=None):
- cached_resource_by_id(self, resource_id, name=None):

As you can see, the name is optional. Current API code never explicitly provides the resource type name. This could be problematic is we try to cache 2 types of resources during the same request.

Furthermore, the documentation provided in cinder.api.openstack.wsgi is wrong:

    Different resources types might need to be cached during the same
    request, they can be cached using the name parameter. For example:

        Controller 1:
            request.cache_resource(db_volumes, 'volumes')
            request.cache_resource(db_volume_types, 'types')
        Controller 2:
            db_volumes = request.cached_resource('volumes')
            db_type_1 = request.cached_resource_by_id('1', 'types')

The second parameter of cache_resource is id_attribute, not name.

To improve the situation, it has been suggested to take the Nova's implementation instead.

The Nova's implementation provides a better interface. Each resource type has a dedicated method which makes the resource type cached explicitly mentioned.

An example of such implementation for Cinder would be:
- cache_db_volumes(volumes) vs cache_resource(volumes, name='volumes')
- cache_db_volume(volume) vs cache_resource(volumes, name='volumes')
- get_db_volumes() vs cached_resource('volumes')
- get_db_volume(id) vs cached_resource_by_id(id, name='volumes')

This interface makes it clear that a volume is added or retrieved from the cache. A side-effect is that code will be shorter.

The proposed implementation will be backward compatible for people with out-of-tree extensions by preserving existing methods and still using the same variable to store the cached resources."
0,XML serialization and deserialization of updating a server metadata through the V3 API is broken. As is deserialization for retrieving a single metadata item
0,"my devstack script is failing with the above mentioned error. It is causing my CI to fail.

the stack trace is

2014-08-06 12:31:44.935 TRACE neutron Traceback (most recent call last):
2014-08-06 12:31:44.935 TRACE neutron File ""/usr/local/bin/neutron-l3-agent"", line 10, in <module>
2014-08-06 12:31:44.935 TRACE neutron sys.exit(main())
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1787, in main
2014-08-06 12:31:44.935 TRACE neutron manager=manager)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/service.py"", line 264, in create
2014-08-06 12:31:44.935 TRACE neutron periodic_fuzzy_delay=periodic_fuzzy_delay)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/service.py"", line 197, in __init__
2014-08-06 12:31:44.935 TRACE neutron self.manager = manager_class(host=host, *args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1706, in __init__
2014-08-06 12:31:44.935 TRACE neutron super(L3NATAgentWithStateReport, self).__init__(host=host, conf=conf)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 430, in __init__
2014-08-06 12:31:44.935 TRACE neutron self.plugin_rpc.get_service_plugin_list(self.context))
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 142, in get_service_plugin_list
2014-08-06 12:31:44.935 TRACE neutron version='1.3')
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
2014-08-06 12:31:44.935 TRACE neutron return method(*args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/common/rpc.py"", line 170, in call
2014-08-06 12:31:44.935 TRACE neutron context, msg, rpc_method='call', **kwargs)
2014-08-06 12:31:44.935 TRACE neutron File ""/opt/stack/neutron/neutron/common/rpc.py"", line 196, in __call_rpc_method
2014-08-06 12:31:44.935 TRACE neutron return func(context, msg['method'], **msg['args'])
2014-08-06 12:31:44.935 TRACE neutron File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-08-06 12:31:44.935 TRACE neutron retry=self.retry)
2014-08-06 12:31:44.935 TRACE neutron File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-08-06 12:31:44.935 TRACE neutron timeout=timeout, retry=retry)
2014-08-06 12:31:44.935 TRACE neutron File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 404, in send
2014-08-06 12:31:44.935 TRACE neutron retry=retry)
2014-08-06 12:31:44.935 TRACE neutron File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 395, in _send
2014-08-06 12:31:44.935 TRACE neutron raise result
2014-08-06 12:31:44.935 TRACE neutron RemoteError: Remote error: UnsupportedVersion Endpoint does not support RPC version 1.3"
1,"According to the hypervisor support matrix the vmware driver doesn't support the diagnostics API:

https://wiki.openstack.org/wiki/HypervisorSupportMatrix

But the code suggests otherwise:

https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L268

https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1263

There is an unused get_diagnostics method in vmwareapi.vmops though:

https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1284

i'm guessing that was stubbed out before the get_info method was added and someone connected the dots to the vmwareapi driver code to use it.

We should remove the unused get_diagnostics method.

Note that there aren't actually really any unit tests associated with the vmwareapi diagnostics API support though so we should add that in when we remove the unused method."
1,"Both for xml and json:

{
    ""server"": {
        ""admin_pass"": ""%(password)s"",
        ""id"": ""%(id)s"",
        ""links"": [
            {
                ""href"": ""http://openstack.example.com/v3/servers/%(uuid)s"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://openstack.example.com/servers/%(uuid)s"",
                ""rel"": ""bookmark""
            }
        ],
        ""security_groups"": [{""name"": ""test""}]
    }
}

<?xml version='1.0' encoding='UTF-8'?>
<server xmlns:atom=""http://www.w3.org/2005/Atom"" xmlns=""http://docs.openstack.org/compute/api/v1.1"" id=""%(id)s"" admin_pass=""%(password)s"">
  <metadata/>
  <atom:link href=""%(host)s/v3/servers/%(uuid)s"" rel=""self""/>
  <atom:link href=""%(host)s/servers/%(uuid)s"" rel=""bookmark""/>
  <security_groups>
   <security_group name=""test"" />
  </security_groups>
</server>

'security_groups' should be 'os-security-groups:security_groups'"
0,"The delete operations (networks, subnets and ports) haven't been managed since the 12th review of the initial support.
It seems sync_single_resource only implements create and update operations."
1,"This is found during functional testing, when .start() is called with
block=True during sightly high load.
This suggest the default timeout needs to be rised to make this module
work in all situations.
https://review.openstack.org/#/c/112798/14/neutron/agent/linux/ovsdb_monitor.py (I will extract patch from here)"
1,"The suds project appears to be largely unmaintained upstream. The default cache implementation stores pickled objects to a predictable path in /tmp. This can be used by a local attacker to redirect SOAP requests via symlinks or run a privilege escalation / code execution attack via a pickle exploit.
cinder/requirements.txt:suds>=0.4
gantt/requirements.txt:suds>=0.4
nova/requirements.txt:suds>=0.4
oslo.vmware/requirements.txt:suds>=0.4
The details are available here -
https://bugzilla.redhat.com/show_bug.cgi?id=978696
(CVE-2013-2217)
Although this is an unlikely attack vector steps should be taken to prevent this behaviour. Potential ways to fix this are by explicitly setting the cache location to a directory created via tempfile.mkdtemp(), disabling cache client.set_options(cache=None), or using a custom cache implementation that doesn't load / store pickled objects from an insecure location."
0,"The FlavorNotFound exception is handled in multiple places in the server create API, the first being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L951
and the latter being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L970
They both return a 400 response and only the former is used, so remove the latter.
This is both in the v2 and v3 API."
1,"Hey,
I am running Xubuntu 12.04.2 and I installed OpenStack Grizzly through Ubuntu Cloud Archive.
I configured nova-compute to use LVM as instance storage backend and I put these two lines in nova.conf.
libvirt_images_type=lvm
libvirt_images_volume_group=nova
When I am trying to boot a VM with a self-created flavor like:
nova flavor-create --ephemeral 10 m1.myWithEphSmall 7 4096 20 2
(with ephemeral block device != 0) the VM doesn't boot.
All VM with pre-defined flavors works fine.
here the /var/log/nova/nova-compute.log:
http://paste.openstack.org/show/41529/
some other infos:
http://paste.openstack.org/show/41533/
nova.conf:
http://paste.openstack.org/show/41534/
many thanks..."
0,"The request_id middleware is designed to generate a request ID during process_request() and attach this value to the as an HTTP header during process_response(). Unfortunately, it stores the request ID value in a variable within the RequestIdMiddleware class. This violates the ""shared nothing"" rule, and can cause problems when several requests are run concurrently. For example, if requests A and B come in back-to-back, and A completes first, A will have B's request ID value in the HTTP response.

This problem was discovered when running nova's api.compute.servers.test_instance_actions test in parallel while working on https://review.openstack.org/#/c/66903/"
0,"In c5402ef4fc509047d513a715a1c14e9b4ba9674f we added support for the new cinder V2 API.

When a user who was previously using the Cinder v1 API (which would have been required) updates to the new code the immediate defaults cause the cinder v2 API to be chosen. This is because we now default cinder_catalog_info to 'volumev2:cinder:publicURL'. So if a user was using the previous default value of 'volumev2:cinder:publicURL' their configuration would now be broken.

Given the new deprecation code hasn't been released yet I think we need to wait at least one release before we can make this change to our cinder_catalog_info default value."
1,"This is happening with the xenapi driver, but it's possible that this can happen with others. The sequence of events I'm witnessing is:
An attach_volume request is made and shortly after a terminate_instance request is made.
From the attach_volume request the block device mapping has been updated, the volume has been connected to the hypervisor, but has not been attached to the instance. The terminate request begins processing before the volume connection is attached to the instance so when it detaches volumes and their connections it misses the latest one that's still attaching. This leads to a failure when asking Cinder to clean up the volume, such as:
2014-08-06 20:30:14.324 30737 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://127.0.0.1/volumes/<uuid>/export?force=False returned '409' with 'Volume '<uuid>' is currently attached to '127.0.0.1'' (HTTP 409) (Request-ID: req-)
And in turn, when the attach_volume tries to attach the volume to the instance it finds that the instance no longer exists due to the terminate request. This leaves the instance undeletable and the volume stuck.
Having attach_volume share the instance lock with terminate_instance should resolve this. Virt drivers may also want to try to cope with this internally and not rely on a lock."
1,If you have debug logging enabled the libvirt driver's to_xml method logs the iscsi auth_password in plain text.
1,"A previous fix as already been done to manage an empty dhcp_domain:
https://bugs.launchpad.net/neutron/+bug/1099625
Hence in case of an empty dhcp_domain it remains a bug on Dnsmasq config files.
With an empty dhcp_domain Dnsmasq launch command line will be:
 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tapbdb96782-bb --except-interface=lo --pid-file=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/host --addn-hosts=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/opts --leasefile-ro --dhcp-range=set:tag0,20.0.0.0,static,86400s --dhcp-lease-max=256 --conf-file=
""addn_hosts"" file contains:
20.0.0.3 host-20-0-0-3. host-20-0-0-3
20.0.0.4 host-20-0-0-4. host-20-0-0-4
""host"" file contains:
fa:16:3e:bf:e1:e4,host-20-0-0-3.,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4.,20.0.0.4
=> for both ""addn_hosts"" and ""host"" files the hostname (2nd parameter) is ended with an extra dot char.
(it should be ""host-20-0-0-3"" instead of ""host-20-0-0-3."" )
So generated files should be:
""addn_hosts"" file:
20.0.0.3 host-20-0-0-3 host-20-0-0-3
20.0.0.4 host-20-0-0-4 host-20-0-0-4
""host"" file:
fa:16:3e:bf:e1:e4,host-20-0-0-3,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4,20.0.0.4"
0,"nova icehouse Ubuntu14.04
Version: 1:2014.1-0ubuntu1.2
Current Nova vmwareapi cannot configure guest VM's SCSI controller type as ""Paravirtual"".
(though this requires vmwaretools running on the guest VM)"
1,"With openvswitch neutron agent, during the daemon loop, the phase for setup_port_filters will try to grab/call method 'security_group_rules_for_devices' to Neutron Server.

And this operation will be very time consuming and have big performance bottleneck as it include ports query, rules query, network query as well as reconstruct the huge Security groups Dict Message. This message size is very large and for processing it, it will occupy a lot of CPU of Neutron Server. In cases like VM/perhost arrive to 700, the Neutron server will be busy doing the message and couldn't to do other thing and this could lead to message queue connection timeout and make queue disconnect the consumers. As a result the Neutron server is crashed and not function either for deployments or for API calls.

For the Noopfirewall or security group disabled situation, this operation should be avoided. Because eventually these reply message would not be used by Noopfirewall driver. (There methods are pass).

 with self.firewall.defer_apply():
            for device in devices.values():
                LOG.debug(_(""Update port filter for %s""), device['device'])
                self.firewall.update_port_filter(device)"
0,"Seeing the following warning in cinder logs (juno):

cinder.service [-] Value of config option osapi_volume_workers must be integer greater than 1. Input value ignored.

We're not specifying this in cinder.conf, so the following OpenStack code from cinder/service.py attempts to get the default:

        self.workers = getattr(CONF, '%s_workers' % name,
                               processutils.get_worker_count())
        setup_profiler(name, self.host)

        if self.workers < 1:
            LOG.warn(_(""Value of config option %(name)s_workers must be ""
                       ""integer greater than 1. Input value ignored."") %
                     {'name': name})
            # Reset workers to default
            self.workers = processutils.get_worker_count()

processutils.get_worker_count() looks like this:

    try:
        return multiprocessing.cpu_count()
    except NotImplementedError:
        return 1

and multiprocessing.cpu_count() returns this:

>>> import multiprocessing
>>> multiprocessing.cpu_count()
8

It looks to me like getattr is reading a value for osapi_volume_workers from the conf file even though there is no default set and we didn't specify this in the conf, preventing the default processutils.get_worker_count() from being returned by getattr when it should have been. It still works because processutils.get_worker_count() gets called again, but that shouldn't have been necessary, and neither should the warning (in this case)."
1,"In Juno in nova/virt/libvirt/config.py:

LibvirtConfigGuestPUNUMA.parse_dom() calls super with a capital 'D' in parse_dom().

        super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)

LibvirtConfigObject does not have a 'parse_Dom()' method. It has a 'parse_dom()' method. This causes the following exception to be raised.

...
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 1733, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack obj.parse_dom(c)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 542, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack numa.parse_dom(child)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 509, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstackAttributeError: 'super' object has no attribute 'parse_Dom'
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35"
1," so the only way to disable this option is not to have it in nova.conf. If it is set to 'false' it is treated as a string and evaluates to true in configdrive.required_by()"""
0,"I hit this locally, not sure how it got through the gate:
pep8 runtests: commands[0] | flake8
./nova/tests/objects/test_quotas.py:49:9: H233 Python 3.x incompatible use of print operator"
1,"Following files have log messages that are not translated:
neutron/agent/securitygroups_rpc.py
neutron/plugins/hyperv/agent/security_groups_driver.py
neutron/plugins/hyperv/agent/utilsfactory.py
neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"
0,"RbdTestCase in nova/tests/virt/libvirt/test_imagebackend.py now has two slightly different versions of the same test case:
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L759
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L806
The redundant version was added in:
https://review.openstack.org/82840
I think it should be removed, it doesn't do anything the original test case doesn't already have."
1,"The code recently added https://review.openstack.org/#/c/30988/ very nicely cleans out stale routers assuming that self.conf.router_delete_namespaces is true.
The problem is that automatic namespace deletion is still a bit unstable because of problems with the kernel and the iproute utility. So, many users may not have self.conf.router_delete_namespaces set to True. In this case, all of the advantages added by the above mentioned patch don't help us.
The problem arises if a router gets deleted or moved to another agent while the L3 agent is down. When the L3 agent comes back up, it will not touch the router and the router will continue to function as if it were never deleted."
1,"Steps to reproduce:
1. Create a snapshot.
2. Manually delete it from 3PAR outside of cider
3. Delete the snapshot from cinder.
Result: Snapshot stuck in error deleting state
Expected Result: Snapshot removed from cinder."
1,"During Jenkins tests I got this error two times with a different patchset
ft1.8205: nova.tests.virt.libvirt.test_libvirt.LibvirtNonblockingTestCase.test_connection_to_primitive_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{WARNING [nova.virt.libvirt.driver] URI test:///default does not support events: internal error: could not initialize domain event timer}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 7570, in test_connection_to_primitive
    jsonutils.to_primitive(connection._conn, convert_instances=True)
  File ""nova/virt/libvirt/driver.py"", line 678, in _get_connection
    wrapped_conn = self._get_new_connection()
  File ""nova/virt/libvirt/driver.py"", line 664, in _get_new_connection
    wrapped_conn.registerCloseCallback(
  File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/tpool.py"", line 172, in __getattr__
    f = getattr(self._obj,attr_name)
AttributeError: virConnect instance has no attribute 'registerCloseCallback'"
0,"We should deprecate the libvirt.api_thread_pool config option in icehouse and remove it in the J release. Looks like the last time it was touched was in January for some refactoring:
https://github.com/openstack/nova/commit/ce27bca5497600ff9ab7195e27ede94e7cffe5d0
The reasons why this should be removed are detailed here:
https://review.openstack.org/#/c/57000"
1,"2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp legacy_bdm_in_spec)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp filter_properties, instance_uuids)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 336, in _schedule
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp filter_properties, index=num)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 397, in get_filtered_hosts
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp hosts, filter_properties, index)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/filters.py"", line 82, in get_filtered_objects
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp list_objs = list(objs)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/filters.py"", line 43, in filter_all
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp if self._filter_one(obj, filter_properties):
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filters/__init__.py"", line 27, in _filter_one
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp return self.host_passes(obj, filter_properties)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filters/pci_passthrough_filter.py"", line 41, in host_passes
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp return host_state.pci_stats.support_requests(
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'support_requests'
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp"
0,"for code compatible with Python 3, we should use the ""__self__"" instead of ""im_self"".
for example :
cinder/volume/flows/common.py
def make_pretty_name(method):
    """"""Makes a pretty name for a function/method.""""""
    meth_pieces = [method.__name__]
    # If its an instance method attempt to tack on the class name
    if hasattr(method, 'im_self') and method.im_self is not None:
        try:
            meth_pieces.insert(0, method.im_self.__class__.__name__)
        except AttributeError:
            pass
    return ""."".join(meth_pieces)
For reference here(thanks Alex for adding this):
""Changed in version 2.6: For Python 3 forward-compatibility, im_func is also available as __func__, and im_self as __self__.""
http://docs.python.org/2/reference/datamodel.html"
0,"If I use block_device_mapping_v2 api to set a backend volume as boot device for instance. The instance will be launched by this remote volume successfully.
But, in such case, we can also use detach api to force this boot device being detached. So, when we do it, the guestOS of this instance would be damaged, and the whole system would not work normally if do a I/O operation. It seems can not resume.
I think we should give a warning for user at least or even forbiden this operation."
0,"The utils method get_major_minor_version was being used to get the major and minor versions from the hypervisor version. This was used to determine whether the version is compatible with the required version, to then set the device_id on a vm record during vm creation.
This method was required by the feature: https://review.openstack.org/#/c/55117/
The method is not really required, because the hypervisor version in xen is always in the form of a tuple. This can directly be used, instead of using a utils method.
This fix will include:
- Removing the utils get_major_minor_version method
- Correctly handling the hypervisor version as a tuple"
1,"I have booted an instance from a volume, successfully booted,
now another volume, i try to attach to same instance, it is failing.
see the stack trace..
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher raise exception.InvalidDevicePath(path=root_device_name)
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher InvalidDevicePath: The supplied device path (vda) is invalid.
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] Returning exception The supplied device path (vda) is invalid. to caller
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] ['Traceback (most recent call last):\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n incoming.message))\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n return self._do_dispatch(endpoint, method, ctxt, args)\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n result = getattr(endpoint, method)(ctxt, **new_args)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 401, in decorated_function\n return function(self, context, *args, **kwargs)\n', ' File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped\n payload)\n', ' File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n six.reraise(self.type_, self.value, self.tb)\n', ' File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped\n return f(self, context, *args, **kw)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 286, in decorated_function\n pass\n', ' File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n six.reraise(self.type_, self.value, self.tb)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function\n return function(self, context, *args, **kwargs)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 314, in decorated_function\n kwargs[\'instance\'], e, sys.exc_info())\n', ' File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n six.reraise(self.type_, self.value, self.tb)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 302, in decorated_function\n return function(self, context, *args, **kwargs)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 4201, in reserve_block_device_name\n return do_reserve()\n', ' File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner\n return f(*args, **kwargs)\n', ' File ""/opt/stack/nova/nova/compute/manager.py"", line 4188, in do_reserve\n context, instance, bdms, device)\n', ' File ""/opt/stack/nova/nova/compute/utils.py"", line 106, in get_device_name_for_instance\n mappings[\'root\'], device)\n', ' File ""/opt/stack/nova/nova/compute/utils.py"", line 155, in get_next_device_name\n raise exception.InvalidDevicePath(path=root_device_name)\n', 'InvalidDevicePath: The supplied device path (vda) is invalid.\n']
The reason behind this issue is: because of the root device_name being set 'vda' in the case of boot from volume, The future volume attaches to the VM fail saying ""The supplied device path (vda) is invalid"""
1,"VMware drivers cannot dynamically add iscsi targets presented to it while attaching a cinder volume. As a result, the instance cannot be attached to a cinder volume and fails with a message 'unable to find iscsi targets'.
This is because the driver fails to scan the Host Bus Adapter with the iscsi target portal (or target host). We need to fix the driver to scan the HBA by specifying the target portal."
1,"when a security group has bellow rule, it should not create ipset chain:
security group id is: fake_sgid, it has rule bellow:
{'direction': 'ingress', 'remote_group_id': 'fake_sgid2'}
but the security group:fake_sgid2 has no member, so when the port in security group:fake_sgid should not create corresponding ipset chain
root@devstack:/opt/stack/neutron# ipset list
Name: IPv409040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 16520
References: 1
Members:
20.20.20.11
Name: IPv609040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet6 hashsize 1024 maxelem 65536
Size in memory: 16504
References: 1
Members:
because the security group:09040f9f-cb86-4f72-af74-4de4f2b86442 has no ipv6 member, so it should't create ipset chain:IPv609040f9f-cb86-4f72-a"
1,"A race condition can sometimes occur in l-3 agent when a dvr based floatingip is being deleted from one router and another dvr based floatingip is being configured on another router in the same node. Especially if the floatingip being deleted was the last floatingip on the node. Although fix for Bug # 1373100 [1] eliminated frequent observation of this behavior in upstream tests, it still shows up. Couple of recent examples:
http://logs.openstack.org/88/128288/1/check/check-tempest-dsvm-neutron-dvr/8fdd1de/
http://logs.openstack.org/03/123403/7/check/check-tempest-dsvm-neutron-dvr/859534a/
Relevant log messages:
2014-10-14 16:06:15.803 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:15.838 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', 'addr', 'show', 'rfp-7ed86ca6-b']
Exit code: 0
Stdout: '2: rfp-7ed86ca6-b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n link/ether c6:88:ee:71:a7:51 brd ff:ff:ff:ff:ff:ff\n inet 169.254.30.212/31 scope global rfp-7ed86ca6-b\n valid_lft forever preferred_lft forever\n inet6 fe80::c488:eeff:fe71:a751/64 scope link \n valid_lft forever preferred_lft forever\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:15.839 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.221 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b']
Exit code: 0
Stdout: ''
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.l3_agent [-] DVR: unplug: fg-f04e25ef-e3 _destroy_fip_namespace /opt/stack/new/neutron/neutron/agent/l3_agent.py:679
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-ex'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.251 22303 ERROR neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b']
Exit code: 1
Stdout: ''
Stderr: 'Cannot find device ""rfp-7ed86ca6-b""\n'
[1] https://bugs.launchpad.net/neutron/+bug/1373100"
1,"The vmdk driver uses the number of hosts connected to a datastore as one of the criteria for selecting a datastore for volume creation. It might incorrectly consider a host to which the datastore is inaccessible while computing the number of connected hosts.
if hasattr(mount_info, ""accessible""):
            accessible = mount_info.accessible
else:
           # If accessible attribute is not set, we look at summary
            summary = self.get_summary(datastore)
            accessible = summary.accessible
If a datastore is accessible through a subset of hosts, then the value of summary.accessible will be true."
1,"Version
=======
Havana on rhel
openstack-neutron-2013.2-3.el6ost
Description
===========
The Loadbalancer agent's binary in the ""agents"" database table differs from the real binary name and service name.
# mysql -u root ovs_neutron
mysql> select * from agents where agent_type like ""Loadbalancer agent"";
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| id | agent_type | binary | topic | host | admin_state_up | created_at | started_at | heartbeat_timestamp | description | configurations |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 7f0b2ac3-5478-4a80-9c09-5856956d0160 | Loadbalancer agent | neutron-loadbalancer-agent | lbaas_process_on_host_agent | shtutgmuralegamrey.redhat.com | 1 | 2013-11-05 13:02:45 | 2013-11-05 13:02:45 | 2013-11-05 14:54:09 | NULL | {""device_driver"": ""neutron.services.loadbalancer.drivers.haproxy.namespace_driver.HaproxyNSDriver"", ""interface_driver"": ""neutron.agent.linux.interface.OVSInterfaceDriver"", ""devices"": 1} |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
The binary here is ""neutron-loadbalancer-agent"" while the real binary should probably be ""neutron-lbaas-agent"".
# rpm -ql openstack-neutron | grep neutron-loadbalancer-agent
<empty output>
# rpm -ql openstack-neutron | grep neutron-lbaas-agent
/etc/rc.d/init.d/neutron-lbaas-agent
/usr/bin/neutron-lbaas-agent
/usr/share/neutron/neutron-lbaas-agent.upstart
# service neutron-lbaas-agent status
neutron-lbaas-agent (pid 2321) is running...
# service neutron-loadbalancer-agent status
neutron-loadbalancer-agent: unrecognized service
# ps -elf | grep neutron-loadbalancer-agent | grep -v grep
<empty output>
# ps -elf | grep neutron-lbaas-agent | grep -v grep
0 S neutron 2321 1 0 80 0 - 70231 ep_pol 15:02 ? 00:01:10 /usr/bin/python /usr/bin/neutron-lbaas-agent --log-file /var/log/neutron/lbaas-agent.log --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/lbaas_agent.ini"
1,"The relevant portion of an example stack trace is below:
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packa
ges/nova/virt/xenapi/vmops.py"", line 2316, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher volume_utils.forget_sr(self._session, sr_uuid_map[sr_re
f])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vmops.py"", line 2307, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher hotplug=False)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/volumeops.py"", line 53, in attach_volume
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher vm_ref = vm_utils.vm_ref_or_raise(self._session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 2661, in vm_ref_or_raise
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher vm_ref = lookup(session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 1743, in lookup
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher vm_refs = session.call_xenapi(""VM.get_by_name_label"", name_label)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/client/session.py"", line 179, in call_xenapi
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher return session.xenapi_request(method, args)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 133, in xenapi_request
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher result = _parse_result(getattr(self, methodname)(*full_params))
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 203, in _parse_result
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher raise Failure(result['ErrorDescription'])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher Failure: ['FIELD_TYPE_ERROR', 'label']
attach_block_device_volumes() in nova/virt/xenapi/vmops.py calls attach_volume() in nova/virt/xenapi/volumops.py and passes None for instance_name, and this causes a failure when looking up the vm_ref."
1,"Http connections in connection pool to NSX controller can be reset due to reasons such as when LB is in the middle, and connection idle timeout.
In such case, while recreating the connection, NSX plugin would retry the request with ""next"" connection, which could also be reset already because of idle timeout. This would leads to continuous retry failure and the request fails finally, with a misleading error log: ""Request timeout ..."".
Error log example:
2014-10-12 11:07:18,163 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,163 43793136 DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,163 43793136 DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,164 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,164 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,164 43793136 WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,164 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,165 43793136 INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,165 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,165 43793136 DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,165 43793136 DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,166 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,166 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,166 43793136 WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,166 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,166 43793136 INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,167 43793136 DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,167 43793136 DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,167 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136 WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,168 43793136 WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,168 43793136 DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,168 43793136 INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,168 133917104 ERROR [NVPApiHelper] Request timed out: GET to /ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,169 133917104 ERROR [NeutronPlugin] An exception occured while selecting logical switch for the port
...
Suggestion to the fix issue by using the newly created connection when retry instead of using the ""next"" connection."
1,"liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
ERROR: Bad request (HTTP 400) (Request-ID: req-f8c083dc-a327-48dc-b705-293f03e834dd)
liugya@liugya-ubuntu:~$ nova service-list
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary | Host | Zone | Status | State | Updated_at | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor | liugya-ubuntu | internal | enabled | up | 2013-11-15T11:06:46.000000 | None |
| nova-compute | liugya-ubuntu | nova | disabled | up | 2013-11-15T11:06:42.000000 | | << disabled already
| nova-cert | liugya-ubuntu | internal | enabled | up | 2013-11-15T11:06:38.000000 | None |
| nova-network | liugya-ubuntu | internal | enabled | up | 2013-11-15T11:06:39.000000 | None |
| nova-scheduler | liugya-ubuntu | internal | enabled | up | 2013-11-15T11:06:40.000000 | None |
| nova-consoleauth | liugya-ubuntu | internal | enabled | up | 2013-11-15T11:06:45.000000 | None |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+"
0,"The following methods are public and should be prefixed with a _ to indicate that they are private:
     is_daemon_running
     find_container_by_name
     get_available_resource"
0,"When I try to update port for additional fixed IP or any other attributes other than portbinding, _process_portbindings_create_and_update is set 'None' in port update messages sent to the agents in spite of it has host_id.
It may not be a problem if the agents do not use portbinding information but if the agent uses 'binding:host_id' information for port update it will cause a problem, which is in my case.
When look at the codes, Neutron server sets 'binding:host_id' to 'None' as long as 'binding:host_id' is not in the requested update items. It should query DB to set correct port binding instead of 'None' in that case."
0,"-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
openstack.common.setup and openstack.common.version are now in the
standalone library pbr. Migrating involves moving build config to
setup.cfg, copying in a stub setup.py file, adding pbr and d2to1 to the
build depends, removing openstack.common.(setup|version) from the
filesystem and from openstack-common.conf and making sure *.egg is in
.gitignore.
 affects ceilometer
 affects cinder
 affects git-review
 affects heat-cfntools
 affects heat
 affects keystone
 affects openstack-ci
 affects oslo
 affects python-ceilometerclient
 affects python-cinderclient
 affects python-gear
 affects python-glanceclient
 affects python-heatclient
 affects python-keystoneclient
 affects python-novaclient
 affects python-openstackclient
 affects python-quantumclient
 affects python-swiftclient
 affects reddwarf
 affects swift
 affects zuul
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.12 (GNU/Linux)
Comment: Using GnuPG with undefined - http://www.enigmail.net/
iEYEARECAAYFAlGObdUACgkQ2Jv7/VK1RgFlkACgzycOW0/rPvnLaXXX9/oqYA7q
kGEAoMaEzGbFEAnsQA6+cEsKIUSMWAPD
=W8F0
-----END PGP SIGNATURE-----"
0,"The VIF type tests currently have separate classes that all extend the ports test class. This means in addition to testing the VIF changing logic, it's unnecessarily exercising a lot of code that is not impacted by the VIF type."
1,"Nova compute service should be disabled on a node when a connection to libvirt is lost
and resumed when libvirtd become fuctional again.
This is in order to avoid new instances or migrations to be scheduled compute node, when it'd disconnected from libvirt."
1,"The get_active_networks_info rpc call causes high number of sql query.
For example the following query
SELECT subnetroutes.destination AS subnetroutes_destination, subnetroutes.nexthop AS subnetroutes_nexthop, subnetroutes.subnet_id AS subnetroutes_subnet_id
FROM subnetroutes
WHERE %s = subnetroutes.subnet_id
was used on this trace:
  File ""/usr/lib/python2.7/site-packages/eventlet/greenpool.py"", line 82, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 92, in get_active_networks_info
    networks = self._get_active_networks(context, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 42, in _get_active_networks
    plugin.auto_schedule_networks(context, host)
  File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 211, in auto_schedule_networks
    self.network_scheduler.auto_schedule_networks(self, context, host)
  File ""/opt/stack/new/neutron/neutron/scheduler/dhcp_agent_scheduler.py"", line 114, in auto_schedule_networks
    subnets = plugin.get_subnets(context, fields=fields)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 1333, in get_subnets
    page_reverse=page_reverse)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 209, in _get_collection
    items = [dict_func(c, fields) for c in query]
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 931, in _make_subnet_dict
    for route in subnet['routes']],
  File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/models.py"", line 57, in __getitem__
    return getattr(self, key)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 237, in __get__
    return self.impl.get(instance_state(instance), dict_)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 590, in get
    value = self.callable_(state, passive)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 529, in _load_for_state
    return self._emit_lazyload(session, state, ident_key, passive)
  File ""<string>"", line 1, in <lambda>
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 598, in _emit_lazyload
    result = q.all()
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2363, in all
    return list(self)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2480, in __iter__
    return self._execute_and_instances(context)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2495, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 730, in execute
    return meth(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/sql/elements.py"", line 322, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 827, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 913, in _execute_context
    tb = str(traceback.format_stack())
1. dhcp_agent_scheduler.py explicitly specifies he is interested only in two fields ['network_id', 'enable_dhcp']
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/scheduler/dhcp_agent_scheduler.py#L103
2. _get_collection makes lazy query (not fetching every related data)
3. make_subnet_dict forces the ORM to post load not required filed
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/db/db_base_plugin_v2.py#L830
4. The self._fields drops the lazy fetched fields.
So the Db api provides the interface for efficient selective DB load, but at the and it does one of the most inefficient thing is doable on listing. Issues new SQL SELECt statements per row/object.
Looks like all function does similar thing which uses the self._field method.
The make dict methods MUST NOT try to fetch the not requested fields from the data object at all.
In this case the post _filter method also a not needed."
1,"If the consistency watchdog encounters an exception, it is uncaught and it kills the greenthread so it no longer works until the neutron server is restarted."
1,"There are 4 methods in the powervm driver that run ssh calls to the backing hypervisor (2 in blockdev, 2 in operator) and check for stderr output but only log it at debug level, which could be masking more serious issues. The stderr results should be logged at warning instead."
1,"if VIP (only with session_persistence=HTTP_COOKIE)&Pool without one member added, the Haproxy agent would report config file error like the following:
2014-04-02 10:14:10.632 17283 DEBUG neutron.openstack.common.lockutils [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Semaphore / lock released ""deploy_instance"" inner /opt/stack/neutron/neutron/openstack/common/lockutils.py:252
2014-04-02 10:14:10.633 17283 ERROR neutron.services.loadbalancer.agent.agent_manager [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Unable to deploy instance for pool: a61dd6b6-8dbe-4576-b213-f8632892a58c
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Traceback (most recent call last):
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/agent/agent_manager.py"", line 180, in _reload_pool
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager self.device_drivers[driver_name].deploy_instance(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager return f(*args, **kwargs)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 280, in deploy_instance
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager self.update(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 95, in update
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager self._spawn(logical_config, extra_args)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 110, in _spawn
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager ns.netns.execute(cmd)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 466, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager check_exit_code=check_exit_code)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 76, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager raise RuntimeError(m)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager RuntimeError:
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qlbaas-a61dd6b6-8dbe-4576-b213-f8632892a58c', 'haproxy', '-f', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/conf', '-p', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/pid', '-sf', '4524']
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Exit code: 1
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stdout: ''
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stderr: '[ALERT] 091/101410 (20752) : config : HTTP proxy a61dd6b6-8dbe-4576-b213-f8632892a58c has a cookie but no server list !\n[ALERT] 091/101410 (20752) : Fatal errors found in configuration.\n'
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager
I think one solution: check the member list, if no member exists, Haproxy agent would not deploy the instance or undeploy existed haproxy process."
0,"Files must be written with ""wb"" instead of ""w"" in order to support multiple platforms:
https://github.com/openstack/nova/blob/b823db737855149ba847e5b19df9232f109f6001/nova/virt/configdrive.py#L92"
1,"Actions are recorded in database while users take atcions in specific
instance. These actions can be listed by comand 'nova instance-action-list'.
Resize and migrate use same code path, but always record the action as
'resize' even user migrate instance."
1,"I create a LVM volume with too large size, my cinder-volumes VG's free space is insufficient. so, the volume's status is ""error"".
But, when I try to create a snapshot by this error volume, it' ok, and the snapshot' status is ""available"", progress is ""100%"".
I find that in cinder.volume.manager.py, create_shapshot, self.driver.create_snapshot return false, but only exception would be hanlde and set to error status.So, after that, the snapshot is ok in db.
some logs show that:
""""""
2014-02-22 10:55:07.439 29737 ERROR cinder.brick.local_dev.lvm [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] Unable to find LV: volume-8cb6eb05-6bfa-4422-a683-2cceb4c4ff1f
2014-02-22 10:55:07.498 29737 INFO cinder.volume.manager [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] snapshot 9e2e363b-a0ed-4be8-9874-ced1c8705fde: created successfully
"""""""
0," we can see the exception is InstanceNotFound from: https://github.com/openstack/nova/blob/master/nova/compute/api.py#L1758,
but, the exception is NotFound in https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/server_diagnostics.py#L47.
so, the NotFound should be insteaded of InstanceNotFound"
1,"When using offline migration mode with neturon-db-manage, we need to pass config file containing connection string to database. For offline migration is sufficient to know database engine. This and used plugins could be passed directly from command line."
1,"Now neutron support ipv6 address like 2001:db8::10:10:10:0/120,
but don't support ipv6 address with omiting zero like 2001:db8:0:0:10:10:10:0/120
that will cause the exception ""'2001:db8:0:0:10:10:10:0/120' isn't a recognized IP subnet cidr, '2001:db8::10:10:10:0/120' is recommended"""
0,"The current debug logs in the scheduler are at critical points in the code, and are causing performance issues.
After the DB, the scheduler is spending more time doing logging, than anything else.
This was discovered using the test_performance_check_select_destination unit test, and modifying it to look at when there are around 200 hosts, which is still quite a modest size."
0,A few methods of the Hyper-V driver ops classes use a catch / raise pattern that replaces original exceptions with specific domain exceptions inherited from vmutils.HyperVUtilsException(). The original exception is thus masqueraded and trackable only by analysing the logs.
0,"Some unit test cases in the unit test module for cisco n1kv plugin has incorrect create resource calls causing breakage in build.
This was exposed via a fix for bug/1330095"
1,"The cinder list versions API call works fine for JSON:
curl -i http://23.253.228.211:8776/ -H ""Accept: application/json"" -H ""X-Auth-Token: $token""
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 298
Date: Tue, 22 Apr 2014 17:35:12 GMT
{""versions"": [{""status"": ""CURRENT"", ""updated"": ""2012-01-04T11:33:21Z"", ""id"": ""v1.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v1/"", ""rel"": ""self""}]}, {""status"": ""CURRENT"", ""updated"": ""2012-11-21T11:33:21Z"", ""id"": ""v2.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v2/"", ""rel"": ""self""}]}]}
However, the same call with XML fails:
curl -i http://23.253.228.211:8776/.xml -H ""Accept: application/xml"" -H ""X-Auth-Token: $token""
HTTP/1.1 500 Internal Server Error
Content-Length: 189
Content-Type: application/xml; charset=UTF-8
Date: Tue, 22 Apr 2014 17:37:09 GMT
<computeFault code=""500"" xmlns=""http://docs.openstack.org/volume/api/v1""><message>The server has either erred or is incapable of performing the requested operation.</message></computeFault>
Additionally, the XML call to show v1 details fails:
curl -i http://23.253.228.211:8776/v1/.xml -H ""Accept: application/xml"" -H ""X-Auth-Token:$token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9ff05fb1-0b20-4cb7-a60f-8d956ec81dd9
Date: Tue, 22 Apr 2014 17:49:59 GMT
404 Not Found
The resource could not be found.
Call for XML to show v2 details fails:
curl -i http://23.253.228.211:8776/v2/.xml -H ""Accept: application/xml"" -H ""X-Auth-Token: $token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9f96a04b-dc35-4ceb-bb53-ab6ade803075
Date: Tue, 22 Apr 2014 17:58:52 GMT
404 Not Found
The resource could not be found.
Also, the JSON call for show cinder v2 details seems to show cinder v1:
curl -i http://23.253.228.211:8776/v2/ -H ""Accept: application/json"" -H ""X-Auth-Token: $token""
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
0,"Boto version boto==2.14.0 breaks the gate with
2013-10-10 05:15:05.036 | Traceback (most recent call last):
2013-10-10 05:15:05.036 | File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 389, in test_group_name_valid_length_security_group
2013-10-10 05:15:05.036 | self.expect_http()
2013-10-10 05:15:05.036 | File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 246, in expect_http
2013-10-10 05:15:05.037 | is_secure).AndReturn(self.http)
2013-10-10 05:15:05.037 | File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 765, in __call__
2013-10-10 05:15:05.037 | return mock_method(*params, **named_params)
2013-10-10 05:15:05.038 | File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 998, in __call__
2013-10-10 05:15:05.038 | self._checker.Check(params, named_params)
2013-10-10 05:15:05.038 | File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 929, in Check
2013-10-10 05:15:05.039 | % (' '.join(sorted(still_needed))))
2013-10-10 05:15:05.039 | AttributeError: No values given for arguments: is_secure"
0,"If I create a flavor but failed to operate on db layer , a InstanceTypeCreateFailed exception will be raised but
nova-api didn't handle it well, it will report
[root@controller ~]# nova flavor-create test1 111 512 1 1
ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-74050da0-6f78-408e-8a1b-97c2a03e597d)
give more accurate reason will be better"
1,"I think this should be caught at API level, it means instant feedback for the user and also the volume goes to ""error_extending"" which can confuse a user thinking the volume is no longer usable."
1,"While attaching the volume, the API response is that the volume state is detached, while it should be 'attaching' as per EC2's API docs.
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-AttachVolume.html"
1,"See: http://logs.openstack.org/10/56310/4/check/check-grenade-devstack-vm/29a7de6/logs/syslog.txt

UNCAUGHT EXCEPTION
Traceback (most recent call last):
  File ""/usr/local/bin/swift-container-sync"", line 10, in <module>
    execfile(__file__)
  File ""/opt/stack/new/swift/bin/swift-container-sync"", line 20, in <module>
    run_daemon('container-sync')
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 186, in run_daemon
    daemon.run(**options)
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 75, in run
    self.run_forever(**kwargs)
TypeError: run_forever() got an unexpected keyword argument 'verbose'"
1,Add a config parameter to Cisco N1kv neutron plugin to determine and control the number of active REST calls to the VSM (controller)
1,"In ""nova/virt/vmwareapi/vmops.py""
def finish_migration(self, context, migration, instance, disk_info,
                         network_info, image_meta, resize_instance=False,
                         block_device_info=None, power_on=True):
        """"""Completes a resize, turning on the migrated instance.""""""
        if resize_instance:
            client_factory = self._session._get_vim().client.factory
            vm_ref = vm_util.get_vm_ref(self._session, instance)
            vm_resize_spec = vm_util.get_vm_resize_spec(client_factory,
                                                        instance)
            reconfig_task = self._session._call_method(
                                            self._session._get_vim(),
                                            ""ReconfigVM_Task"", vm_ref,
                                            spec=vm_resize_spec)
.....................
finish_migration uses vm_util.get_vm_resize_spec() to get resize parameters.
But in ""nova/virt/vmwareapi/vm_util.py""
def get_vm_resize_spec(client_factory, instance):
    """"""Provides updates for a VM spec.""""""
    resize_spec = client_factory.create('ns0:VirtualMachineConfigSpec')
    resize_spec.numCPUs = int(instance['vcpus'])
    resize_spec.memoryMB = int(instance['memory_mb'])
    return resize_spec
the get_vm_resize_spec action does not set up disk size to resize."
1,add and save method in ImageMemberRepo are returning a new object. It is expected to modify the values in-place rather than adding them in the new object.
1,"Given a network setup like this on the network node (before starting any of the neutron services):
eth0: inet 192.168.100.230/24
eth0.1001@eth0: inet 172.24.4.224/28
Now when the linuxbridge-agent needs to create a VLAN-bridge for the 1001 VLAN it will correctly enslave eth0.1001 into the bridge and move the IP Address from eth0.1001 to the bridge.
But when linuxbridge removes the bridge again (e.g. because the last port on it was deleted) it will not reassign the IP back to the eth0.1001 device, but instead just take the whole interface down."
1,"As recommended in http://docs.openstack.org/havana/config-reference/content/section_compute-cells.html#cell-config-optional-json I'm creating the nova-cells config with the cell information stored in a json file. However, when I do this nova-cells fails to start with this error in the logs:
2014-04-29 11:52:05.240 16759 CRITICAL nova [-] __init__() takes exactly 3 arguments (1 given)
2014-04-29 11:52:05.240 16759 TRACE nova Traceback (most recent call last):
2014-04-29 11:52:05.240 16759 TRACE nova File ""/usr/bin/nova-cells"", line 10, in <module>
2014-04-29 11:52:05.240 16759 TRACE nova sys.exit(main())
2014-04-29 11:52:05.240 16759 TRACE nova File ""/usr/lib/python2.7/dist-packages/nova/cmd/cells.py"", line 40, in main
2014-04-29 11:52:05.240 16759 TRACE nova manager=CONF.cells.manager)
2014-04-29 11:52:05.240 16759 TRACE nova File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 257, in create
2014-04-29 11:52:05.240 16759 TRACE nova db_allowed=db_allowed)
2014-04-29 11:52:05.240 16759 TRACE nova File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 139, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova self.manager = manager_class(host=self.host, *args, **kwargs)
2014-04-29 11:52:05.240 16759 TRACE nova File ""/usr/lib/python2.7/dist-packages/nova/cells/manager.py"", line 87, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova self.state_manager = cell_state_manager()
2014-04-29 11:52:05.240 16759 TRACE nova TypeError: __init__() takes exactly 3 arguments (1 given)
I have had a dig into the code and it appears that CellsManager creates an instance of CellStateManager with no arguments. CellStateManager __new__ runs and creates an instance of CellStateManagerFile which runs __new__ and __init__ with cell_state_cls and cells_config_path set. At this point __new__ returns CellStateManagerFile and the new instance's __init__() method is invoked (CellStateManagerFile.__init__) with the original arguments (there weren't any) which then results in the stack trace.
It seems reasonable for CellStateManagerFile to derive the cells_config_path info for itself so I've patched it locally with
=== modified file 'state.py'
--- state.py 2014-04-30 15:10:16 +0000
+++ state.py 2014-04-30 15:10:26 +0000
@@ -155,7 +155,7 @@
             config_path = CONF.find_file(cells_config)
             if not config_path:
                 raise cfg.ConfigFilesNotFoundError(config_files=[cells_config])
- return CellStateManagerFile(cell_state_cls, config_path)
+ return CellStateManagerFile(cell_state_cls)
         return CellStateManagerDB(cell_state_cls)
@@ -450,7 +450,9 @@
 class CellStateManagerFile(CellStateManager):
- def __init__(self, cell_state_cls, cells_config_path):
+ def __init__(self, cell_state_cls=None):
+ cells_config = CONF.cells.cells_config
+ cells_config_path = CONF.find_file(cells_config)
         self.cells_config_path = cells_config_path
         super(CellStateManagerFile, self).__init__(cell_state_cls)
Ubuntu: 14.04
nova-cells: 1:2014.1-0ubuntu1
nova.conf:
[DEFAULT]
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
logdir=/var/log/nova
state_path=/var/lib/nova
lock_path=/var/lock/nova
force_dhcp_release=True
iscsi_helper=tgtadm
libvirt_use_virtio_for_bridges=True
connection_type=libvirt
root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf
verbose=True
ec2_private_dns_show_ip=True
api_paste_config=/etc/nova/api-paste.ini
volumes_path=/var/lib/nova/volumes
enabled_apis=ec2,osapi_compute,metadata
auth_strategy=keystone
compute_driver=libvirt.LibvirtDriver
quota_driver=nova.quota.NoopQuotaDriver
[cells]
enable=True
name=cell
cell_type=compute
cells_config=/etc/nova/cells.json
cells.json:
{
    ""parent"": {
        ""name"": ""parent"",
        ""api_url"": ""http://api.example.com:8774"",
        ""transport_url"": ""rabbit://rabbit.example.com"",
        ""weight_offset"": 0.0,
        ""weight_scale"": 1.0,
        ""is_parent"": true
    }
}"
1,"in /var/log/nova/nova-api.log

```
013-06-04 12:01:17.998 ERROR nova.api.openstack [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] Caught error: Network 5332f0f7-3156-4961-aa67-0b8507265fa5 is duplicated.
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack Traceback (most recent call last):
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/__init__.py"", line 81, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack return req.get_response(self.application)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack application, catch_exc_info=False)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application"
1,"The bulk-create-floating-ip and bulk-delete-floating-ip commands do not interact with floating_ip quotas. This is by design, since they're for admins rather than tenants.
However, in one case this causes a bug. If a tenant initially allocates the floating IP with create-floating-ip and consumed quota, and the admin later deletes the floating Ip with bulk-delete-floating-ip, the floating IP is freed but the quota is still consumed.
So we should change bulk-delete-floating-ip to release any quota that was associated with those floating IP addresses. (In many cases there will not be any so we need to check.)
This is https://bugzilla.redhat.com/show_bug.cgi?id=1029756 (but that bug is mostly private so won't people outside Red Hat much good)."
0,"gate-neutron-python26 failis for test_rollback_on_router_delete with following error:
2014-04-07 03:53:51,643 ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: POST failure for servers: ('localhost', 9000) Response: {'status': 'This server is broken, please try another'}
2014-04-07 03:53:51,643 ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: Error details: status=500, reason='Internal Server Error', ret={'status': 'This server is broken, please try another'}, data=""{'status': 'This server is broken, please try another'}""
}}}
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_router_db.py"", line 536, in test_rollback_on_router_delete
    expected_code=exc.HTTPInternalServerError.code)
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 204 != 500
full log is here:
http://logs.openstack.org/29/82729/3/check/gate-neutron-python26/a1065eb/testr_results.html.gz"
1,"Steps to reproduce
1. Create a new flavor, setting and add the current project as flavor access
2. The flavor is created but an error is displayed saying ""Unable to set flavor access for project.....""
The error is thrown because Horizon creates the flavor and add the access after that. The problem is that once a private flavor is created, nova adds the current project within the flavor accesses so when Horizon tries to add the access, nova throws an ""Access already exist for this flavor"" exception"
1,"neutron/agent/l3_agent.py:

 LOG.debug(""not hosting snat for router: %"", ri.router['id'])"
0,"Version
=======
Havana on rhel
Description
===========
It's possible to delete health monitor while they are associated to one or more pools, I think that this should be prohibited, and dissociation from pool should be required first."
0,"gkotton@ubuntu:~/nova$ tox nova.tests.virt.xenapi.test_xenapi
py26 create: /home/gkotton/nova/.tox/py26
ERROR: InterpreterNotFound: python2.6
py27 develop-inst-nodeps: /home/gkotton/nova
py27 runtests: commands[0] | python -m nova.openstack.common.lockutils python setup.py test --slowest --testr-args=nova.tests.virt.xenapi.test_xenapi
[pbr] Excluding argparse: Python 2.6 only dependency
running test
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --list
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --load-list /tmp/tmp349WwU
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --load-list /tmp/tmpccaidD
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --load-list /tmp/tmpMvMcYb
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --load-list /tmp/tmpzJ2QHG
Ran 201 (+199) tests in 16.446s (+0.396s)
PASSED (id=350)
Slowest Tests
Test id Runtime (s)
----------------------------------------------------------------------------------------------------------------------- -----------
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_agent_upgrade_fails_silently 16.223
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_agent_not_implemented 16.222
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_with_agent_bad_return 16.034
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_add_aggregate_host_raise_err 1.116
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_maintenance_mode 0.297
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off 0.277
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_vlanmanager 0.195
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_remote_master_non_empty_pool 0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off_with_zero_gb_old_and_new_works 0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIDom0IptablesFirewallTestCase.test_static_filters 0.171
py33 create: /home/gkotton/nova/.tox/py33
ERROR: InterpreterNotFound: python3.3"
0,PLUMgrid Director error messages should be reported at plugin level as well.
1,"When a user with admin role tries to list volumes belong to a given tenant, it does not work when all_tenants=0 is specified.
Requesting with all_tenants=0 should get the same response as requesting without all_tenants parameter.
Details:
OpenStack Release Version: IceHouse
Issue a REST API with an admin token (user's role in the specified tenant is admin).
- GET /v2/{tenant_id}/volumes
  Lists volumes in the specified tenant.
- GET /v2/{tenant_id}/volumes?all_tenants=0
  Lists volumes in all tenants.
- GET /v2/{tenant_id}/volumes?all_tenants=1
  Lists volumes in all tenants."
0,"the new jsonutils hacking check produces a pep8 traceback because it returns a set (column offset and error text) instead of an iterable (as logical line checks, like this check, should).

commit 243879f5c51fc45f03491bcb78765945ddf76be8
Change-Id: I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3

===== 8< ===== TEST CASE ===== 8< =====
$ echo 'foo = json.dumps(bar)' >nova/foobar.py
$ flake8 -vv nova/foobar.py
local configuration: in /home/dev/Desktop/nova-test
  ignore = E121,E122,E123,E124,E125,E126,E127,E128,E129,E131,E251,H405,H803,H904
  exclude = .venv,.git,.tox,dist,doc,*openstack/common*,*lib/python*,*egg,build,tools
checking nova/foobar.py
foo = json.dumps(bar)
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/.venv/bin/flake8"", line 9, in <module>
    load_entry_point('flake8==2.1.0', 'console_scripts', 'flake8')()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/main.py"", line 32, in main
    report = flake8_style.check_files()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1672, in check_files
    runner(path)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/engine.py"", line 73, in input_file
    return fchecker.check_all(expected=expected, line_offset=line_offset)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1436, in check_all
    self.check_logical()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1338, in check_logical
    for offset, text in self.run_check(check, argument_names) or ():
TypeError: 'int' object is not iterable
===== 8< ===== TEST CASE ===== 8< =====

diff --git a/nova/hacking/checks.py b/nova/hacking/checks.py
index a1dd614..7fe7412 100644
--- a/nova/hacking/checks.py
+++ b/nova/hacking/checks.py
@@ -300,7 +300,7 @@ def use_jsonutils(logical_line, filename):
         for f in json_funcs:
             pos = logical_line.find('json.%s' % f)
             if pos != -1:
- return (pos, msg % {'fun': f})
+ yield (pos, msg % {'fun': f})

 def factory(register):
===== 8< ===== PATCH ===== 8< =====

it's late, so tomorrow, if there hasn't been any activity on this, then i'll submit a patch for review."
1,"Migrate a non existent server throws HTTPBadRequest exception ,the correct is InstanceNotFound exception.
live-migrate has the same problem.
The other server actions throw InstanceNotFound exception.
I think it's a bug."
1,"When launching several VMs in rapid succession, it is possible that libvirt's image caching will fetch the same image several times. This can occur when all of the VMs in question are using the same base image and this base image has not been previously fetched. The inline fetch_func_sync method prevents multiple threads from fetching the same image at the same time, but it does not prevent a thread that is waiting to acquire the lock from fetching the image that was being fetched while the lock was still in use. This is because the presence of the image is checked only before the lock has been acquired, not after."
1,"Havana3 is installed using Packstack on CentOS 6.4.
Nova-compute dies right after start with error ""NameError: global name '_' is not defined"".
Here is the info:
* /etc/nova/nova.conf:
pci_alias={""name"":""test"", ""product_id"":""7190"", ""vendor_id"":""8086"", ""device_type"":""ACCEL""}
pci_passthrough_whitelist=[{""vendor_id"":""8086"",""product_id"":""7190""}]
 With that configuration, nova-compute fails with the following log:
* /var/log/nova/compute.log:
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
    **args)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
    result = getattr(objinst, objmethod)(context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
    self._from_db_object(context, self, db_pci)
NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup x.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return self.thread.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 166, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return self._exit_event.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/eventlet/event.py"", line 116, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return hubs.get_hub().switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py"", line 177, in switch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return self.greenlet.switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 192, in main
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup result = function(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 65, in run_service
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup service.start()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 164, in start
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self.manager.pre_start_hook()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 805, in pre_start_hook
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self.update_available_resource(nova.context.get_admin_context())
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4773, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup rt.update_available_resource(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return f(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 318, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self._sync_compute_node(context, resources)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 347, in _sync_compute_node
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self._update(context, resources, prune_stats=True)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 420, in _update
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self.pci_tracker.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/pci/pci_manager.py"", line 126, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup dev.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 134, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup ctxt, self, fn.__name__, args, kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/conductor/rpcapi.py"", line 497, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup objmethod=objmethod, args=args, kwargs=kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 85, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 63, in _invoke
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return cast_or_call(ctxt, msg, **self.kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup result = rpc.call(context, real_topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return _get_impl().call(CONF, context, topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 794, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup rpc_amqp.get_connection_pool(conf, Connection))
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 574, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup rv = list(rv)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup raise result
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup **args)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup result = getattr(objinst, objmethod)(context, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup return fn(self, ctxt, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup self._from_db_object(context, self, db_pci)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined"
1,"LibvirtGenericVIFDriver, when using the hybrid bridge method of plugging instances, needs to disable hairpinning to prevent IPv6 ICMPv6 packets from being sent back to the instance, which will cause IPv6 configuration to fail, because the instance will believe that the address it has configured has already been used."
1,"2014-03-27 14:22:13.716 ERROR cinder.volume.drivers.san.san [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c4178
0fac445b3b6e9a1ff2c805d8d] Error running SSH command: svctask addvdiskcopy -rsize 2% -autoexpand -warning 20% -grainsize 256 -easytier on -mdi
skgrp Anna_Test volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef
2014-03-27 14:22:13.717 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b8
79ba6 c9c41780fac445b3b6e9a1ff2c805d8d] CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u
'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.718 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Volume a9b15c37-f70f-4a47-8f29-101d7e3970ef: driver error when trying to retype, falling back to generic mechanism.
2014-03-27 14:22:13.719 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Bad or unexpected response from the storage volume backend API: CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.719 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1232, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager diff, host)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 734, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager self.configuration)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 703, in add_vdisk_copy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager new_copy_id = self.ssh.addvdiskcopy(vdisk, dest_pool, params)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 304, in addvdiskcopy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager return self.run_ssh_check_created(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 60, in run_ssh_check_created
2014-03-27 14:22:13.719 TRACE cinder.volume.manager out, err = self._run_ssh(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 41, in _run_ssh
2014-03-27 14:22:13.719 TRACE cinder.volume.manager raise exception.VolumeBackendAPIException(data=msg)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: CLI Exception output:
2014-03-27 14:22:13.719 TRACE cinder.volume.manager command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']"
0, there is a _get_security_groups_on_port(...) which checks that all security groups on port belong to tenant. This method is called every time a port is created. This method calls get_security_groups(...) and with 'service' tenant
1,"Only one firewall is allowed per tenant. This works as expected for non-admin tenants.
When a new firewall is added in the context of admin, this fails if some other tenant already has a firewall. This is because 'get_firewall_count' returns sum of all firewalls in the system. Addition of a new firewall for admin fails with the following error message.
500-{u'NeutronError': {u'message': u'Exceeded allowed count of firewalls for tenant tenant-2. Only one firewall is supported per tenant.', u'type': u'FirewallCountExceeded', u'detail': u''}}
fwaas_plugin.py
----------------
def create_firewall(self, context, firewall):
        LOG.debug(_(""create_firewall() called""))
        tenant_id = self._get_tenant_id_for_create(context,
                                                   firewall['firewall'])
        fw_count = self.get_firewalls_count(context)
        if fw_count:
            raise FirewallCountExceeded(tenant_id=tenant_id)
----------------
=> fw_count = self.get_firewalls_count(context)
In the context of admin, the function counts other tenant's firewall."
1,"Steps to Reproduce:
                                   1. create two network connected to the router and each network having a VM
                                   2. create firewall rule of icmp deny
                                   3. attach the firewall rule to the policy
                                   4. Create firewall with that policy and check that firewall is active
                                   5. Try to ping from one vm to another vm.
Actual Results:
                                VM is able to ping even though firewall is active. However the ping fails as expected after creating external gateway to the router.
Expected Results:
                                It should fail since the firewall is active"
1,"Hi,
I run tempest on a freshly installed packages in debian wheezy.
And tests around tempurl fail to compute the hmac in the tempurl middleware
(One of test that failed: tempest.api.object_storage.test_object_temp_url.ObjectTempUrlTest.test_put_object_using_temp_url)
The problem seems due that the HMAC key is 'unicode' instead of 'str'.
Here the error from the proxy-server daemon:
Oct 21 07:26:40 packages-va-wheezy-havana proxy-server Error: character mapping must return integer, None or unicode:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/catch_errors.py"", line 37, in handle_request
    resp = self._app_call(env)
  File ""/usr/lib/python2.7/dist-packages/swift/common/wsgi.py"", line 388, in _app_call
    resp = self.app(env, self._start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/healthcheck.py"", line 57, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py"", line 268, in __call__
    iterable = self.app(env, my_start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/memcache.py"", line 67, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/swob.py"", line 1188, in _wsgify_self
    return func(self, Request(env))(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/slo.py"", line 458, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/ratelimit.py"", line 266, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/formpost.py"", line 330, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 278, in __call__
    hmac_vals = self._get_hmacs(env, temp_url_expires, keys)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 381, in _get_hmacs
    for key in keys]
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 404, in _get_hmac
    env['PATH_INFO']), sha1).hexdigest()
  File ""/usr/lib/python2.7/hmac.py"", line 133, in new
    return HMAC(key, msg, digestmod)
  File ""/usr/lib/python2.7/hmac.py"", line 72,
Regards,
sileht"
1,"The following traces were found when VolumesActionsTest has failed:
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/console.html
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/logs/screen-q-svc.txt.gz?level=TRACE#_2014-01-04_11_56_37_489"
1,"Currently the ObjectList is mostly immutable, i.e. although the items in the list is changable, but the list itself should not be add or remove.
However the PCI manager use a ObjectList to track all the devices in the host and may add/remove, this is not correct. We should not use the Object List but a simple list to track all the devices."
1,"At the moment we are quite inconsistent with some of the error notifications.

Those triggered by the wrappers of compute manager calls are good. However:

* where an instance fault is registered inline, we don't always send an error notification
* where we send an error notification inline, we generally don't send the message and exception information in a consistent way

I am looking to resolve that, so there is more consistency when raising error notifications in the compute manager."
1,"Erwan Velu from eNovance reported a vulnerability in OpenStack Nova.
The hypervisor is passing host system uuid (-smbios version) to guests, and this happen to be a critical info leak.
The defect have been pinpointed to:
 https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L3054
From a simple virtual machine, this may allow numerous info leak like:
    Allow compute hardware enumeration from guests
    Deduce service tag and get all hardware configuration
    Ability to know if two instances are on the same compute
Dell hardware is particulary impacted as :
    - the uuid encodes the service tag
    - the service tag can be used on support site to determine:
    - detailled hardware configuration
    - date & country where the hw was shipped
    - date & type of support contract
    - amount of servers bought during this shipment
If there is no use case for this, we should scrambled that piece of information."
0," they should be renamed."""
0,"  File ""neutron/neutron/tests/functional/agent/linux/base.py"", line 59, in create_resource
    name = self.get_rand_name(n_const.DEV_NAME_MAX_LEN, name_prefix)
AttributeError: 'module' object has no attribute 'DEV_NAME_MAX_LEN'"
0,The file nova/virt/vmwareapi/fake.py should be moved to the tests directory. This is solely used in the unit tests. There is no need for this to reside in the virt directory.
1,"Catch ""does not exist"" exception of NMS scsidisk lu_exists method."
1,"If the volume creation fails, we won't be able to delete the volume. It will go from error state to error-deleting. The reason is that the delete_volume method fails when the iSCSI disk does not exist. In fact, it should skip deleting the disk if it does not exist.
Trace: http://paste.openstack.org/show/75340/"
1,"In edge_appliance_driver.py, there is a comma added when setting the datacenter moid, so the result is the value datacenter moid is changed to the tuple type, that is wrong.
 if datacenter_moid:
edge['datacenterMoid'] = datacenter_moid, ===> Should remove the ','
return edge"
1,"When spawning some instances, nova VMware driver could have a race condition in VNC port allocation. Although the get_vnc_port function has a lock it not guarantee that the whole vnc port allocation process is locked, so another instance could receive the same port if it requests the VNC port before nova has finished the vnc port allocation to another VM.

If the instances with the same VNC port are allocated in same host it could lead to a improper access to the instance console.

Reproduce the problem: Launch two or more instances at same time. In some cases one instance could execute the get_vnc_port and pick a port but before this instance has finished the _set_vnc_config another instance could execute get_vnc_port and pick the same port.

How often this occurs: unpredictable."
0,"The rpc api build_and_run_instance should be back-compatible with icehouse, as the code https://github.com/openstack/nova/blob/master/nova/compute/rpcapi.py#L887

It turn the request_network object into tuple that can be understand by icehouse code.

But the commit a8a5d44c8aca218f00649232c2b8a46aee59b77e change the request_network parameter. It add pci_request_id for each item. When request_network object turn it to tuple, it will add pci_request_id into
tuple also, that make each tuple have four items. https://github.com/openstack/nova/blob/master/nova/objects/network_request.py#L37

Old code only accept three items for each tuple: https://github.com/openstack/nova/blob/2014.1/nova/network/neutronv2/api.py#L237

Then the rpc api back-compatiblity is broken."
1,"It's not clear if n-cpu is dying trying to acquire the lock ""lock_bridge"" or if it's just hanging.
http://logs.openstack.org/08/109108/1/check/check-tempest-dsvm-full/4417111/logs/screen-n-cpu.txt.gz
The logs for n-cpu stop about 15 minutes before the rest of the test run, and all tests doing things that require the hypervisor executed after that point fail with different errors."
0,"When creating a new task, the api correctly returns a 201. However per spec it should also return a 'Location' header with the URI to the newly created resource. Currently it does not."
0,"This happened in 14 out of 156 runs. Here is a sample:
https://review.openstack.org/#/c/51751/9
http://logs.openstack.org/51/51751/9/check/check-tempest-devstack-vm-postgres-full/ab64cc3
2013-10-22 20:48:20.036 | 2013-10-22 20:41:24.129 21399 ERROR nova.virt.libvirt.driver [-] Getting disk size of instance-00000069: [Errno 2] No such file or directory: '/opt/stack/data/nova/instances/cd1428aa-fa8a-43d9-8180-888e832c35c2/disk'"
0,"code duplication between neutron.unit.services.loadbalancer.test_loadbalancer_plugin
and neutron.unit.services.vpn.test_vpnaas_extension"
1,"When creating a consistency group, the scheduler will find a backend that
supports all input volume types. If volume types are not provided, the
default_volume_type in cinder.conf will be used, however, this could cause
inconsistent behavior in a user environment where the default_volume_type
is defined in some places but not in others. This fix removed the use of
default_volume_type for CG creation, added a check to verify that
volume types are provided when creating a CG.
When creating a volume and adding it to a CG, we need to make sure a
volume type is provided as well."
0,"As of today we are seeing the following scheduler errors when trying to schedule Ironic instances:

Sep 13 16:42:48 ubuntu nova-scheduler: a/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations\n filter_properties)\n', ' File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 300, in _schedule\n chosen_host.obj.consume_from_instance(context, instance_properties)\n', 'TypeError: consume_from_instance() takes exactly 2 arguments (3 given)\n']"
0,"The Cisco Nexus plugin currently has no support for migration events via update_port when an instance is migrated from Nova. As a result the plugin currently does not do anything when an instance is migrated.

Portbinding support has been added to the Cisco plugin via this bug: https://bugs.launchpad.net/neutron/+bug/1218033

The plugin should be able to detect changes in the binding:host_id and unconfigure/reconfigure ports on the Nexus appropriately."
1,"how to duplicate
Manually creating host in 3PAR IMC with wwn from host
create volume
Attach volume
detach volume
2014-01-29 15:09:47.582 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got semaphore ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:191
2014-01-29 15:09:47.583 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Attempting to grab file lock ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:202
2014-01-29 15:09:47.584 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/loc
kutils.py:232
2014-01-29 15:09:47.584 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Connecting to 3PAR from (pid=6822) client_login /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:178
2014-01-29 15:09:48.021 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Disconnect from 3PAR from (pid=6822) client_logout /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:189
2014-01-29 15:09:48.022 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Released file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/commo
n/lockutils.py:239
2014-01-29 15:09:48.022 ERROR cinder.openstack.common.rpc.amqp [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455fc
ad] Exception during message handling
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp **args)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 821, in wrapper
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp self.driver.terminate_connection(volume_ref, connector, force=force)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp wwn=connector['wwpns'])
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connec
tion
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp hostname = self._get_3par_hostname_from_wwn_iqn(wwn, iqn)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostna
me_from_wwn_iqn
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp if wwn == fc['WWN']:
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp KeyError: 'WWN'
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp
2014-01-29 15:09:48.024 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] Returning exception 'WWN' to caller
2014-01-29 15:09:48.025 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] ['Traceback (most recent call last):\n', ' File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n **args)\n', ' File ""/op
t/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n return getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/opt/stack/cinder/ci
nder/utils.py"", line 821, in wrapper\n return func(self, *args, **kwargs)\n', ' File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
\n self.driver.terminate_connection(volume_ref, connector, force=force)\n', ' File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner\n
 retval = f(*args, **kwargs)\n', ' File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection\n wwn=connector[\'wwpns\']
)\n', ' File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connection\n hostname = self._get_3par_hostname_from_wwn_iq
n(wwn, iqn)\n', ' File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostname_from_wwn_iqn\n if wwn == fc[\'WWN\']:\n'
, ""KeyError: 'WWN'\n""]"
1,"There are typos in the following lines, as TestDhcpRpcCallbackMixin is misspelled.
test_db_rpc_base.py:24:class TestDhcpRpcCallackMixin(base.BaseTestCase):
test_db_rpc_base.py:27: super(TestDhcpRpcCallackMixin, self).setUp()
test_db_rpc_base.py:39: super(TestDhcpRpcCallackMixin, self).tearDown()"
0," it doesn't report it's version number or the driver it's using."""
0,"There are some test cases in test_compute_mgr.py using contextlib.nested to mock up functions even there is only one function.
We should use mock.patch.object directly if only mock one function.
def test_init_instance_sets_building_error(self):
        with contextlib.nested( <<<<< No need use nested here
            mock.patch.object(self.compute, '_instance_update')
          ) as (
            _instance_update,
          ):
            instance = instance_obj.Instance(self.context)
            instance.uuid = 'foo'
            instance.vm_state = vm_states.BUILDING
            instance.task_state = None
            self.compute._init_instance(self.context, instance)
            call = mock.call(self.context, 'foo',
                             task_state=None,
                             vm_state=vm_states.ERROR)
            _instance_update.assert_has_calls([call])"
0,"Launting an instance from bootable volume passing legacy bdm is available only using vda (no /dev/ prefix) as root device name. This is weird restriction. It prevents to create consistent instance data, because root_device_name instance attribute has /dev/ prefix, but device_name bdm attribute doesn't.

Environment: DevStack

Steps to reproduce:
1 Create bootable volume
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.

2 Boot instance from the volume by legacy bdm.
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=yyy:::1 inst

3 Wait instance status Active, go to instance console and look to 'No bootable device' message.

The reason is in _get_bdm_image_metadata in nova/compute/api.py. There only 'vda' devices are processed as root device for legacy bdm."
0,"Since this change https://review.openstack.org/#/c/98607/, if the conductor sends back a field of type ListOfObjects field in the updates dictionary after a remotable decorator has called the object_action RPC method, restoring them into objects will fail since they will already be 'hydrated' but the field's from_primitive logic won't know hot to deal with that."
1,"http://logs.openstack.org/42/44542/1/gate/gate-tempest-devstack-vm-postgres-full/2d73c40/console.html
2013-09-03 19:29:16.317 | ======================================================================
2013-09-03 19:29:16.318 | FAIL: tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | ----------------------------------------------------------------------
2013-09-03 19:29:16.319 | _StringException: Empty attachments:
2013-09-03 19:29:16.319 | stderr
2013-09-03 19:29:16.319 | stdout
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.320 | pythonlogging:'': {{{2013-09-03 19:23:46,092 running: '/usr/local/bin/cinder --os-username admin --os-tenant-name admin --os-password secret --os-auth-url http://127.0.0.1:5000/v2.0/ quota-class-show abc'}}}
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.321 | Traceback (most recent call last):
2013-09-03 19:29:16.321 | File ""tempest/cli/simple_read_only/test_cinder.py"", line 56, in test_cinder_quota_class_show
2013-09-03 19:29:16.321 | params='abc'))
2013-09-03 19:29:16.322 | File ""tempest/cli/__init__.py"", line 86, in cinder
2013-09-03 19:29:16.322 | 'cinder', action, flags, params, admin, fail_ok)
2013-09-03 19:29:16.323 | File ""tempest/cli/__init__.py"", line 102, in cmd_with_auth
2013-09-03 19:29:16.323 | return self.cmd(cmd, action, flags, params, fail_ok)
2013-09-03 19:29:16.323 | File ""tempest/cli/__init__.py"", line 123, in cmd
2013-09-03 19:29:16.324 | result)
2013-09-03 19:29:16.324 | CommandFailed: Command '['/usr/local/bin/cinder', '--os-username', 'admin', '--os-tenant-name', 'admin', '--os-password', 'secret', '--os-auth-url', 'http://127.0.0.1:5000/v2.0/', 'quota-class-show', 'abc']' returned non-zero exit status 1"
0," there should be an external sanity check added that tests the running systems ability to import neutron.notifiers.nova."""
0,"diff --git a/cinder/tests/image/fake.py b/cinder/tests/image/fake.py
index 18cea57..9b5a83f 100644
--- a/cinder/tests/image/fake.py
+++ b/cinder/tests/image/fake.py
@@ -229,8 +229,6 @@ def FakeImageService_reset():

 def stub_out_image_service(stubs):
- def fake_get_remote_image_service(context, image_href):
- return (FakeImageService(), image_href)
     stubs.Set(cinder.image.glance, 'get_remote_image_service',
               lambda x, y: (FakeImageService(), y))
     stubs.Set(cinder.image.glance, 'get_default_image_service',"
1,"On a dev machine I've recently create I noticed failures at startup when Neutron is configured with the NVP plugin.
I root caused the failure to port being explicitly passed to HTTPSConnection constructor as a string rather than an integer.
This can be easily fixed ensuring port is always an integer.
I am not sure of the severity of this bug as it might strictly related to this specific dev env, but it might be worth applying and backporting it"
1,"SIGTERM should be handled properly and agent should exit with 0.
https://github.com/openstack/neutron/blob/master/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py#L1444"
1,"q-svc frequently tries to iterate on None Type.
The job can succeed even if this issue happens.
message: ""Exception during message handling"" AND message:""NoneType"" AND message:""object is not iterable"" AND filename:""logs/screen-q-svc.txt""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkV4Y2VwdGlvbiBkdXJpbmcgbWVzc2FnZSBoYW5kbGluZ1wiIEFORCBtZXNzYWdlOlwiTm9uZVR5cGVcIiBBTkQgbWVzc2FnZTpcIm9iamVjdCBpcyBub3QgaXRlcmFibGVcIiBBTkQgZmlsZW5hbWU6XCJsb2dzL3NjcmVlbi1xLXN2Yy50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA1NTMzMDE3NzE4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
 [req-ef892503-3f93-4c68-adeb-17394b66406c ] Exception during message handling: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 76, in _ensure_host_set_on_ports
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher interface)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 84, in _ensure_host_set_on_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher {'port': {portbindings.HOST_ID: host}})
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 870, in update_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher need_notify=need_port_update_notify)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 302, in _bind_port_if_needed
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher plugin_context, port_id, binding, bind_context)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher"
1,"Ephemeral drives are destinaton == local, but the new bdm code bases it on source instead. This leads to improper errors:
$ nova boot --flavor m1.tiny --block-device source=blank,dest=volume,bus=virtio,size=1,bootindex=0 test
ERROR (BadRequest): Ephemeral disks requested are larger than the instance type allows. (HTTP 400) (Request-ID: req-53247c8e-d14e-43e2-b01e-85b49f520e61)
The code is here:
https://github.com/openstack/nova/blob/106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71/nova/block_device.py#L411
This should be checking destination_type == 'local' instead of source type."
0,More Radware LBaaS driver unit-testing should be added
1,"In server`s action `confirmResize` status code in @wsgi.response decorator is set as 202 but this is overridden/ignored by return statement (return exc.HTTPNoContent()) which return 204 status code - https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L1080
This is very confusing and we should have expected status code in @wsgi.response decorator as consistence with other APIs.
There is no change required in API return status code but in code it should be fixed to avoid the confusion."
0,"This method needs to be split in several small methods then each methods has to be tested.
A possible solution could be:
  * determines the disk size from instance properties
  * methods to convert disk from qcow2 to raw and raw to qcow2
  * method to resize the disk"
0,"As quantum.agent.linux.dhcp.Dnsmasq makes use of the --server command line when starting dnsmasq there can only be one server configured for the dnsmasq_dns_server option in dhcp_agent.ini

This is not ideal.

Ideally if a network is not created with its own dns server options, the options provided in dnsmasq_dns_server should be used and put into the opts file just as the network-configured options would be.

This would enable dns to work without dns options configured on the network using defaults provided by the deployer."
0,"If we use RBD as the backend for ephemeral drives, compute nodes still calculate their available disk size looking back to the local disks.
This is the path how they do it:

* nova/compute/manager.py

    def update_available_resource(self, context):
        """"""See driver.get_available_resource()

        Periodic process that keeps that the compute host's understanding of
        resource availability and usage in sync with the underlying hypervisor.

        :param context: security context
        """"""
        new_resource_tracker_dict = {}
        nodenames = set(self.driver.get_available_nodes())
        for nodename in nodenames:
            rt = self._get_resource_tracker(nodename)
            rt.update_available_resource(context)
            new_resource_tracker_dict[nodename] = rt
....................
    def _get_resource_tracker(self, nodename):
        rt = self._resource_tracker_dict.get(nodename)
        if not rt:
            if not self.driver.node_is_available(nodename):
                raise exception.NovaException(
                        _(""%s is not a valid node managed by this ""
                          ""compute host."") % nodename)

            rt = resource_tracker.ResourceTracker(self.host,
                                                  self.driver,
                                                  nodename)
            self._resource_tracker_dict[nodename] = rt
        return rt

* nova/compute/resource_tracker.py

    def update_available_resource(self, context):
        """"""Override in-memory calculations of compute node resource usage based
        on data audited from the hypervisor layer.

        Add in resource claims in progress to account for operations that have
        declared a need for resources, but not necessarily retrieved them from
        the hypervisor layer yet.
        """"""
        LOG.audit(_(""Auditing locally available compute resources""))
        resources = self.driver.get_available_resource(self.nodename)

* nova/virt/libvirt/driver.py

    def get_local_gb_info():
        """"""Get local storage info of the compute node in GB.

        :returns: A dict containing:
             :total: How big the overall usable filesystem is (in gigabytes)
             :free: How much space is free (in gigabytes)
             :used: How much space is used (in gigabytes)
        """"""

        if CONF.libvirt_images_type == 'lvm':
            info = libvirt_utils.get_volume_group_info(
                                 CONF.libvirt_images_volume_group)
        else:
            info = libvirt_utils.get_fs_info(CONF.instances_path)

        for (k, v) in info.iteritems():
            info[k] = v / (1024 ** 3)

        return info

It would be nice to have something like ""libvirt_utils.get_rbd_info"" which could be used in case CONF.libvirt_images_type == 'rbd'"
1,"When there are 100's of VM deployed there are problems with nova compute. This is due to the fact that each interaction with the VM;s via get_vm_ref reads all of the VM's ont he system and then filters by the UUID. The filtering is done on the client side.
There are specific API's that optimize this search - http://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.wssdk.apiref.doc%2Fvim.SearchIndex.html more specifically FindAllByUuid"
1,"The neutron network API currently throws a error code 500 for an invalid input against the VLAN field.
The error can be reproduced by having the following JSON request body:
{
            ""network"": {
                ""admin_state_up"": ""false"",
                ""provider:segmentation_id"": ""abc"",
                ""name"": ""Network1"",
                ""provider:physical_network"": ""XYZ"",
                ""provider:network_type"": ""vlan""
            }
        }
An error code 400 should be thrown much like how it is thrown for the other fields - if they correspond to incorrect values."
0,The help text for gpfs config flags should be editted to be more clear.
1,"The call to _get_direct_account_container in direct_get_account
has several of its args =None instead of set to the value passed
to direct_get_account. Similarly it is not passing the timeout args.
The same applies to _get_direct_account_container in
direct_get_container.
The direct_get_container is only called by the account-reaper
and this bug will have limited impact on it. The marker,
maintained in reap_container, is ignored by direct_get_container.
This is not as bad as it sounds, if the account-reaper successfully
deletes the first 10K objects, assuming the container has > 10K
objects, the next call to direct_get_container will in fact return
the next 10K objects even though it sets marker=None (assuming the
first 10K objects were successfully deleted)."
1,"Reproduce:
1.nova keypair-list
+----------+-------------------------------------------------+
| Name | Fingerprint |
+----------+-------------------------------------------------+
| root_key | 41:f3:fc:23:07:1d:99:cc:fd:e4:7a:a3:20:ba:78:25 |
+----------+-------------------------------------------------+
2.nova keypair-show root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-542fa1da-0ab0-4624-b662-7d7c908508e2)
3.nova keypair-delete root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-2f8587a3-ee5e-4134-ba5d-a2b3f0968cbc)
expected:
1.nova keypair-show root
ERROR: No keypair with a name or ID of 'root' exists.
2.nova keypair-delete root
ERROR: No keypair with a name or ID of 'root' exists."
1,"Saw this while looking at logs for bug 1290468:
http://logs.openstack.org/85/78385/3/gate/gate-grenade-dsvm/2b66f90/logs/new/screen-c-api.txt.gz
You'll see a ton of errors like this:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 705, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 669, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'
Logged from file middleware.py, line 100
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiS2V5RXJyb3I6IFxcJ3VzZXJfaWRlbnRpdHlcXCdcIiBBTkQgZmlsZW5hbWU6bG9ncypzY3JlZW4tYy1hcGkudHh0IiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzk0NDc2MTE4MTQ2fQ==
Thinking it's related to this:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L370
Does that need to be popped off kwargs?
From the review https://review.openstack.org/#/c/55938/ it looks like Ben and Doug were questioning it.
Also wondering if that's somehow impacted by the logging format condition used here:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L657
Because logging_context_format_string is the format that uses user_identity.
This started showing up on 3/7 which is when the lazy translation code was enabled in Cinder so it looks like that is exposing the bug."
1,"The Hyper-V agent throws exceptions each time it tries to add a security group rule which has ICMP as protocol.
This is caused by the fact that Hyper-V Msvm_EthernetSwitchPortExtendedAclSettingData can accept only TCP and UDP as text as protocols. Any other protocol must be a string of the protocol number.
Source: http://technet.microsoft.com/en-us/library/dn464289.aspx
Agent log:
http://pastebin.com/HwrczsfX"
0,"This happens on master and appears to be a regression of https://review.openstack.org/#/c/112465/
Steps to reproduce:
- Deploy devstack with DVR *ON*
- Run tempest test test_volume_boot_pattern (more details available in [1])
- Watch the test fail
Once change 112465 is reverted, the test will be successful.
This does not imply that the change is at fault, instead, that another failure mode has been unveiled.
In fact, prior to change 112465, call [2] was made with the wrong agent id, which caused the delete_port operation to abort altogether; this is wrong and addressed in 112465. However, the boot from volume test, and its sequence of operations revealed that the clean-up logic is not working as it should.
[1] - https://github.com/openstack/tempest/blob/master/tempest/scenario/test_volume_boot_pattern.py#L31
[2] - https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L1029"
1,"As shown by the stack trace below, when a volume is attached but the VM is not present the volume can't be cleaned up by Cinder and will raise an Exception which puts the instance into an error state. The volume attachment isn't removed because an if statement is hit in the xenapi destroy method which logs ""VM is not present, skipping destroy..."" and then moves on to trying to cleanup the volume in Cinder. This is because most operations in xen rely on finding the vm_ref and then cleaning up resources that are attached there. But if the volume is attached to an SR but not associated with an instance it ends up being orphaned.
014-08-29 15:54:02.836 8766 DEBUG nova.volume.cinder [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] Cinderclient connection created using URL: https://localhost/v1/<tenant>
cinderclient /opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py:108
2014-08-29 15:54:03.251 8766 ERROR nova.compute.manager [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] [instance: <uuid>] Setting instance vm_state to ERROR
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] Traceback (most recent call last):
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2443, in do_terminate_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._delete_instance(context, instance, bdms, quotas)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/hooks.py"", line 131, in inner
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] rv = f(*args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2412, in delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] quotas.rollback()
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 82, in exit
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] six.reraise(self.type, self.value, self.tb)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2390, in _delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._shutdown_instance(context, instance, bdms)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2335, in _shutdown_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 189, in wrapper
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] res = method(self, ctx, volume_id, *args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 309, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 331, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] {'connector': connector})
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self.api.client.post(url, body=body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 223, in post
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self._cs_request(url, 'POST', **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 187, in _cs_request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 170, in request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] raise exceptions.from_response(resp, body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://localhost:8081/volumes/<volume_uuid>/export?force=False returned '409' with 'Volume '<volume_uuid>' is currently attached to '<ip>'' (HTTP 409) (Request-ID: req-d8a81cfc-5ba2-4bfb-b519-c92a2"
1,"cinder/cinder/image/glance.py is used as follows.
  raise exception.GlanceConnectionFailed(netloc=netloc,
                                         reason=str(e))
'netloc' is always ignored."
1,"Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68 added typed consoles, and updated drivers to use them. However, when it touched the VMware driver, it modified get_vnc_console in VMwareVMOps, but not in VMwareVCVMOps, which is the one which is actually used.

Incidentally, VMwareVMOps has now been removed, so this type of confusion should not happen again."
0,"https://bugs.launchpad.net/neutron/+bug/1045613 is no more valid because ""patch port support has been added to the upstream Linux kernel OVS implementation"" (comment #4). That's why we can use openvswitch patch ports to interconnect br-int to physnets in order to use the same interconnection technology with br-tun and physnets and increase performance (according to http://www.opencloudblog.com/?p=96)."
1,"When showing the floating ips by host, I got this error.
$ nova floating-ip-bulk-list --host xxx
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-9a17c8cf-b1cd-4092-b853-7e24126db7e8)
and in the nova-api log I got this message:
014-03-05 04:01:40.270 ERROR nova.api.openstack [req-9a17c8cf-b1cd-4092-b853-7e24126db7e8 admin demo] Caught error: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack Traceback (most recent call last):
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 125, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return req.get_response(self.application)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2014-03-05 04:01:40.270 TRACE nova.api.openstack application, catch_exc_info=False)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2014-03-05 04:01:40.270 TRACE nova.api.openstack app_iter = application(self.environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 598, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return self.app(env, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack response = self.app(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack resp = self.call_func(req, *args, **self.kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-03-05 04:01:40.270 TRACE nova.api.openstack return self.func(req, *args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 929, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack content_type, body, accept)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 991, in _process_stack
2014-03-05 04:01:40.270 TRACE nova.api.openstack action_result = self.dispatch(meth, request, action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1078, in dispatch
2014-03-05 04:01:40.270 TRACE nova.api.openstack return method(req=request, **action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 48, in show
2014-03-05 04:01:40.270 TRACE nova.api.openstack return self._get_floating_ip_info(context, id)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 57, in _get_floating_ip_info
2014-03-05 04:01:40.270 TRACE nova.api.openstack floating_ips = db.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/api.py"", line 356, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack return IMPL.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-03-05 04:01:40.270 TRACE nova.api.openstack return f(*args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 918, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack raise exception.FloatingIpNotFoundForHost(host=host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack FloatingIpNotFoundForHost: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack
The FloatingIpNotFoundForHost exception should be handled."
0,"This bug is a follow-up to Nikola Dipanov's comment in https://review.openstack.org/#/c/109834/2/nova/compute/manager.py.
The logic to identify volumes is currently a nested function in _default_block_device_names, named _is_mapping. It should be moved to a more general place so others could utilize it."
1,"When Distributed Routers are created with Interface Ports and after a Gateway is set to the router, the plugin will create the ""csnat"" interface ports with ""device_owner"" as ""router_csnat_interface"".
These ports should be deleted when the Gateway is cleared or when the interfaces are removed from the particular router.
In the current plugin code, these ""interface"" ports are not deleted when a Gateway is cleared. But the ports are deleted when the ""router interfaces are removed"".
This needs to be fixed.
Since we don't clean up the ports there may be an odd chance of having unused ports in the ""Service-node""."
1,"If an instance fails during its network creation (for example if the network-vif-plugged event doesn't arrive in time) a subsequent delete will also fail when it tries to delete the vif, leaving the instance in a Error(deleting) state.
This can be avoided by including the ""--if-exists"" option to the ovs=vsctl command.
Example of stack trace:
 2014-04-16 12:28:51.949 AUDIT nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Terminating instance
2014-04-16 12:28:52.309 ERROR nova.virt.libvirt.driver [-] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] During wait destroy, instance disappeared.
2014-04-16 12:28:52.407 ERROR nova.network.linux_net [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] Unable to execute ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']. Exception: Unexpected error while running command.
Command: sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 del-port br-int qvo67a96e96-10
Exit code: 1
Stdout: ''
Stderr: 'ovs-vsctl: no port named qvo67a96e96-10\n'
2014-04-16 12:28:52.573 ERROR nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Setting instance vm_state to ERROR
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Traceback (most recent call last):
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/compute/manager.py"", line 2261, in do_terminate_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self._delete_instance(context, instance, bdms, quotas)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/hooks.py"", line 103, in inner
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] rv = f(*args, **kwargs)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/compute/manager.py"", line 2231, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] quotas.rollback()
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/compute/manager.py"", line 2203, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self._shutdown_instance(context, db_inst, bdms)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/compute/manager.py"", line 2145, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] requested_networks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/compute/manager.py"", line 2135, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] block_device_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 955, in destroy
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] destroy_disks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 991, in cleanup
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self.unplug_vifs(instance, network_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 863, in unplug_vifs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self.vif_driver.unplug(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self.unplug_ovs(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] self.unplug_ovs_hybrid(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] v2_name)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1318, in delete_ovs_vif_port
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] _ovs_vsctl(['del-port', bridge, dev])
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] raise exception.AgentError(method=full_args)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']"
1,"There are several nova virt drivers that don't implement the rescue API, but the os compute API doesn't handle NotImplementedError, it returns a 400 instead of a 501.
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
The API could be tightened up a bit to return a 501 instead like how the pause admin action is handling NotImplementedError."
1,"When a gateway device is created, the client certificate is not stored anywhere on the neutron server, but then passed directly to the backend, which validates the certificate.
Currently the NSX backend raises an exception when the certificate is not valid.
This exception is treated by the NSX plugin as a backend failure and a 500 is then returned.
However, the correct error would a 400 with an appropriate error message."
1,"q-svc and q-dhcp services are printing the following stack traces in their screen tabs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 850, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 566, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 530, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: u'project_name'
As a workaround, colorized logging config could be fixed to remove project_name from the list of fields in neutron.conf and that solves the issue. However new devstack setup brings the problem back."
1,"The base object infrastructure has been comparing Object.version instead of the Object.VERSION that *all* the objects have been setting and incrementing when changes have been made. Since the base object defined a .version, and that was used to determine the actual version of an object, all objects defining a different VERSION were ignored.

All systems in the wild currently running broken code are sending version '1.0' for all of their objects. The fix is to change the base object infrastructure to properly examine, compare and send Object.VERSION.

Impact should be minimal at this point, but getting systems patched as soon as possible will be important going forward."
1,"I used a normal user to create a network successfully,then I wanted to update the ""shared"" property of the network.
It failed,and response 404 erorr,the message is :The resource could not be found.But I have created the network,it is so strange.
I check the policy.json of neutron, the rule is: ""update_network:shared"": ""rule:admin_only"", so the normal user can't update it.
So the error information is wrong.
Check the code:
    def update(self, request, id, body=None, **kwargs):
        """"""Updates the specified entity's attributes.""""""
      ......
      ......
        try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            msg = _('The resource could not be found.')
            raise webob.exc.HTTPNotFound(msg)
I think we couldn't provide the wrong response information to avoid giving away information,and there isn't any information that need to avoid giving away here, So I think it is a bug.
I suggest to modify the code like this:
       try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            # msg = _('The resource could not be found.')
            raise webob.exc.HTTPForbidden(exceptions.PolicyNotAuthorized.message)"
1,"If Claim is not successful, compute manager triggers a call to destroy instance.
Destroy fails since the compute node (cluster) is set only after claim is successful.
This issue occurs when multiple parallel nova boot operations are triggered simultaneously.
Snippet from nova-compute.log
2014-04-06 22:48:52.454 [00;32mDEBUG nova.compute.utils [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mInsufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m with rt.instance_claim(context, instance, limits):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m return f(*args, **kwargs)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m overhead=overhead, limits=limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m self._claim_test(resources, limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m ""; "".join(reasons))
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m
2014-04-06 22:48:52.455 [00;32mDEBUG nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mClean up resource before rescheduling.[00m [00;33mfrom (pid=9041) _reschedule_or_error /opt/stack/nova/nova/compute/manager.py:1401[00m
2014-04-06 22:48:52.455 [01;36mAUDIT nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[01;36m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [01;36mTerminating instance[00m
2014-04-06 22:48:52.544 [00;32mDEBUG nova.network.api [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[00;32mUpdating cache with info: [][00m [00;33mfrom (pid=9041) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/api.py:74[00m
2014-04-06 22:48:52.555 [00;32mDEBUG nova.objects.instance [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[00;32mLazy-loading `system_metadata' on Instance uuid b22186ec-9f05-4f7d-a0d6-2276baeb6572[00m [00;33mfrom (pid=9041) obj_load_attr /opt/stack/nova/nova/objects/instance.py:519[00m
2014-04-06 22:48:52.563 [00;32mDEBUG nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mDeallocating network for instance[00m [00;33mfrom (pid=9041) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1784[00m
2014-04-06 22:48:52.593 [01;31mERROR nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[01;31m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [01;31mError: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m with rt.instance_claim(context, instance, limits):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m return f(*args, **kwargs)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m overhead=overhead, limits=limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m self._claim_test(resources, limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m ""; "".join(reasons))
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m
2014-04-06 22:48:52.664 [00;32mDEBUG nova.compute.utils [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mThe resource None does not exist[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1202, in _run_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m instance, image_meta, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1366, in _build_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m filter_properties, bdms, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1412, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m self._log_original_error(exc_info, instance_uuid)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m bdms, requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 2136, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/compute/manager.py"", line 2126, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m block_device_info)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 656, in destroy
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m _vmops = self._get_vmops_for_compute_node(instance['node'])
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 544, in _get_vmops_for_compute_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m resource = self._get_resource_for_node(nodename)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 536, in _get_resource_for_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m raise exception.NotFound(msg)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mNotFound: The resource None does not exist"
1,"After ImageCacheManager judges a base image is not used recently and marks it as to be removed, there is some time before the image is actually removed. So if an instance using the image is launched during the time, the image will be removed unfortunately."
1,"When I try to live migration a VM, I got the following exception from source host.

2014-04-04 12:50:20.330 8862 INFO nova.compute.manager [-] [instance: 160fb719-7f84-466a-a19d-9284dd6d56fa] NV-FA2EA85 _post_live_migration() is started..
2014-04-04 12:50:20.371 8862 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/openstack/common/loopingcall.py"", line 78, in _inner
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall self.f(*self.args, **self.kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4495, in wait_for_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall migrate_data)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall payload)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall return f(self, context, *args, **kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 315, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall e, sys.exc_info())
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 302, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall return function(self, context, *args, **kwargs)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4600, in _post_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall ctxt, instance.uuid)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall AttributeError: 'dict' object has no attribute 'uuid'
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall"
0,"Huawei drivers create Linux hosts by default when attaching volumes. It's necessary to support XenServer, Windows, VMware ESX, etc."
1,"It is currently possible to have swift-recon report on disk usage by two means:
swift-recon -d OR
swift-recon --all (which includes -d)

Disk usage provides two options:
--human-readable to provide usage in MB/GB/TB++ rather than bytes
--top <integer> to list devices with the most usage

These options are currently usable when using -d but not --all.
This is because the options are not passed to disk_usage when using all:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L930

Versus when using -d:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L965"
1,"In downgrade for 4eca4a84f08a_remove_ml2_cisco_cred_db there is a mistake in usage SQLAlchemy String type. Used sa.string instead of sa.String

akamyshnikova@akamyshnikova:/opt/stack/neutron$ neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini downgrade -10"
0,"Normal operation of the openvswitch-agent results in various warning and error messages being logged to /var/log/openvswitch/ovs-vswitchd.log, which, although not representing actual failures, can cause concern.
For example, setting up the patch port connecting br-int and br-tun results in:
2013-12-02T16:35:45Z|00103|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00104|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00105|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00106|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00107|bridge|INFO|bridge br-int: added interface patch-tun on port 1
2013-12-02T16:35:46Z|00108|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device patch-int failed: No such device
2013-12-02T16:35:46Z|00109|dpif|WARN|system@ovs-system: failed to add patch-int as port: No such device
2013-12-02T16:35:46Z|00110|netdev_vport|ERR|patch-int: patch type requires valid 'peer' argument
2013-12-02T16:35:46Z|00111|bridge|WARN|could not configure network device patch-int (Invalid argument)
2013-12-02T16:35:46Z|00112|bridge|INFO|bridge br-tun: added interface patch-int on port 1
And setting up a tunnel port on br-tun results in:
2013-12-02T16:35:48Z|00113|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device gre-1 failed: No such device
2013-12-02T16:35:48Z|00114|dpif|WARN|system@ovs-system: failed to add gre-1 as port: No such device
2013-12-02T16:35:49Z|00115|netdev_vport|ERR|gre-1: gre type requires valid 'remote_ip' argument
2013-12-02T16:35:49Z|00116|bridge|WARN|could not configure network device gre-1 (Invalid argument)
2013-12-02T16:35:49Z|00117|bridge|INFO|bridge br-tun: added interface gre-1 on port 2
These messages seem to be due to the neutron.agent.linux.ovs_lib.add_patch_port() and neutron.agent.linux.ovs_lib.add_tunnel_port() functions setting DB attributes one-by-one, which results in invalid intermediate states. In addition to the noise in the log files, attempts to use the ports while in these intermediate states could result in transient incorrect behavior.
This can be resolved by changing these functions to atomically create and configure the OVS ports using a single ovs-vsctl command, with multiple sub-commands separated by ""--"" as arguments. This would also have the benefit of reducing the overhead of executing multiple commands via rootwrap when a single command would do."
0,"We've been seeing things that appear to be races between the hosts files being written out for dnsmasq and dhcp requests coming in. We will get occasional errors from dnsmasq saying ""no address available"", ""duplicate IP address"" but by the time you look, the corresponding host file has long since been replaced.
Outputting the dnsmsaq config file in the debug logs would help in establishing what the DHCP server state was at the time of the problem"
1,"If you configure the NetApp cinder driver with clustered mode and use a login/password with only the vsadmin role, many things will not work and APIs will fail with stack traces."
1,"Libvirt's close callback causes deadlocks
Unlike libvirt's lifecycle event callback which is triggered every time an event occurs, the close callback is only triggered when an attempt is made to use a connection that has been closed. In that case, the sequence of events is usually as follows:
LibvirtDriver._get_connection acquires _wrapped_conn_lock
LibvirtDriver._get_connection calls _test_connection
LibvirtDriver._test_connection calls libvirt.getLibVersion
libvirt.getLibVersion triggers LibvirtDriver._close_callback (because the connection is closed and this method is the registered handler)
LibvirtDriver._close_callback attempts to acquire _wrapped_conn_lock
_get_connection cannot release the lock because it is waiting for _close_callback to return and the latter cannot complete until it has acquired the lock.
Making the handling of the close callback asynchronous (like the lifecycle event handling) won't work, because by the time the lock is released, the connection object that was passed into the callback will no longer be equal to LibvirtDriver._wrapped_conn. Even if the connection object is ignored, the instance will have already been disabled via the _get_new_connection method's existing error-handling logic.
The best solution would appear to be to simply not register a close callback. The only case where it might provide some benefit is when a connection is closed after _get_connection has returned a reference to it. The benefit of disabling the instance a little earlier in such marginal cases is arguably outweighed by the complexity of a thread-safe implementation, especially when the difficulty of testing such an implementation (to ensure it is indeed thread safe) is taken into consideration.
Note that having _close_callback use _wrapped_conn_lock.acquire(False) instead of ""with _wrapped_conn_lock"" by itself is not a viable solution, because due to connections being opened via tpool.proxy_call, the close callback is called by a native thread, which means it should not be used to perform the various operations (including logging) involved in disabling the instance."
1,"The following appears in the log files when this happens:
[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi:vmops:destroy, got this exception while deleting the VM contents from the disk: local variable 'datastore_name' referenced before assignment"
1,"Referring commit 61a715e17e8d6e6dbe60d35447c70fba990bd2e9
https://github.com/openstack/glance/blob/master/glance/store/gridfs.py#L99
Fix typo:
msg = (""Missing dependecies: pymongo"")
It should be,
msg = (""Missing dependencies: pymongo"")"
0,"The base FibreChannelDriver doesn't implement the validate_connector() method. It falls back to the parent class which simply does a pass. The FC Driver should do a check on the connector and ensure that is has wwnns, wwpns to ensure that the child classes can actually do a volume attach. Without wwnns, wwpns, no FC driver can successfully do initialize_connection."
1,"No handlers could be found for logger when running lefthand tests.
Here's an example from https://jenkins03.openstack.org/job/gate-cinder-python26/477/console:
2014-05-30 17:27:15.848 | No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy"""
1,"nova-network with floating IPs is currently broken due to a direct DB access
2014-04-22 08:04:32.395 ESC[01;31mERROR oslo.messaging.rpc.dispatcher [ESC[01;36mreq-8fa8da3a-2e61-47e9-a6c6-68f598e979ad ESC[00;36mTestServerAdvancedOps-228418898
TestServerAdvancedOps-892915532ESC[01;31m] ESC[01;35mESC[01;31mException during message handling: nova-computeESC[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 276, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 262, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 329, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 250, in decorated_function
    migration.instance_uuid, exc_info=True)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 237, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 305, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 292, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3472, in resize_instance
    migration_p)
  File ""/opt/stack/nova/nova/network/api.py"", line 45, in wrapped
    return func(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/network/api.py"", line 492, in migrate_instance_start
    self._get_floating_ip_addresses(context, instance)
  File ""/opt/stack/nova/nova/network/api.py"", line 474, in _get_floating_ip_addresses
    instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 700, in instance_floating_address_get_all
    return IMPL.instance_floating_address_get_all(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 52, in __call__
    raise exception.DBNotAllowed('nova-compute')
DBNotAllowed: nova-compute"
0,This change was wrong https://review.openstack.org/#/c/89628/5/neutron/tests/unit/services/loadbalancer/drivers/netscaler/test_netscaler_driver.py and needs to be taken back. Previous version imported module correctly and there was no need to change.
0,"Running stable/havana tempest against havana 2013.2 nova with this test:
tempest/tempest/api/compute/images/test_images_oneserver.py:test_create_second_image_when_first_image_is_being_saved
Using the libvirt driver on x86_64 RHEL 6.5 fails with this:
http://paste.openstack.org/show/50398/
Essentially: Stderr: ""convert: invalid option -- 's'\n""
This is the code:
https://github.com/openstack/nova/blob/2013.2/nova/virt/libvirt/utils.py#L549
Looks like it's not a new change, it's just incompatible with the latest qemu-img:
https://github.com/openstack/nova/commit/b216ed51914986087ea7dee57bc29904fda001a0
Looks like in the latest qemu-img the snapshot was moved into it's own sub-command:
[root@rhel62 ~]# qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
usage: qemu-img command [command options]
QEMU disk image utility
Command syntax:
  check [-f fmt] filename
  create [-f fmt] [-o options] filename [size]
  commit [-f fmt] [-t cache] filename
  convert [-c] [-p] [-f fmt] [-t cache] [-O output_fmt] [-o options] [-S sparse_size] filename [filename2 [...]] output_filename
  info [-f fmt] filename
  snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename
  rebase [-f fmt] [-t cache] [-p] [-u] -b backing_file [-F backing_fmt] filename
  resize filename [+ | -]size"
1,"(Marked this as a security issue for now, since cert revocation not working is pretty serious)

https://github.com/openstack/nova/blob/master/nova/crypto.py#L277-L278

os.chdir *always* returns None, which means that path is always taken and the cert is never revoked"
0,"the VMwareVcVmdkDriver currently does not support extending volumes. When executing:

     cinder extend 666a2dae-db61-43bd-9e37-de1c2695c998 5

The following error is seen in the screen-c-vol.log:

     Traceback (most recent call last):
       File ""/opt/stack/cinder/cinder/volume/manager.py"", line 845, in extend_volume
         self.driver.extend_volume(volume, new_size)
       File ""/opt/stack/cinder/cinder/volume/driver.py"", line 430, in extend_volume
         raise NotImplementedError(msg)
     NotImplementedError: Extend volume not implemented"
0,The http timeout parameter used in the Cisco n1kv client module is a constant defined in cisco_constants module. It should be a configurable parameter.
1,The default dhcp lease time is fairly short (120s). Is this simply a hold-over from the time before force_dhcp_release was the default and dhcp_release was expected throughout or is there another reason for the default to be so brief?
1,"the steps to reproduce:
1. boot an instance with user volume: nova boot --flavor 11 --image cirros --block-device-mapping /dev/vdb=a6118113-bce9-4e0f-89ce-d2aecb0148f8 test_vm1
2. shelve the instance: nova shelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
3. unshelve the instance: nova unshelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
the instance will be in task_state of ""unshelving"", and the error message in log file is:
[-] Exception during message handling: Invalid volume: status must be 'available'
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3593, in unshelve_instance
    do_unshelve_instance()
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3592, in do_unshelve_instance
    filter_properties, node)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3617, in _unshelve_instance
    block_device_info = self._prep_block_device(context, instance, bdms)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1463, in _prep_block_device
    instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1442, in _prep_block_device
    self.driver, self._await_block_device_map_created) +
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 364, in attach_block_devices
    map(_log_and_attach, block_device_mapping)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 362, in _log_and_attach
    bdm.attach(*attach_args, **attach_kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 44, in wrapped
    ret_val = method(obj, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 218, in attach
    volume_api.check_attach(context, volume, instance=instance)
  File ""/opt/stack/nova/nova/volume/cinder.py"", line 229, in check_attach
    raise exception.InvalidVolume(reason=msg)
InvalidVolume: Invalid volume: status must be 'available'"
0,"In the services.l3_router.L3RouterPlugin.create_floatingip(), we have some comments (docstring) to help us to know this function, but there are some nits inside them:
  :param floatingip: data fo the floating IP being created
  :returns: A floating IP object on success
  AS the l3 router plugin aysnchrounously creates floating IPs
  leveraging tehe l3 agent, the initial status fro the floating
  IP object will be DOWN.
data fo the floating IP --> data of the floating IP
aysnchrounously creates --> asynchronously creates
status fro the floating IP --> status for the floating IP"
0,It should be referenced by the last saved value. Remove the duplicated statements.
1,"After creating a volume transfer, I got a detail information of volume transfer like this:
{
    ""transfer"": {
        ""auth_key"": ""cfcb4552f7c1f2df"",
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://192.168.83.241:8776/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""bookmark""
            }
        ],
        ""created_at"": ""2013-10-22T01:18:35.627545"",
        ""volume_id"": ""f636037c-1aa0-4533-aef2-bcbd7ab3896c"",
        ""id"": ""dfedf224-1aba-420a-a387-3b971447a28f"",
        ""name"": null
    }
}
But I cannot describe its detail information with this href of this retrun.
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
The correct url is:
""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/os-volume-transfer/dfedf224-1aba-420a-a387-3b971447a28f""
Then I found that the define of collection_name are defferent between /cinder/api/views/transfers.py and /cinder/api/contrib/volume_transfer.py.
So I think this is a bug."
1,"The neutron full job is exhibiting a rather high number of cases where network-vif-plugged timeout are reported.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIlRpbWVvdXQgd2FpdGluZyBmb3IgdmlmIHBsdWdnaW5nIGNhbGxiYWNrIGZvciBpbnN0YW5jZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiIxNzI4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDAzNjA5MTk0NDg4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
95.78% of this kind of messages appear for the neutron full job. However, only a fraction of those cause build failures, but that's because the way the tests are executed.
This error is currently being masked by another bug as tempest tries to get the console log of a VM in error state: https://bugs.launchpad.net/tempest/+bug/1332414
This bug will target both neutron and nova pending a better triage.
Fixing this is of paramount importance to get the full job running.
Note: This is different from https://bugs.launchpad.net/nova/+bug/1321872 and https://bugs.launchpad.net/nova/+bug/1329546"
0,See http://logs.openstack.org/87/44287/9/check/gate-tempest-devstack-vm-full/c3a07eb/logs/screen-n-cond.txt.gz
0,"Now, start of VM fails because Ryu plugin does not support Quota extension.
This problem and other problems due to Quota request will be solved by adding Quota support."
1,"With VC VMDK cinder driver, the size of volumes created from image is currently based on the size of the image and the size input by the user is ignored. So if the image in the following command is 1GB,
    cinder create --image-id <image> 5
The 5GB is effectively ignored and the volume is created as 1GB.
Instead of this behavior, we should create a 5GB volume and then copy in the image."
0,"In NeutronDbPluginV2TestCase and its subclasses (most DB-based tests), there are two ways to specify a core plugin and an extension manager in the unit tests: test_config and ""plugin"" arguments of the constructor. Both are used and it sometimes makes it a bit difficult to debug.
It is better to unify the way to pass ""core plugin"" and ""extension manager"" into one.
I think it is better to remove test_config['plugin_name_v2'] and use ""plugin"" argument."
0,"PATCH indices on the Glance server currently use 1-based index for location entries. This goes against the JSON-pointer RFC (rfc6901) which requires array indices be 0-based.
The glance client should also be fixed to use 0-based indexing."
0,"I started a change in devstack here but we decided to move this into Cinder itself:

https://review.openstack.org/#/c/98492/

Nova made the same change for it's API and conductor workers in Icehouse:

https://review.openstack.org/#/c/69266/"
1,"Most of v3 servers core API missing decorator expected_errors.

All the v3 api should be under the protection of expected_errors"
0,"NSX: change api mapping for Service Cluster to Edge Cluster

NSX 4.2 GA has tweaked the naming for certain resources. Edge
Cluster vs Service CLuster is one of them."
1,"[Nova]2b17aa7f5af2adb418592b7169b50b231989bf37
Now, when I boot VM from volume with down of cinder-api, openstack returns 400.
It seems incompatibility in this case.
When HTTPClient exception ocuured, it should return 500Internal server error.
In fact, Nova turned into normal responses from HTTPClient exception.
$cinder list --all-tenants
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1 | available | None | 1 | None | false | |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
1. die cinder-api service
2. boot_from_volume
$curl -v -H ""X-Auth-Token: $TOKEN"" -H ""Content-type: application/json"" -X POST http://192.168.122.180:8774/v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot -d '{
    ""server"": {
        ""block_device_mapping"": [
            {
                ""delete_on_termination"": 0,
                ""device_name"": ""vda"",
                ""volume_id"": ""2facb85a-5ea1-4a9c-b615-e1dc75f30bb1"",
                ""volume_size"": 1
            }
        ],
        ""flavorRef"": ""1"",
        ""imageRef"": ""ce5505ca-4d22-4a57-bc63-261a1d6dd664"",
        ""name"": ""test-vm"",
        ""networks"": [
            {
                ""port"": ""cb10dd1d-87e5-421c-9382-32fe933317ea""
            }
        ]
    }
}'
* About to connect() to 192.168.122.180 port 8774 (#0)
* Trying 192.168.122.180... connected
> POST /v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot HTTP/1.1
> User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
> Host: 192.168.122.180:8774
> Accept: */*
> X-Auth-Token: --skip--
> Content-type: application/json
> Content-Length: 314
>
* upload completely sent off: 314out of 314 bytes
< HTTP/1.1 400 Bad Request
< Content-Length: 135
< Content-Type: application/json; charset=UTF-8
< X-Compute-Request-Id: req-0a8323af-bd7c-4701-ad5a-afa0774fb213
< Date: Tue, 15 Oct 2013 06:32:31 GMT
<
* Connection #0 to host 192.168.122.180 left intact
* Closing connection #0
{
    ""badRequest"": {
        ""code"": 400,
        ""message"": ""Block Device Mapping is Invalid: failed to get volume 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1.""
    }
}
(For reference, this report is same as openstack-dev mail post.
== [openstack-dev] behaviour about boot-from-volume (possible bug))"
1,"If there's a space in the server's name, the storwize driver's SSH command will get caught by the SSH injection check in cinder."
1,"I am seeing an error occur due to a race condition in cinder/brick/connector.py ISCSIConnector's disconnect_volume method. The scenario being that when we do the scsi delete it is expected that the symlink in /dev/disk/by-path is removed if it is no longer in use, and that when we call self.driver.get_all_block_devices() we can check to see if anyone is still using it by assuming if the symlink is there it is in use. Then only if it is no longer in the list we disconnect the iscsi portal.
The issue I am seeing is that there is some delay between calling the scsi delete and the symlink being removed which causes us to see the broken symlink when calling os.listdir(). My understanding is that the rules for udev to clean up the symlinks is done asynchronously, which means we are not guaranteed it will be cleaned up before we call self.driver.get_all_block_devices(). The end result of this causes the iscsi portal to be left open indefinitely when it should have been closed."
1,"If creating a flavor with is_public ""false"", the flavor should be accessible only by admin or user who is granted to access.
Now ""get flavor details"" API checks the is_public of flavor but ""create an instance"" API does not check.
In the following case, a user (not admin) cannot access non-public flavor through ""get flavor details"" API, this is right behavior.
However, he can access non-public flavor through ""create an instance"" API."
0,"We are occasionally seeing the test nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping fail in the gate due to
Traceback (most recent call last):
  File ""nova/tests/db/test_sqlite.py"", line 53, in test_big_int_mapping
    output, _ = utils.execute(get_schema_cmd, shell=True)
  File ""nova/utils.py"", line 166, in execute
    return processutils.execute(*cmd, **kwargs)
  File ""nova/openstack/common/processutils.py"", line 168, in execute
    result = obj.communicate()
  File ""/usr/lib/python2.7/subprocess.py"", line 754, in communicate
    return self._communicate(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1314, in _communicate
    stdout, stderr = self._communicate_with_select(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1438, in _communicate_with_select
    data = os.read(self.stdout.fileno(), 1024)
OSError: [Errno 11] Resource temporarily unavailable
logstash query: message:""FAIL: nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRkFJTDogbm92YS50ZXN0cy5kYi50ZXN0X3NxbGl0ZS5UZXN0U3FsaXRlLnRlc3RfYmlnX2ludF9tYXBwaW5nXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImFsbCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTAzMzk1MTU1NDcsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0="
0,Removed unused imports and assignments
1,"In this code review, https://review.openstack.org/#/c/104262/ brings an error in log because of the line at 137:
+ LOG.info(_LI(""Unable to force TCG mode, libguestfs too old?""),
+ ex)

Error is:

Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 685, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 649, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Logged from file guestfs.py, line 139

To fix this issue, we just need to add %s"
0,"Test tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent fails for job gate-tempest-dsvm-neutron-full with traceback:
ft335.1: tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent[gate,smoke]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-08-18 15:31:39,865 535 INFO [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 200 POST http://127.0.0.1:5000/v2.0/tokens
2014-08-18 15:31:40,148 535 INFO [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 201 POST http://127.0.0.1:9696/v2.0/routers 0.282s
2014-08-18 15:31:40,259 535 INFO [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 409 POST http://127.0.0.1:9696/v2.0/agents/47dd83c6-f92d-40d9-8601-5a38b6b9eda0/l3-routers 0.109s
2014-08-18 15:31:40,714 535 INFO [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:_run_cleanups): 204 DELETE http://127.0.0.1:9696/v2.0/routers/fd7d082c-71db-4fa0-bd0c-0b31acef9b1a 0.450s
}}}
Traceback (most recent call last):
  File ""tempest/api/network/admin/test_l3_agent_scheduler.py"", line 66, in test_add_list_remove_router_on_l3_agent
    self.agent['id'], router['router']['id'])
  File ""tempest/services/network/xml/network_client.py"", line 218, in add_router_to_l3_agent
    resp, body = self.post(uri, str(common.Document(router)))
  File ""tempest/services/network/network_client_base.py"", line 73, in post
    return self.rest_client.post(uri, body, headers)
  File ""tempest/common/rest_client.py"", line 219, in post
    return self.request('POST', url, extra_headers, headers, body)
  File ""tempest/common/rest_client.py"", line 431, in request
    resp, resp_body)
  File ""tempest/common/rest_client.py"", line 485, in _error_checker
    raise exceptions.Conflict(resp_body)
Conflict: An object with that identifier already exists
Details: {'message': 'The router fd7d082c-71db-4fa0-bd0c-0b31acef9b1a has been already hosted by the L3 Agent 47dd83c6-f92d-40d9-8601-5a38b6b9eda0.', 'type': 'RouterHostedByL3Agent', 'detail': {}}
There is log of test results: http://logs.openstack.org/43/97543/10/gate/gate-tempest-dsvm-neutron-full/8f09c74/logs/testr_results.html.gz"
1,"Hi , i was trying migrate a volume from one backend to another
while when operation is in copy_volume_data (cinder/volume/driver.py)
error occured while copy data from src_volume to dst_volume
root@DVT-Ubuntu-srv2:~# cinder --version
1.0.6
Below is trace back ==================================================
2014-03-06 17:38:36.768 12090 ERROR cinder.openstack.common.rpc.amqp [req-beec71a7-1eec-43af-bb83-a66c58376b42 bfe8d3624c044e5db34bc8abd12cd752 42d88e9851cc4af4afa6a0896834b915] Exception during message handling
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp **args)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/utils.py"", line 808, in wrapper
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 798, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp self.db.volume_update(ctxt, volume_ref['id'], updates)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 791, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp self._migrate_volume_generic(ctxt, volume_ref, host)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 729, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp new_volume['migration_status'] = None
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 709, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp remote='dest')
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/cinder/volume/driver.py"", line 326, in copy_volume_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp force=copy_error)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp UnboundLocalError: local variable 'copy_error' referenced before assignment
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp
2014-03-06 17:39:17.404 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.7/dist-packages/cinder/openstack/common/periodic_task.py:176
2014-03-06 17:39:17.404 12096 DEBUG cinder.manager [-] Notifying Schedulers of capabilities ... _publish_service_capabilities /usr/lib/python2.7/dist-packages/cinder/manager.py:135
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] Making asynchronous fanout cast... fanout_cast /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:640
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] UNIQUE_ID is b0e5e286d0094ff9a5e908fb49b5106d. _add_unique_id /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:345
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] Closed channel #1 _do_close /usr/lib/python2.7/dist-packages/amqp/channel.py:88
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] using channel_id: 1 __init__ /usr/lib/python2.7/dist-packages/amqp/channel.py:70
2014-03-06 17:39:17.408 12096 DEBUG amqp [-] Channel open _open_ok /usr/lib/python2.7/dist-packages/amqp/channel.py:420
2014-03-06 17:39:17.408 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._report_driver_status run_p
from the source code of Havana, following exception handling may somehow contain defect:
 312 try:
 313 size_in_mb = int(src_vol['size']) * 1024 # vol size is in GB
 314 volume_utils.copy_volume(src_attach_info['device']['path'],
 315 dest_attach_info['device']['path'],
 316 size_in_mb)
 317 copy_error = False
 318 except Exception:
 319 with excutils.save_and_reraise_exception():
 320 msg = _(""Failed to copy volume %(src)s to %(dest)d"")
 321 LOG.error(msg % {'src': src_vol['id'], 'dest': dest_vol['id']})
 322 copy_error = True
 323 finally:
 324 self._copy_volume_data_cleanup(context, dest_vol, properties,
 325 dest_attach_info, dest_remote,
 326 force=copy_error)
 327 self._copy_volume_data_cleanup(context, src_vol, properties,
 328 src_attach_info, src_remote,
 329 force=copy_error)"
1,"We can resize a shutoff instance , but resouce_tracker doesn't include this case when judge if an instance is in resize state.
Will show a warning like
""2013-10-31 16:35:18.969 18483 WARNING nova.compute.resource_tracker [-] [instance: 813e5a44-41ba-4ee7-8b7b-442d3fc017a7] Instance not resizing, skipping migration."
1,"nova-compute.log:
2013-09-30 15:52:18.897 12884 ERROR nova.compute.manager [req-d112a8fd-89c4-4b5b-b6c2-1896dcd0e4ab f70773b792354571a10d44260397fde1 b9e4ccd38a794fee82dfb06a52ec3cfd] [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Error: libvirt_info() takes exactly 6 arguments (7 given)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Traceback (most recent call last):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1037, in _build_instance
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] set_access_ip=set_access_ip)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1410, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] LOG.exception(_('Instance failed to spawn'), instance=instance)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1407, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2069, in spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] write_to_disk=True)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3042, in to_xml
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] disk_info, rescue, block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2922, in get_guest_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] inst_type):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2699, in get_guest_storage_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] inst_type)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2662, in get_guest_disk_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] self.get_hypervisor_version())
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] TypeError: libvirt_info() takes exactly 6 arguments (7 given)"
1,"In migration 1341ed32cc1e_nvp_netbinding_update Enum type had been changed incorrectly from ('flat', 'vlan', 'stt', 'gre') to ('flat', 'vlan', 'stt', 'gre', 'l3_ext') for PostgeSQL so in database this type was not changed.
This could be checked as it shown there http://paste.openstack.org/show/74835/.
The same problem is taken place for vlan_type in migrations 38fc1f6789f8_cisco_n1kv_overlay from ('vlan', 'vxlan', 'trunk',
'multi-segment') to ('vlan', 'overlay', 'trunk', 'multi-segment') and in 46a0efbd8f0_cisco_n1kv_multisegm from ('vlan', 'vxlan') to
('vlan', 'vxlan', 'trunk', 'multi-segment').
This could be checked as it shown there http://paste.openstack.org/show/75387/"
1,Just came across this. I noticed that the ip_lib.py code uses replace instead of add. Using the common code would be good anyway.
1,"We run a script to configure networks, VMs, Routers and assigin floatingIP to the VM.
After it is created, then we run a script to clean all ports, networks, routers and gateway and FIP.
The issue is seen when there is a back to back call to router-interface-delete and router-gateway-clear.
There are three calls to router-interface-delete and the fourth call to router-gateway-clear.
At this time there is a db lock obtained for port delete and when the other delete comes in, it timeout.
2014-10-03 09:28:39.587 DEBUG neutron.openstack.common.lockutils [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] Got semaphore ""db-access"" from (pid=25888) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-03 09:29:30.777 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54899)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54900)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54901)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54902)
2014-10-03 09:29:30.780 ERROR neutron.api.v2.resource [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] remove_router_interface failed
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 200, in _handle_action
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 247, in remove_router_interface
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource context.elevated(), router, subnet_id=subnet_id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 557, in delete_csnat_router_interface_ports
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource l3_port_check=False)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/ml2/plugin.py"", line 983, in delete_port
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource port_db, binding = db.get_locked_port_and_binding(session, id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/ml2/db.py"", line 135, in get_locked_port_and_binding
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource with_lockmode('update').
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2310, in one
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource ret = list(self)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2353, in __iter__
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource return self._execute_and_instances(context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2368, in _execute_and_instances
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource result = conn.execute(querycontext.statement, self._params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 662, in execute
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 761, in _execute_clauseelement
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource compiled_sql, distilled_params
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 874, in _execute_context
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 125, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource six.reraise(type(newraise), newraise, sys.exc_info()[2])
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 102, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource per_fn = fn(ctx)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 323, in handler
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource context.is_disconnect)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 254, in _raise_operational_errors_directly_filter
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource raise operational_error
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner \nFROM ports \nWHERE ports.id = %s FOR UPDATE' ('bec69266-227d-4482-a346-ef47dd3a7a78',)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource"
1,"[Impact]
 * Ensure attching already attached volume to second instance does not
   interfere with attached instance volume record.
[Test Case]
 * Create cinder volume vol1 and two instances vm1 and vm2
 * Attach vol1 to vm1 and check that attach was successful by doing:
   - cinder list
   - nova show <vm1>
   e.g. http://paste.ubuntu.com/12314443/
 * Attach vol1 to vm2 and check that attach fails and, crucially, that the
   first attach is unaffected (as above). You can also check the Nova db as
   follows:
   select * from block_device_mapping where source_type='volume' and \
       (instance_uuid='<vm1>' or instance_uuid='<vm2>');
   from which you would expect e.g. http://paste.ubuntu.com/12314416/ which
   shows that vol1 is attached to vm1 and vm2 attach failed.
 * finally detach vol1 from vm1 and ensure that it succeeds.
[Regression Potential]
 * none
---- ---- ---- ----
nova assumes there is only ever one bdm per volume. When an attach is initiated a new bdm is created, if the attach fails a bdm for the volume is deleted however it is not necessarily the one that was just created. The following steps show how a volume can get stuck detaching because of this.
$ nova list
c+--------------------------------------+--------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+--------+--------+------------+-------------+------------------+
| cb5188f8-3fe1-4461-8a9d-3902f7cc8296 | test13 | ACTIVE | - | Running | private=10.0.0.2 |
+--------------------------------------+--------+--------+------------+-------------+------------------+
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | available | test10 | 1 | lvm1 | false | |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
+----------+--------------------------------------+
| Property | Value |
+----------+--------------------------------------+
| device | /dev/vdb |
| id | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
| serverId | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
| volumeId | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
+----------+--------------------------------------+
$ cinder list
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | in-use | test10 | 1 | lvm1 | false | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
ERROR (BadRequest): Invalid volume: status must be 'available' (HTTP 400) (Request-ID: req-1fa34b54-25b5-4296-9134-b63321b0015d)
$ nova volume-detach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | detaching | test10 | 1 | lvm1 | false | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
2014-07-29 14:47:13.952 ERROR oslo.messaging.rpc.dispatcher [req-134dfd17-14da-4de0-93fc-5d8d7bbf65a5 admin admin] Exception during message handling: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 406, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher payload)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return f(self, context, *args, **kw)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher pass
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 277, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 319, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher kwargs['instance'], e, sys.exc_info())
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 307, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 4363, in detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher self._detach_volume(context, instance, bdm)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 4309, in _detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher connection_info = jsonutils.loads(bdm.connection_info)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/jsonutils.py"", line 176, in loads
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher return json.loads(strutils.safe_decode(s, encoding), **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/strutils.py"", line 134, in safe_decode
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher raise TypeError(""%s can't be decoded"" % type(text))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher"
0,"The Hyper-V driver does not support resize down and is currently rising an exception if the user attempts to do that, causing the instance to go in ERROR state.

The driver should use the recently introduced instance faults ""exception.InstanceFaultRollback"" instead, which will leave the instance in ACTIVE state as expected."
0,"2014-02-24 17:44:45.585 5282 TRACE neutron.openstack.common.db.sqlalchemy.session
2014-02-24 17:44:45.589 5282 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/amqp.py"", line 438, in _process_data
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp **args)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", lin
e 44, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/dispatcher.py"", line 172, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/services/loadbalanc
er/drivers/stingray/plugin_driver.py"", line 155, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self.plugin.update_pool_stats(context, pool_id, data=stats)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/db/loadbalancer/loa
dbalancer_db.py"", line 500, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self.update_status(context, Member, member, stats_status)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 447, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self.rollback()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/sqlalchemy/util/langhelpers
.py"", line 58, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp compat.reraise(exc_type, exc_value, exc_tb)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 444, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self.commit()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 354, in commit
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self._prepare_impl()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 334, in _prepare_impl
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp self.session.flush()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db
/sqlalchemy/session.py"", line 545, in _wrap
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp raise exception.DBError(e)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp DBError: (DataError) integer out of range
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp 'UPDATE poolstatisticss SET bytes_in=%(bytes_in)s, bytes_out=%(byte
s_out)s, active_connections=%(active_connections)s, total_connections=%(total_connections)s WHERE poolstatisticss.pool_id = %(poolstatist
icss_pool_id)s' {'bytes_in': 290286902, 'poolstatisticss_pool_id': u'0df91b58-e04b-4a36-bed8-8fcb3f73ed6e', 'total_connections': 2362054,
 'active_connections': 1, 'bytes_out': 3152985859}"
1,Need to fix typo where raise keyword is missing in get_profile_binding
1,"http://logs.openstack.org/98/81098/2/check/check-tempest-dsvm-neutron-pg/df24b97/console.html
2014-03-19 09:58:15.379 | Traceback (most recent call last):
2014-03-19 09:58:15.379 | File ""tempest/test.py"", line 121, in wrapper
2014-03-19 09:58:15.379 | return f(self, *func_args, **func_kwargs)
2014-03-19 09:58:15.379 | File ""tempest/scenario/test_load_balancer_basic.py"", line 225, in test_load_balancer_basic
2014-03-19 09:58:15.379 | self._check_load_balancing()
2014-03-19 09:58:15.379 | File ""tempest/scenario/test_load_balancer_basic.py"", line 213, in _check_load_balancing
2014-03-19 09:58:15.379 | ""http://{0}/"".format(self.vip_ip)).read())
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/urllib.py"", line 86, in urlopen
2014-03-19 09:58:15.380 | return opener.open(url)
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/urllib.py"", line 207, in open
2014-03-19 09:58:15.380 | return getattr(self, name)(url)
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/urllib.py"", line 345, in open_http
2014-03-19 09:58:15.380 | errcode, errmsg, headers = h.getreply()
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/httplib.py"", line 1102, in getreply
2014-03-19 09:58:15.380 | response = self._conn.getresponse()
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/httplib.py"", line 1030, in getresponse
2014-03-19 09:58:15.380 | response.begin()
2014-03-19 09:58:15.380 | File ""/usr/lib/python2.7/httplib.py"", line 407, in begin
2014-03-19 09:58:15.381 | version, status, reason = self._read_status()
2014-03-19 09:58:15.381 | File ""/usr/lib/python2.7/httplib.py"", line 365, in _read_status
2014-03-19 09:58:15.381 | line = self.fp.readline()
2014-03-19 09:58:15.381 | File ""/usr/lib/python2.7/socket.py"", line 430, in readline
2014-03-19 09:58:15.381 | data = recv(1)
2014-03-19 09:58:15.381 | IOError: [Errno socket error] [Errno 104] Connection reset by peer"
0,"import urlparse
should be changed to :
import six.moves.urllib.parse as urlparse
for python3 compatible."
1,"The vim.get_soap_url function incorrectly builds an IPv6 address using hostname/IP and port.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vim.py#L151
The result of this line would create an address as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348:443]/sdk
Ports should be outside the square brackets, not inside, as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:443/sdk
For reference see: http://en.wikipedia.org/wiki/IPv6_address section Literal IPv6 addresses in network resource identifiers"
0,"Hi team,
I encounter this problem in the below situation:
OS : Ubuntu
Version : Icehouse
1. Normally boot a vm
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE | None | Running | private=10.0.0.5 |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
xianghui@xianghui:/opt/stack/nova$ neutron net-list
+--------------------------------------+---------+----------------------------------------------------+
| id | name | subnets |
+--------------------------------------+---------+----------------------------------------------------+
| 2d6842e2-b82c-4d5c-8601-7928ab85a8fd | private | 44d8d50a-197d-4f52-90c4-487495fdb8b5 10.0.0.0/24 |
+--------------------------------------+---------+----------------------------------------------------+
2. Attach the instance with an interface
xianghui@xianghui:/opt/stack/nova$ nova interface-attach cirros_3 --net-id=2d6842e2-b82c-4d5c-8601-7928ab85a8fd
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE | None | Running | private=10.0.0.5, 10.0.0.5, 10.0.0.6 |
| fa79e7a9-a838-484b-b8a2-4447d4f5d6a0 | fedora-1 | SHUTOFF | None | Shutdown | private=10.0.0.3, 172.24.0.2 |
| 97a3758f-9777-44cc-9035-ac95e57f8304 | fedora-2 | SHUTOFF | None | Shutdown | private=10.0.0.4 |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
3. Above shows the previous ip twice until next update_info_cache() happens."
0,"Enums must contain name parameter for PostgreSQL
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/c88b6b5fea3_cisco_n1kv_tables.py"", line 90, in upgrade
    sa.PrimaryKeyConstraint('tenant_id', 'profile_id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 148, in create_table
    _ddl_runner=self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/event.py"", line 389, in __call__
    fn(*args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 293, in __call__
    return getattr(self.target, self.name)(*arg, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/types.py"", line 1835, in _on_table_create
    t._on_table_create(target, bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 576, in _on_table_create
    self.create(bind=bind, checkfirst=checkfirst)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 527, in create
    bind.execute(CreateEnumType(self))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1536, in _execute_ddl
    compiled = ddl.compile(dialect=dialect)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/expression.py"", line 1778, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2927, in _compiler
    return dialect.ddl_compiler(dialect, self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 705, in __init__
    self.string = self.process(self.statement)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 724, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py"", line 72, in _compiler_dispatch
    return getter(visitor)(self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 757, in visit_create_enum_type
    self.preparer.format_type(type_),
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 898, in format_type
    raise exc.CompileError(""Postgresql ENUM type requires a name."")
CompileError: Postgresql ENUM type requires a name."
1,"fix to 1293818 introduced a regression for the code path that is used in loadbalancer namespace driver to list namespaces.
The following tracebacks are seen during tempest runs:
http://logs.openstack.org/14/82514/2/check/check-tempest-dsvm-neutron-pg/73ff42d/logs/screen-q-lbaas.txt.gz?level=TRACE#_2014-03-25_09_49_57_261
As a consequence, this causes agent to resync balancer state and to respawn haproxy process sometimes leading to unavailable service. That in turn makes loadbalancer basic scenario test fail.
Related bugs: 1294603, 1295165"
0,"Radware plugin driver uses task queue to perform interaction with the backend device.
Several operations such as lbaas objects deletion are performed in async manner.
In the unit test code actual object deletion happens in separate thread; it leads to a need for tricks like putting test thread to sleep.
Such unit tests are not reliable and could lead to failures that are hard to catch or debug.

Unit test code should be refactored in such way that it uses single-threaded strategy to perform driver operations."
1,"On Havana, when an instance errors out and gets rescheduled, attached ports are currently not being cleaned up and is causing NVS/OVS to have multiple ports with the same UUID and MAC.

This is currently occurring with the VC Driver + Neutron and is blocking VMware Minesweeper CI."
1,"cinder list does not work with --metadata filtered
>cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| b67cffdd-e573-4cdc-8362-dea6b1fe49a9 | available | test2 | 1 | None | false | |
| cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 | available | test | 1 | None | false | |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
one of them has the readonly metadata attribute setted
 cinder show test
+--------------------------------+--------------------------------------+
| Property | Value |
+--------------------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| created_at | 2014-04-21T18:34:40.000000 |
| display_description | None |
| display_name | test |
| encrypted | False |
| id | cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 |
| metadata | {u'readonly': u'True'} |
| os-vol-host-attr:host | jmolle-Controller |
| os-vol-mig-status-attr:migstat | None |
| os-vol-mig-status-attr:name_id | None |
| os-vol-tenant-attr:tenant_id | 55088aa5b5054b878b11d765e960c459 |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | available |
| volume_type | None |
+--------------------------------+--------------------------------------+
cinder show test2
+--------------------------------+--------------------------------------+
| Property | Value |
+--------------------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| created_at | 2014-04-22T16:19:01.000000 |
| display_description | None |
| display_name | test2 |
| encrypted | False |
| id | b67cffdd-e573-4cdc-8362-dea6b1fe49a9 |
| metadata | {} |
| os-vol-host-attr:host | jmolle-Controller |
| os-vol-mig-status-attr:migstat | None |
| os-vol-mig-status-attr:name_id | None |
| os-vol-tenant-attr:tenant_id | 55088aa5b5054b878b11d765e960c459 |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | available |
| volume_type | None |
+--------------------------------+--------------------------------------+
But if I try to get the list with the metadata readonly as True it return that no volume has it setted
cinder list --metadata readonly=True
+----+--------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+----+--------+--------------+------+-------------+----------+-------------+
+----+--------+--------------+------+-------------+----------+-------------+
It is spected that volume test is listed (it has the readonly attribute setted)"
1,"On the environment where using Cinder LVM driver, extend_volume fails like the following.
# cinder extend 9dd8b44a-6835-40a6-ad9d-a103c25c532d 2
# cinder list
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending | test03 | 1 | None | false | |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
2013-10-16 09:00:05.573 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Error extending Volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Traceback (most recent call last):
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm File ""/opt/openstack/cinder/cinder/brick/local_dev/lvm.py"", line 469, in extend_volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm run_as_root=True)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm File ""/opt/openstack/cinder/cinder/utils.py"", line 142, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm return processutils.execute(*cmd, **kwargs)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm File ""/opt/openstack/cinder/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm cmd=' '.join(cmd))
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm ProcessExecutionError: Unexpected error while running command.
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Command: sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Exit code: 3
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stdout: ' Rounding up size to full physical extent 4.00 MiB\n'
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stderr: "" New size given (1 extents) not larger than existing size (256 extents)\n Run `lvextend --help' for more information.\n""
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm
2013-10-16 09:00:05.577 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Cmd :sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdOut : Rounding up size to full physical extent 4.00 MiB
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdErr : New size given (1 extents) not larger than existing size (256 extents)
  Run `lvextend --help' for more information.
# cinder help extend
usage: cinder extend <volume> <new-size>
Attempt to extend the size of an existing volume.
Positional arguments:
  <volume> Name or ID of the volume to extend.
  <new-size> New size of volume in GB
In cinder cli, the integer of 'new-size' is in GB .
But In LVM command, (lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d ) '-L 2' is in MB.
so, If I really want to extend size of volume, 1 GB to 2 GB, I have to try like the following.
# cinder extend 54a7125f-3a8f-48dc-a8b8-8f623c742ecb 2048
# cinder list
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
| 54a7125f-3a8f-48dc-a8b8-8f623c742ecb | available | haha-vol01 | 2048 | None | false | |
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending | test03 | 1 | None | false | |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
# lvs
  LV VG Attr LSize Origin Snap% Move Log Copy% Convert
  volume-54a7125f-3a8f-48dc-a8b8-8f623c742ecb cinder-volumes -wi-ao 2.00g
please check this bug. Thank you."
0,"Observed here: http://logs.openstack.org/92/71692/12/check/gate-neutron-python26/496b1b1/console.html
This behaviour might have been introduced by the patch for introducing a mapping between neutron and nsx uuids for networks."
0,"Problem description
===================
In the Open vSwitch agent, the Agent id is currently based off the mac address of the br-int.
Userspace Open vSwitch derivatives such as the Intel? DPDK Accelerated Open vSwitch do not currently create a tap device in the kernel to back the ovs bridges local port.
This limitation prevents reuse of the OpenVSwitch agent between both switches.
By allowing integration of high throughput vSwitch implementations via existing agents, NFV workloads can be enabled in OpenStack without significant extension of the current codebase.

Proposed change
===============
To enable reuse of the ovs agent with the Intel? DPDK Accelerated Open vSwitch the proposal is to change the generation of the agent_id to use the hostname instead of the mac address of the br-int.

- mac = self.int_br.get_local_port_mac()
- self.agent_id = '%s%s' % ('ovs', (mac.replace("":"", """")))
+ self.agent_id = 'ovs-agent-%s' % socket.gethostname()

For several plugins such as the nec,mlnx,hyperv and onconvergence agents the hostname is used to create the agent id.
Using the hostname will normalise the agent_id between these 5 neutron agents.

at present the Agent_id is only used in log messages. by using the hostname instead of mac of br-int log readability will also be improved as it will be easier to identify which node the log is from if log aggregation is preformed across a cluster.

Patch
===============
initial patch submitted for review
https://review.openstack.org/#/c/95138/1"
1,neutron_metadata_proxy_shared_secret should not be written to log file
1,"I don't believe this is actually a valid network configuration:

arosen@arosen-MacBookPro:~/devstack$ neutron subnet-show be0a602b-ea52-4b13-8003-207be20187da
+------------------+------------------------------------------------+
| Field | Value |
+------------------+------------------------------------------------+
| allocation_pools | {""start"": ""10.11.12.1"", ""end"": ""10.11.12.254""} |
| cidr | 10.11.12.0/24 |
| dns_nameservers | |
| enable_dhcp | True |
| gateway_ip | 10.0.0.1 |
| host_routes | |
| id | be0a602b-ea52-4b13-8003-207be20187da |
| ip_version | 4 |
| name | private-subnet |
| network_id | 53ec3eac-9404-41d4-a899-da4f32045abd |
| tenant_id | f2d9c1726aa940d3bd5a8ee529ea2480 |
+------------------+------------------------------------------------+"
1,"libvirt support swap volume. But if the new volume is larger than the old one, the block device isn't resized. The instance can't see the extra space.
$ nova show vm3
+--------------------------------------+----------------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------------+
| OS-DCF:diskConfig | AUTO |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | os3 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | os3 |
| OS-EXT-SRV-ATTR:instance_name | instance-00000039 |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2014-07-14T01:43:31.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | |
| created | 2014-07-14T01:43:23Z |
| flavor | m1.nano (42) |
| hostId | c8e8cab21e9e22dbc3779fd171e77f44940ba1c81161dc114ba4ad85 |
| id | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| image | cirros-0.3.2-x86_64-uec (da82a342-aeac-407a-bf9d-cf28bf68dc6b) |
| key_name | - |
| metadata | {} |
| name | vm3 |
| net1 network | 10.0.0.66 |
| os-extended-volumes:volumes_attached | [{""id"": ""756d0869-2ef2-4537-90c8-66df9657135f""}] |
| progress | 0 |
| security_groups | sg1, default |
| status | ACTIVE |
| tenant_id | fdbb1e8f23eb40c89f3a677e2621b95c |
| updated | 2014-07-14T06:34:57Z |
| user_id | 158d3c971e244f479593c86ff751bf8f |
+--------------------------------------+----------------------------------------------------------------+
$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d | available | vol3 | 2 | None | false | |
| 756d0869-2ef2-4537-90c8-66df9657135f | in-use | vol1 | 1 | None | false | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 | 1 | None | false | |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Then login guest OS:
$ sudo fdisk -l
.....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot Start End Blocks Id System
/dev/vdc1 2048 2097151 1047552 83 Linux
Swap the volume to larger one.
$ nova volume-update vm3 756d0869-2ef2-4537-90c8-66df9657135f 13097504-5b0c-4581-b1a5-9e05f616b89d
vm3 attached with the vol3
$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d | in-use | vol3 | 2 | None | false | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| 756d0869-2ef2-4537-90c8-66df9657135f | available | vol1 | 1 | None | false | |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 | 1 | None | false | |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Check the guest again:
$ sudo fdisk -l
....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot Start End Blocks Id System
/dev/vdc1 2048 2097151 1047552 83 Linux
The device size isn't changed."
1,"del_arp_table_entry crashes when it gets unknown ip.
l2pop sometimes send us fdb removal without the corresponding fdb add."
1,"In some cases, the session with the VC is terminated and restarted again. This can happen for example when the user does:
nova list (and there are no running VMs)
In addition to the restart of the session the operation also waits 2 seconds."
0,"If https://review.openstack.org/#/c/18042/ is merged, a new configuration option/feature will be added to nova. The manuals should be updated to reflect this - configuration references and sample files
In past versions of Nova it was possible to explicitly configure
the cache mode of disks via the libvirt XML template. The curent approach
makes this a derived setting of either 鈥渘one鈥?or 鈥渨ritethrough鈥?based
on the support of O_DIRECT. Whilst this provides a good set of default
settings it removes the ability of the cloud provider to use other
modes such as 鈥渨riteback鈥?and 鈥渦nsafe鈥?which are valuable in certain
configurations.
This change allows the cache mode to be specified on a per-disk type
basis. Leaving the specify_cachemode option set to False retains the
current behaviour.
 189 cfg.ListOpt('disk_cachemodes',
   190 default=[],
   191 help='Specific cachemodes to use for different disk types '
   192 'e.g: [""file=directsync"",""block=none""]'),
 189 ] 193 ]
   317 self.valid_cachemodes = [""default"",
   318 ""none"",
   319 ""writethrough"",
   320 ""writeback"",
   321 ""directsync"",
   322 ""writethrough"",
   323 ""unsafe"",
   324 ]"
1,"L2Pop mech driver does not apply L2Pop rules to DVR router interfaces correctly (this might have been caused by competing changes for blueprint ofagent-l2pop)
The patch for this bug will enable DVR Router interface ports to be handled correctly in order to address the network island problem.
Network island is a situation where VMs belonging to two different networks are hosted on two different compute nodes. The compute nodes themselves won't have VMs on the other network except for the one they are hosting.
This fix will enable DVR east-west traffic passthrough for cases where router and its interfaces are added after VMs have already been spawned/active on such routed networks."
1,"After unshelve an instance, found the attached port's binding info didn't get updated.
os@cloudcontroller:~$ nova show vm2
+--------------------------------------+----------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig | AUTO |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | xuhj-c1 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | xuhj-c1 |
| OS-EXT-SRV-ATTR:instance_name | instance-0000000d |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2014-04-11T07:15:36.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | |
| created | 2014-04-11T06:51:04Z |
| flavor | m1.small (2) |
| hostId | 493891ff9d0ff1ab3dddfa99ec9aba8fc30bfb2ccbc34e91f9f5c5b8 |
| id | 6c952df5-8893-47c8-b19d-c3f393c7f688 |
| image | ubuntu-core (0237b107-a492-44ea-817e-d94589943c9c) |
| key_name | - |
| metadata | {} |
| name | vm2 |
| net1 network | 12.0.0.7 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | fac60a38271e4b3bbc4a7dc97aee4e90 |
| updated | 2014-04-11T07:15:36Z |
| user_id | d7f959000a764cccbc0e9c634eb82514 |
+--------------------------------------+----------------------------------------------------------+
os@cloudcontroller:~$ neutron port-show c514acdd-4038-414f-be3a-d30f76b4cba6
+-----------------------+---------------------------------------------------------------------------------+
| Field | Value |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| binding:host_id | xuhj-c2 |
| binding:profile | {} |
| binding:vif_details | {""port_filter"": true} |
| binding:vif_type | bridge |
| binding:vnic_type | normal |
| device_id | 6c952df5-8893-47c8-b19d-c3f393c7f688 |
| device_owner | compute:nova |
| extra_dhcp_opts | |
| fixed_ips | {""subnet_id"": ""b4bf03ac-5243-404d-bd16-705b4b61643e"", ""ip_address"": ""12.0.0.7""} |
| id | c514acdd-4038-414f-be3a-d30f76b4cba6 |
| mac_address | fa:16:3e:49:9b:1f |
| name | |
| network_id | cea70fce-b954-40da-b325-b0bea5179ca0 |
| security_groups | 83dac46b-fa06-41e6-a028-35f9b7bf4046 |
| status | BUILD |
| tenant_id | fac60a38271e4b3bbc4a7dc97aee4e90 |
+-----------------------+---------------------------------------------------------------------------------+
| OS-EXT-SRV-ATTR:host | xuhj-c1 |
-------------
| binding:host_id | xuhj-c2 |"
1,"The neutron metadata service works in the following way:

Instance makes a GET request to http://169.254.169.254/

This is directed to the metadata-agent which knows which router(namespace) he is running on and determines the ip_address from the http request he receives.

Now, the neturon-metadata-agent queries neutron-server using the router_id and ip_address from the request to determine the port the request came from. Next, the agent takes the device_id (nova-instance-id) on the port and passes that to nova as X-Instance-ID.

The vulnerability is that if someone exposes their instance_id their metadata can be retrieved. In order to exploit this, one would need to update the device_id on a port to match the instance_id they want to hijack the data from.

To demonstrate:

arosen@arosen-desktop:~/devstack$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 1eb33bf1-6400-483a-9747-e19168b68933 | vm1 | ACTIVE | None | Running | private=10.0.0.4 |
| eed973e2-58ea-42c4-858d-582ff6ac3a51 | vm2 | ACTIVE | None | Running | private=10.0.0.3 |
+--------------------------------------+------+--------+------------+-------------+------------------+

arosen@arosen-desktop:~/devstack$ neutron port-list
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| 3128f195-c41b-4160-9a42-40e024771323 | | fa:16:3e:7d:a5:df | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.1""} |
| 62465157-8494-4fb7-bdce-2b8697f03c12 | | fa:16:3e:94:62:47 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| 8473fb8d-b649-4281-b03a-06febf61b400 | | fa:16:3e:4f:a3:b0 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.2""} |
| 92c42c1a-efb0-46a6-89eb-a38ae170d76d | | fa:16:3e:de:9a:39 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+

arosen@arosen-desktop:~/devstack$ neutron port-show 62465157-8494-4fb7-bdce-2b8697f03c12
+-----------------------+---------------------------------------------------------------------------------+
| Field | Value |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| device_id | 1eb33bf1-6400-483a-9747-e19168b68933 |
| device_owner | compute:None |
| extra_dhcp_opts | |
| fixed_ips | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| id | 62465157-8494-4fb7-bdce-2b8697f03c12 |
| mac_address | fa:16:3e:94:62:47 |
| name | |
| network_id | 5f68c45d-b729-4e21-9ded-089848eb4ef2 |
| security_groups | 3e29d8e7-0195-4438-a49a-9706736b888d |
| status | ACTIVE |
| tenant_id | 0f9d696fc73d4110ab492ca105881b9b |
+-----------------------+---------------------------------------------------------------------------------+

arosen@arosen-desktop:~/devstack$ neutron port-show 92c42c1a-efb0-46a6-89eb-a38ae170d76d
+-----------------------+---------------------------------------------------------------------------------+
| Field | Value |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| device_id | eed973e2-58ea-42c4-858d-582ff6ac3a51 |
| device_owner | compute:None |
| extra_dhcp_opts | |
| fixed_ips | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
| id | 92c42c1a-efb0-46a6-89eb-a38ae170d76d |
| mac_address | fa:16:3e:de:9a:39 |
| name | |
| network_id | 5f68c45d-b729-4e21-9ded-089848eb4ef2 |
| security_groups | 3e29d8e7-0195-4438-a49a-9706736b888d |
| status | ACTIVE |
| tenant_id | 0f9d696fc73d4110ab492ca105881b9b |
+-----------------------+---------------------------------------------------------------------------------+

From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm2.novalocal

arosen@arosen-desktop:~/devstack$ neutron port-update 92c42c1a-efb0-46a6-89eb-a38ae170d76d --device_id=1eb33bf1-6400-483a-9747-e19168b68933

 From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm1.novalocal

In order to fix this issue I believe we need to also pass the tenant-id in the metadata request to nova. When nova receives the request it will now have to query it's database using the instance_id and check that the tenant_id's match. Using the tenant_id solves this issue as the user is not allowed to specify or update this field."
0,"In each RESTful API, there are useful explanation messages for HTTPNotFound response in the implementation.
However, some messages are not output to the response.
For example, the implementation of the pause action API contains ""Server not found"" message for nonexistent server error, but now the message is not output into the response like the following:
$ curl -i 'http://localhost:8774/v2/<project-id>/servers/<nonexistent-server-id>/action' -X POST [..]"" -d '{""pause"": null}'
HTTP/1.1 404 Not Found
Content-Length: 78
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-a5282f5e-7e59-48cf-a86b-fe2e34347a2f
Date: Thu, 27 Feb 2014 10:21:52 GMT
{""itemNotFound"": {""message"": ""The resource could not be found."", ""code"": 404}}
$
When receiving the above message, client may consider ""what is not found? project, server, or action?""
so it is better to output right message into a response."
1,"    if dest_vg != self.vg.vg_name:
        vg_list = volutils.get_all_volume_groups()
            vg_dict = \
                (vg for vg in vg_list if vg['name'] == self.vg.vg_name).next()
`vg['name'] == self.vg.vg_name` should be `vg['name'] == dest_vg`
https://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py?h=stable/havana#n703"
1,"The openvswitch agent will try to construct veth pairs with names longer than the maximum allowed (15) and fail. VMs will then have no external connectivity.

This happens in cases where the bridge name is very long (e.g. int-br-bonded)."
1,"After parsing a lot of log files related to check failure, looks like the q-vpn at the time when I would expect to add
floating ip , it destroys the router's qg- and qr- interfaces.
However after the floating ip deletion request the q-vpn service restores the qg-, qr- interfaces.
tempest.scenario.test_minimum_basic.TestMinimumBasicScenario.test_minimum_basic_scenario[compute,image,network,volume]
failed in http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/console.html.
admin user: admin/9e02f14321454af6bb27587770f27d9b
admin tenant id: admin/413bb1232bca45069f3a3256839effa1
test user: TestMinimumBasicScenario-1306090821/c8dd95056c0b407e8dd168dbf410a66a
test Tenant: TestMinimumBasicScenario-993819377/2527b8222e3343bca9f70343e608880c
External Net : public/c29040d3-7e73-4a87-9f73-bb5cbe602afb
External subnet: public-subnet/ee754eb6-6194-4a25-a4cc-f9233d366c1e
Network: TestMinimumBasicScenario-1375749858-network/f029d7a8-54e0-484c-a215-cc34066ae830
Subnet: TestMinimumBasicScenario-1375749858-subnet/7edc72be-1207-4571-95d4-911223885ae7 10.100.0.0/28
Router id:TestMinimumBasicScenario-1375749858-router/08216822-5ee2-4313-be7e-dad2d84147db
Expected interfaces in the qrouter-08216822-5ee2-4313-be7e-dad2d84147db:
* lo 127.0.0.1
* qr-529eddd4-2c 10.100.0.1/28 iface_id: 529eddd4-2ca8-43ec-9cab-29c3a6632604, attached-mac: fa:16:3e:2a:f8:ba, ofport 166
* qg-9be8f502-93 172.24.4.85/24 iface-id: 9be8f502-9360-47dc-9eff-33c8743e7c2b attached-mac: fa:16:3e:be:a1:54, ofport 37
Floating IP: 172.24.4.87 (Never appears in the q-vpn log)
port: (net/subnet/port)(c29040d3-7e73-4a87-9f73-bb5cbe602afb/ee754eb6-6194-4a25-a4cc-f9233d366c1e/013b0b2d-80ed-403d-b380-6b6895ce34f5)
mac?: fa:16:3e:15:f2:57
floating ip uuid: cd84111e-af6a-4c26-af73-3167419c664a
Instance:
Ipv4: 10.100.0.2 mac: FA:16:3E:18:F1:69
Instance uuid: 8a552eda-2fbd-4972-bfcf-cee7e6472871
iface_id/port_id: 4188c532-3265-4294-8b4e-9bbfe5a482e8
ovsdb_interface_uuid: 9d7b858b-745e-482b-b91a-1e9ae34fc545
intbr tag: 49
dhcp server dev: tap4c6c6e06-e4
ns: qdhcp-f029d7a8-54e0-484c-a215-cc34066ae830
ip: 10.100.0.3 mac: fa:16:3e:a2:f1:ea
intbr tag: 49
Host:
eth0: 10.7.16.229/15 mac: 02:16:3e:52:5d:ff
Router + router interface creation in the logs:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_49_46_924
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-svc.txt.gz#_2014-05-01_19_49_46_724
Floating IP create:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_447
Floating IP associate:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_814
q-vpn starts destroying the router:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_50_20_277
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-c0cd93e7-5cb0-403b-a509-8e07b352b89d', 'ipsec', 'whack', '--ctlbase', '/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto', '--status']
Exit code: 1
Stdout: ''
Stderr: 'whack: Pluto is not running (no ""/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto.ctl"")\n' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.277 19279 DEBUG neutron.openstack.common.lockutils [-] Semaphore / lock released ""sync"" inner /opt/stack/new/neutron/neutron/openstack/common/lockutils.py:252
2014-05-01 19:50:20.277 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.516 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c']
Exit code: 0
Stdout: '605: qr-529eddd4-2c: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN \\ link/ether fa:16:3e:2a:f8:ba brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.517 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-int'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.533 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['ip', '-o', 'link', 'show', 'br-int']
Exit code: 0
Stdout: '6: br-int: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN \\ link/ether c6:49:9f:72:d2:4a brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.534 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:21.061 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c']
Exit code: 0
Tempest tries to connect to the VM:
2014-05-01 19:50:19,463 .. 2014-05-01 19:53:48,494 ~ 209 sec
Delete request for the floatingip:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_24_191
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_54_25_047
Neutron touches the qr-529eddd4-2c interface + port,
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_54_37_886
Delete request for router and router interface:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_57_423"
1,"Running tempest in a long distance link between the cinder controller and ZFS Storage Appliance, ZFSSA iSCSI Cinder driver timeouts, resulting in tests failures.
e.g
...
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTest.test_volume_create_get_update_delete_from_image [355.316953s] ... FAILED
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTestXML.test_volume_create_get_update_delete_as_clone [389.165116s] ... FAILED
...
Or
{1} tempest.api.volume.test_volumes_get.VolumesV1GetTest.test_volume_create_get_update_delete [507.216176s] ... FAILED
Some of these errors are the results of a volume not been reported as created, or an error uploading an image to a volume.
e.g (from cinder volume service)
2014-09-12 11:15:43.940 ERROR oslo.messaging.rpc.dispatcher [req-1879fa63-44c7-4f22-92fd-b139cce4bae7 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out
2014-09-12 11:15:44.643 ERROR cinder.volume.flows.manager.create_volume [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d26
15e2e47dc 43ab44ae2c7d4606ae30bc3b551cfe25] Volume c74b1a80-0528-41dd-b3d5-275410fa557a: create failed
2014-09-12 11:15:44.648 ERROR oslo.messaging.rpc.dispatcher [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out"
1,"This failure happens with the vmware backend when the size provided to the store is zero but the actual data size is larger than zero.
2014-03-14 17:22:53 | [10.36.11.112] out: Traceback (most recent call last):
2014-03-14 17:22:53 | [10.36.11.112] out: File ""tempest/api/image/v1/test_images.py"", line 50, in test_register_then_upload
2014-03-14 17:22:53 | [10.36.11.112] out: self.assertEqual(1024, body.get('size'))
2014-03-14 17:22:53 | [10.36.11.112] out: File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 321, in assertEqual
2014-03-14 17:22:53 | [10.36.11.112] out: self.assertThat(observed, matcher, message)
2014-03-14 17:22:53 | [10.36.11.112] out: File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-14 17:22:53 | [10.36.11.112] out: raise mismatch_error
2014-03-14 17:22:53 | [10.36.11.112] out: MismatchError: 1024 != 0"
1,"A Nova generated public SSH key contains the comment 'Generated by Nova'. Earlier versions of cloud-init (prior to 0.7.2) can't properly handle spaces in key comments and as a result, fail to disable the root account (if configured to do so).
Earlier Nova versions (Essex and older) didn't have spaces in the comment.
Problem introduced by commit: 114109dbf4094ae6b6333d41c84bebf6f85c4e48
cloud-init bug report: https://bugs.launchpad.net/ubuntu/+source/cloud-init/+bug/1220273"
1,"If neutron plugin doesn't support ext-gw-mode, L3 agent won't set-up SNAT rules on router (By default, it'll assume enable_snat==false). In order to behave like prior to ext-gw-mode extension introduction, it should enbale SNAT rules in that case."
0,"https://review.openstack.org/#/c/58089/ removes check CONF.dhcp_options_enabled from nova but it is still used in the help string in nova/virt/baremetal/pxe.py:
    cfg.StrOpt('pxe_bootfile_name',
               help='This gets passed to Neutron as the bootfile dhcp '
               'parameter when the dhcp_options_enabled is set.',
               default='pxelinux.0'),"
1,"The NetApp driver (in clustered mode) collects free space in an asynchronous background job, which results in the free space being reported as 0 until the job has run at least once (takes 60 seconds). The driver should be changed to report the correct free space immediately upon starting up."
1,"Happens a lot. For example: http://logs.openstack.org/50/51750/8/check/check-tempest-devstack-vm-neutron/7b73c73/logs/screen-n-cpu.txt.gz
2013-10-22 20:29:59.161 3294 ERROR nova.virt.libvirt.driver [-] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] During wait destroy, instance disappeared.
2013-10-22 20:29:59.163 DEBUG nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] vif_type=ovs instance={'vm_state': u'building', 'availability_zone': None, 'terminated_at': None, 'ephemeral_gb': 0, 'instance_type_id': 6, 'user_data': None, 'cleaned': False, 'vm_mode': None, 'deleted_at': None, 'reservation_id': u'r-5n6tip35', 'id': 18, 'disable_terminate': False, 'display_name': u'Server 88a265da-046a-4fc4-b812-59164e7e35c0', 'uuid': '88a265da-046a-4fc4-b812-59164e7e35c0', 'default_swap_device': None, 'hostname': u'server-88a265da-046a-4fc4-b812-59164e7e35c0', 'launched_on': u'devstack-precise-hpcloud-az3-598823', 'display_description': u'', 'key_data': None, 'kernel_id': u'1b2d8a22-2d48-4240-9987-e27de5999746', 'power_state': 0, 'default_ephemeral_device': None, 'progress': 0, 'project_id': u'd66f874bdc1e49a2adedb607234b2f99', 'launched_at': None, 'config_drive': u'', 'node': u'devstack-precise-hpcloud-az3-598823', 'ramdisk_id': u'8243eabc-098b-44cc-8629-73102741d145', 'access_ip_v6': None, 'access_ip_v4': None, 'deleted': False, 'key_name': None, 'updated_at': datetime.datetime(2013, 10, 22, 20, 29, 57, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'host': u'devstack-precise-hpcloud-az3-598823', 'architecture': u'x86_64', 'user_id': u'ebc6b389f1044bed9ebfdc9ef111d268', 'system_metadata': {u'image_architecture': u'x86_64', u'instance_type_memory_mb': u'64', u'instance_type_swap': u'0', u'instance_type_vcpu_weight': None, u'instance_type_root_gb': u'0', u'instance_type_id': u'6', u'image_image_state': u'available', u'instance_type_name': u'm1.nano', u'image_image_location': u's3bucket--tempest-1023546658/cirros-0.3.1-x86_64-blank.img.manifest.xml', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'image_disk_format': u'ami', u'instance_type_flavorid': u'42', u'image_container_format': u'ami', u'instance_type_vcpus': u'1', u'image_min_ram': u'0', u'image_min_disk': u'0', u'image_base_image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee'}, 'task_state': u'deleting', 'shutdown_terminate': False, 'cell_name': None, 'root_gb': 0, 'locked': False, 'name': 'instance-00000012', 'created_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'locked_by': None, 'launch_index': 0, 'metadata': {}, 'memory_mb': 64, 'vcpus': 1, 'image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee', 'root_device_name': u'/dev/vda', 'auto_disk_config': False, 'os_type': None, 'scheduled_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>)} vif=VIF({'ovs_interfaceid': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.1.0.5'})], 'version': 4, 'meta': {'dhcp_server': u'10.1.0.3'}, 'dns': [], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'10.1.0.1'})})], 'meta': {'injected': False, 'tenant_id': u'd66f874bdc1e49a2adedb607234b2f99'}, 'id': u'2b5cb11b-edf9-4e26-a068-bd820c95e106', 'label': u'private'}), 'devname': u'tap725ac9ef-5f', 'qbh_params': None, 'meta': {}, 'address': u'fa:16:3e:31:dd:da', 'type': u'ovs', 'id': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'qbg_params': None}) unplug /opt/stack/new/nova/nova/virt/libvirt/vif.py:755
2013-10-22 20:29:59.164 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.178 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.179 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link show dev qvod0a9091e-91 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.301 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.302 ERROR nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Failed while unplugging vif
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Traceback (most recent call last):
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] File ""/opt/stack/new/nova/nova/virt/libvirt/vif.py"", line 636, in unplug_ovs_hybrid
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] utils.execute('brctl', 'delif', br_name, v1_name, run_as_root=True)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] File ""/opt/stack/new/nova/nova/utils.py"", line 174, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] return processutils.execute(*cmd, **kwargs)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] cmd=' '.join(cmd))
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] ProcessExecutionError: Unexpected error while running command.
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Exit code: 1
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stdout: ''
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stderr: 'interface qvb725ac9ef-5f does not exist!\n'
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]
2"
0,"Most of the Cinder Public API Controllers inherit from wsgi.Controller, but still some Cinder Public APIs controllers directly inherit from object. So it's necessary to unify all the Cinder Public API Controllers inherit from wsgi.Controller."
0,"On neutron/tests/unit/test_api_v2.py:148, _get_collection_kwargs() uses collections.OrderedDict, which does not exist in Python 2.6. As the ordering here is clearly unimportant, this should be converted to be just a plain dict."
0,instance save is used by the driver when it does not need to be. each instance save will invoke a db access. after the spawn method is called the instance is updated so there is no need for the save
1,"if you upload a static large object:
https://gist.github.com/clayg/ae43968b46022ef79235
Then delete some of the .data files and run ssync (object-replicator)
Some of the newly replicated .data files don't have the X-Static-Large-Object metadata set (maybe some other metadata as well?)"
0,"ec2-api returns InvalidVolumeID.NotFound and InvalidSnapshotID.NotFound for absent volumes and snapshots.
But AWS returns InvalidVolume.NotFound and InvalidSnapshot.NotFound as it is documented in http://docs.aws.amazon.com/AWSEC2/latest/APIReference/api-error-codes.html

For example this affects Tempest. Tempest expects correct (AWS version) errors in waitXXXStatus functions and raises an error if other error cames for absent objects. So it make difficult writting tests."
1,"Fedora-x86_64-20-20140407-sda has 2 GiB virtual size.
$ nova boot fed_1G_2 --image Fedora-x86_64-20-20140407-sda --flavor 1 --key-name mykey
$ nova show fed_1G_2
+--------------------------------------+------------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-STS:power_state | 0 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | error |
| OS-SRV-USG:launched_at | - |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | |
| created | 2014-06-17T07:35:43Z |
| fault | {""message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""} |
| flavor | m1.tiny (1) |
| hostId | a904a292f4eb7f6735bef786c4a240a0b9240a6bc4f002519cb0e2b7 |
| id | 3c908a54-9682-40ad-8f12-a5bf64066660 |
| image | Fedora-x86_64-20-20140407-sda (085610a8-77ae-4bc8-9a28-3bcc1020e06e) |
| key_name | mykey |
| metadata | {} |
| name | fed_1G_2 |
| os-extended-volumes:volumes_attached | [] |
| private network | 10.1.0.5 |
| security_groups | default |
| status | ERROR |
| tenant_id | 1d26ad7003cf47e5b0107313be4832c3 |
| updated | 2014-06-17T07:35:44Z |
| user_id | bf52e56b9ca14648b391c5b6d490a0c1 |
+--------------------------------------+------------------------------------------------------------------------------------------+
$ # nova flavor-list
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| ID | Name | Memory_MB | Disk | Ephemeral | Swap_MB | VCPUs | RXTX_Factor | Is_Public |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| 1 | m1.tiny | 512 | 1 | 0 | | 1 | 1.0 | True |
| 2 | m1.small | 2048 | 20 | 0 | | 1 | 1.0 | True |
| 3 | m1.medium | 4096 | 40 | 0 | | 2 | 1.0 | True |
| 4 | m1.large | 8192 | 80 | 0 | | 4 | 1.0 | True |
| 42 | m1.nano | 64 | 0 | 0 | | 1 | 1.0 | True |
| 451 | m1.heat | 1024 | 0 | 0 | | 2 | 1.0 | True |
| 5 | m1.xlarge | 16384 | 160 | 0 | | 8 | 1.0 | True |
| 84 | m1.micro | 128 | 0 | 0 | | 1 | 1.0 | True |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
Many images requires minimum 2,5,10 Gib as minimum disk size, the 1 used by m1.tiny is frequently not enough.
It might be increased to 10 or the original 0 should be restored.
This bug is about why I see 'message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""'
when ""Flavor's disk is too small for requested image'"" was raised on the n-cpu side.
It is confusing, 'No valid host was found' type of messages sounds like there is no n-cpu running, or all of them full.
instance: f62b56da-d1fa-4dc2-ae37-42b8fde3d3a5] Instance failed to spawn
 Traceback (most recent call last):
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2064, in _build_resources
     yield resources
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 1966, in _build_and_run_instance
     block_device_info=block_device_info)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2233, in spawn
     admin_pass=admin_password)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2607, in _create_image
     project_id=instance['project_id'])
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 182, in cache
     *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 374, in create_image
     prepare_template(target=base, max_size=size, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
     return f(*args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 172, in fetch_func_sync
     fetch_func(target=target, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 658, in fetch_image
     max_size=max_size)
   File ""/opt/stack/new/nova/nova/virt/images.py"", line 110, in fetch_to_raw
     raise exception.FlavorDiskTooSmall()
 FlavorDiskTooSmall: Flavor's disk is too small for requested image."
1,"According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation."
1,"By default libguestfs will register an atexit() handler to cleanup any open libguestfs handles when the process exits. Since libguestfs does not provide any mutex locking in its APIs, the atexit handlers are not safe in multi-threaded processes. If they run they are liable to cause memory corruption as multiple threads access the same libguestfs handle. As such at atexit handlers should be disabled in any multi-threaded program using libguestfs. eg by using
  guestfs.GuestFS (close_on_exit = False)
instead of
  guestfs.GuestFS()"
0,"Just pick any recent docs build and you will see a ton of issues:
Example from:
http://logs.openstack.org/46/111146/1/check/gate-nova-docs/4f3e8c4/console.html
2014-08-01 03:40:18.805 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:5: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/plugins/v3/hosts.py:docstring of nova.api.openstack.compute.plugins.v3.hosts.HostController.index:6: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:7: ERROR: Unexpected indentation.
2014-08-01 03:40:18.807 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:8: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:23: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:24: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/utils.py:docstring of nova.db.sqlalchemy.utils.create_shadow_table:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | <autodoc>:0: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.851 | <autodoc>:0: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.851 | /home/jenkins/workspace/gate-nova-docs/nova/image/api.py:docstring of nova.image.api.API.get_all:8: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/keymgr/key_mgr.py:docstring of nova.keymgr.key_mgr.KeyManager.copy_key:9: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/notifications.py:docstring of nova.notifications.info_from_instance:6: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.InstanceList.get_active_by_window_joined:9: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:33: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/network_utils.py:docstring of nova.openstack.common.network_utils.set_tcp_keepalive:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/report/report.py:docstring of nova.openstack.common.report.report.ReportSection:16: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_manager.py:docstring of nova.pci.pci_manager.PciDevTracker.get_free_devices_for_requests:5: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:16: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.890 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:17: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:25: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:27: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.895 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.927 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.928 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.930 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:20: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:13: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.935 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:13: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.936 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.937 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.938 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:21: ERROR: Unexpected indentation.
2014-08-01 03:40:18.940 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:22: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.941 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:24: ERROR: Unexpected indentation.
2014-08-01 03:40:18.942 | /home/jenkins/workspace/gate-nova-docs/nova/tests/image_fixtures.py:docstring of nova.tests.image_fixtures.get_image_fixtures:10: SEVERE: Unexpected section title.
2014-08-01 03:40:18.942 |"
1," it will poll as fast as possible instead of disabling the watchdog as expected."""
1,"So I always hated the way that the status field worked on our db's - but when looking at it I thought I spotted a race, and now I'm pretty sure it's there. Can someone check out this test case (redbo, put on your concurrency hat).
Basically if you PUT on a container if the file on disk already exists it will update the put_timestamp, if not it goes into initialize which *may* raise AlreadyExists - but in the second case we don't update put_timestamp.
Replication was fixing it eventually..."
1,"When running tempest tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern fails with the server going into an ERROR state.
From the console log:
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 154, in test_volume_boot_pattern
keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 53, in _boot_instance_from_volume
create_kwargs=create_kwargs)
  File ""tempest/scenario/manager.py"", line 390, in create_server
self.status_timeout(client.servers, server.id, 'ACTIVE')
  File ""tempest/scenario/manager.py"", line 290, in status_timeout
self._status_timeout(things, thing_id, expected_status=expected_status)
  File ""tempest/scenario/manager.py"", line 338, in _status_timeout
self.config.compute.build_interval):
  File ""tempest/test.py"", line 237, in call_until_true
if func():
  File ""tempest/scenario/manager.py"", line 329, in check_status
raise exceptions.BuildErrorException(message)
  BuildErrorException: Server %(server_id)s failed to build and is in ERROR status
Details: <Server: scenario-server-89179012> failed to get to expected status. In ERROR state.
The exception:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339/logs/screen-n-cpu.txt.gz#_2013-09-24_04_44_31_806
Logs are located here:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339
-----------------
Originally the failure was (before some changes to timeouts in tempest):
t178.1: tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern[compute,image,volume]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2013-09-16 15:59:44,214 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:44,417 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,348 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,495 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:47,644 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:48,762 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:49,879 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:50,980 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:00:52,581 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:52,897 Authentication (publickey) successful!
2013-09-16 16:00:53,105 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,428 Authentication (publickey) successful!
2013-09-16 16:00:53,431 Secsh channel 1 opened.
2013-09-16 16:00:53,607 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,875 Authentication (publickey) successful!
2013-09-16 16:00:53,880 Secsh channel 1 opened.
2013-09-16 16:01:58,999 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,288 Authentication (publickey) successful!
2013-09-16 16:01:59,457 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,784 Authentication (publickey) successful!
2013-09-16 16:01:59,801 Secsh channel 1 opened.
2013-09-16 16:02:00,005 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:00,080 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,127 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,192 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,414 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:02,494 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:03,615 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:04,724 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:05,825 Starting new HTTP connection (1): 127.0.0.1
}}}
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 157, in test_volume_boot_pattern
    ssh_client = self._ssh_to_server(instance_from_snapshot, keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 101, in _ssh_to_server
    private_key=keypair.private_key)
  File ""tempest/scenario/manager.py"", line 453, in get_remote_client
    return RemoteClient(ip, username, pkey=private_key)
  File ""tempest/common/utils/linux/remote_client.py"", line 47, in __init__
    if not self.ssh_client.test_connection_auth():
  File ""tempest/common/ssh.py"", line 148, in test_connection_auth
    connection = self._get_ssh_connection()
  File ""tempest/common/ssh.py"", line 70, in _get_ssh_connection
    time.sleep(bsleep)
  File ""/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py"", line 52, in signal_handler
    raise TimeoutException()
TimeoutException
Full logs here:
http://logs.openstack.org/80/43280/9/gate/gate-tempest-devstack-vm-full/cba22ae/testr_results.html.gz
Looks a bit like the instance failed to boot."
1,"In the libvirt.driver, we now use the code like this:
(state, _max_mem, _mem, _cpus, _t) = virt_dom.info()
if the libvirt add new variables in the domain info, the code will be failed.
the error will like this :
 File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 974, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 882, in _init_instance
    drv_state = self._get_power_state(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 990, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3462, in get_info
    (state, max_mem, mem, num_cpu, cpu_time) = virt_dom.info()
ValueError: too many values to unpack"
0,"The sample configuration file of cinder shipped with Havana and master lacks config groups other than [DEFAULT], which makes the configuration not actually work.
One example is the key ""connection="", which is in the sample configuration in the group [DEFAULT], but actually needs to be in the group [database] in order to be found by the code. But there are dozens of examples like that."
0,"In disk/api.py the method resize2fs does not have a test.
Also, the method firstly use e2fsck to check if the filesystem is correct. if the program failed no information was logged and the actual algorithm try to do the resize anyway. Same with e2fsck, if resize2fs failed not information are logged.
We need to add tests for this function and log every error returned."
1,"operation:
$ mysql
> create database neutron_ml2 character set utf8;
> exit
$ neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini
error:
2014-02-28 15:02:46.550 TRACE neutron ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ml2_vlan_allocations' doesn't exist"") 'SELECT ml2_vlan_allocations.physical_network AS ml2_vlan_allocations_physical_network, ml2_vlan_allocations.vlan_id AS ml2_vlan_allocations_vlan_id, ml2_vlan_allocations.allocated AS ml2_vlan_allocations_allocated \nFROM ml2_vlan_allocations FOR UPDATE' ()
investigation:
This problem introduced by https://review.openstack.org/#/c/74896/ .
Not that this problem does not occur if nuetron-db-manage is run before running neutron-server since ml2_vlan_allocations table is created by neutron-db-manage.
I did skip running neutron-db-manage usually and it was no problem. Is it prohibited now ?"
0,"When I ran
    OS_DEBUG=1 python setup.py testr --testr-args='neutron.tests.unit.openvswitch.test_ovs_security_group'
I got the following result in testrepository log: http://paste.openstack.org/show/47468/
According to the log, neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin is loaded instead of OVS plugin. This means OVS plugin is not well tested and DB plugin with security group is tested again.
This comes from the behavior of Python multiple inheritance.
If a class (C1) inherits multiple classes (P1, P2) which have a common parent class (GP),
for example, C1.setUp() calls super(), P1.setUp is called first, then P2.setUp is called and finally GP.setUp is called.
The behavior is tricky and such usage should be avoided.
In Neutron tests multiple inheritance is used in many places.
I am afraid OVS plugin security group rcp test case is just one of them."
1,"cinder.volume.manager.init_host attempts to delete volumes that have status of ""deleting"". But set_initialized is not called until after this step.
And manager.delete_volume has new decorator @utils.require_driver_initialized.
So the call to delete_volume from init_host will always fail because the driver is not yet marked as initialized.
Proposed solution: move set_initailized up after the call to check_for_setup_error."
1,There are several places in the bigswitch plugin where the db context is not correctly passed down to update network calls so the required data isn't available in the database during certain operations (e.g. floating IP update).
0,"The GlusterFS snapshot code introduced in Havana uses arguments for qemu-img (--backing-chain and --output=json) that are newer than the versions of qemu-img that Cinder is generally expected to support.

Additionally, some code was added to parse qemu-img output rather than using the well-tested image_utils methods which do this today.

This code should not call qemu-img --backing-chain or --output=json, and should leverage image_utils as much as possible for the work being done with qemu_img."
0,"Need to add a check in parse_uri(...) [1] to make sure that a URI with the vsphere scheme is provided. Throw a BadStoreUri otherwise.
[1] https://github.com/openstack/glance/blob/master/glance/store/vmware_datastore.py#L151"
0,", Snapshot's status have update to 'available' in database when snapshot created successfully, but in 'notification-info' it still being 'creating'.
2014-07-28 10:42:25.711 3580 INFO cinder.volume.manager [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] snapshot f2264c3b-bb46-41fc-b59f-978a222d3ded: created successfully
2014-07-28 10:42:25.712 3580 INFO oslo.messaging._drivers.impl_zmq [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] 'notifications-info' {'event_type': 'snapshot.create.end',
 'message_id': '69eece40-3d3f-4d65-a239-4ba88d0a8ff5',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-28 02:42:25',
             'deleted': '',
             'display_name': None,
             'snapshot_id': u'f2264c3b-bb46-41fc-b59f-978a222d3ded',
             'status': u'creating',
             'tenant_id': u'7dfd3b6a98664f7cb78808f57b7984da',
             'user_id': u'ce5e930bf5a44167b149a2d5fd2302e6',
             'volume_id': u'd555f2ec-bc78-4f9b-a732-77c67b1fda3a',
             'volume_size': 1},
 'priority': 'INFO',
 'publisher_id': 'snapshot.controller1',
 'request_id': u'req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194',
 'timestamp': '2014-07-28 02:42:25.712147'}"
1,"In DB migrations the following is detected:
INFO [alembic.autogenerate.compare] Detected NULL on column 'cisco_ml2_apic_contracts.tenant_id'"
1,"The command:
   nova boot --flavor $FLAV --key_name $KEY --image $IMG --meta foo=bar meta1
should inject a file into `/meta.js` with content `{""foo"":""bar""}`. Currently in devstack this doesn't work.
It looks as if the data is arriving to n-cpu as:
    metadata={u'foo': u'bar'}
But n-cpu is expecting:
    metadata = [{""key"": ""foo"", ""value"": ""bar""}]"
0,Remove unneeded call to conductor in network interface
1,"This change broke the ability to force quotas below the current in-use value by adding new validation checks:

https://review.openstack.org/#/c/28232/

$ nova quota-update --force --cores 0 132
ERROR (BadRequest): Quota limit must be greater than 1. (HTTP 400) (Request-ID: req-ff0751a9-9e87-443e-9965-a30768f91d9f)"
0,"The NSX backend from version 4.1 does not have anymore any restriction on transformation of centralized routers in distributed.
Version 3.x instead could not transform distributed routers into centralized, which is anyway consistent with the current DVR extension.

The current restriction specific for the NSX plugin must therefore be lifted."
1,"Happens a log, for example in http://logs.openstack.org/81/52181/8/check/check-tempest-devstack-vm-postgres-full/aa99a8c/logs/screen-c-api.txt.gz
2013-10-23 01:20:17.784 22210 ERROR cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return req.get_response(self.application)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault application, catch_exc_info=False)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return self.app(env, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault response = self.app(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault content_type, body, accept)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return method(req=request, **action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return _snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:20:17.786 22210 INFO cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes returned with HTTP 404
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:558
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:617
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token ba5f0f940d4cc2f687d5825436378d55 _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1016
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: bcf3db5a242f42d9b2e05d2adf9e4b13 with project_id : 6240d8b99d584ebd87a89b63e39f3a0b and roles: _member_ _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Matched POST /6240d8b99d584ebd87a89b63e39f3a0b/volumes __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Route path: '/{project_id}/volumes', defaults: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Match dict: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>, 'project_id': u'6240d8b99d584ebd87a89b63e39f3a0b'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-10-23 01:20:17.799 22210 INFO cinder.api.openstack.wsgi [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] POST http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes
2013-10-23 01:20:17.800 22210 DEBUG cinder.api.v1.volumes [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Create volume request body: {'volume': {'scheduler_hints': {}, 'metadata': {u'Type': u'work'}, 'display_name': u'Volume--tempest-572743380', 'source_volid': u'e4974f44-969d-4cef-944d-2fd9b2b78458', 'size': u'1'}} create /opt/stack/new/cinder/cinder/api/v1/volumes.py:358
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: gigabytes is set by the default quota flag: quota_gigabytes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: volumes is set by the default quota flag: quota_volumes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.817 22210 ERROR cinder.api.middleware.fault [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return req.get_response(self.application)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault application, catch_exc_info=False)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return self.app(env, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault response = self.app(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault content_type, body, accept)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return method(req=request, **action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 390, in create
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault source_volid)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 335, in get_volume
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault rv = self.db.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/api.py"", line 213, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return IMPL.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1149, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return _volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1142, in _volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault raise exception.VolumeNotFound(volume_id=volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault VolumeNotFound: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.139 22210 ERROR cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] Caught error: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return req.get_response(self.application)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault application, catch_exc_info=False)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault app_iter = application(self.environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return self.app(env, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault response = self.app(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return self.func(req, *args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault content_type, body, accept)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return method(req=request, **action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return _snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.140 22210 INFO cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] http://127.0.0.1:8776/v1/5dada9a2750641eab97cc64931d55961/volumes returned with HTTP 404"
0,"The OVS core plugin will be removed in Juno 2 but the embrane plugin code is still using it and will stop working.
This bug tracks changing the dependency to ML2"
0,"There are a number of problems where the virt driver impls do not match the API defined by the base ComputeDriver class.
For example
 - Libvirt: Adds 'SOFT' as default value for 'reboot' method but no other class does
 - XenAPI: set_admin_passwd takes 2 parameters but base class defines it with 3 parameters in a different order
 - VMWare: update_host_status method which doesn't exist in base class & is never called in entire codebase
 - All: names of parameters are not the same as names of parameters in the base class
 - ...more...
These inconsistencies are functional bugs in the worst, or misleading to maintainers in the best case. It should be possible to write a test using the python 'inspect' module which guarantees that the sub-class APis actually match what they claim to implement from the base class."
1,"Failing over a network from one dhcp agent to another results in a new IP address for the dhcp port. This breaks dns for all vms on that network. This can be reproduced by simply doing a ""neutron dhcp-agent-network-remove"" and then a ""neutron dhcp-agent-network-add"" and observing that the dhcp port ip address will change."
0,"Right now when there is no active lbaas agent for HAProxy driver (or other agent-based driver), the following error is returned to the client:
""No eligible loadbalancer agent found for pool <pool_id>""
We need to return some more generic error to the user, skipping the notion of agent.
Also, pool will remain in PENDING_CREATE state, while it probably should move to an ERROR state."
1,"Version
=======
Havana, RHEL, neutron+ovs, python-neutron-2013.2-0.3.3.b3.el6ost
Description
===========
It's impossible to delete a router while it still has inactive ports.
The error message states that the router still has active ports.
# neutron router-port-list router1
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| 052a48e4-0868-4675-bef7-8f763dd697b4 | | fa:16:3e:60:73:8d | {""subnet_id"": ""c5d63940-71e8-4338-865e-f5364fbe4e78"", ""ip_address"": ""10.35.214.1""} |
| 065cef02-a949-45c9-b3df-e005dbf96c9a | | fa:16:3e:a6:6d:d8 | {""subnet_id"": ""044bcc05-f37b-4d1d-a700-c91c4381fbc8"", ""ip_address"": ""10.35.211.1""} |
| 7a020243-90e5-439d-90fb-ec96b07843e7 | | fa:16:3e:04:0c:1f | {""subnet_id"": ""4081fbca-3e59-4be5-a98e-3c9e0d13d3a6"", ""ip_address"": ""10.35.212.1""} |
| 7af56958-674e-472b-8dbe-09b60501a6e6 | | fa:16:3e:1a:07:a4 | {""subnet_id"": ""ef8e7c03-f17f-4c3c-9afe-252aca1283fd"", ""ip_address"": ""10.35.170.102""} |
| f034cb8a-2a09-4d41-b46c-a08fe208461e | | fa:16:3e:de:9d:32 | {""subnet_id"": ""cca4edc7-2872-4c1e-a270-3b0beb60f421"", ""ip_address"": ""10.35.213.1""} |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status | ACTIVE |
| status | ACTIVE |
| status | ACTIVE |
| status | ACTIVE |
| status | ACTIVE |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-update $i --admin_state_up False ; done | grep status
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status | DOWN |
| status | DOWN |
| status | DOWN |
| status | DOWN |
| status | DOWN |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
From /var/log/neutron/server.log
================================
2013-10-07 16:39:40.429 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource raise l3.RouterInUse(router_id=id)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource
2013-10-07 16:40:34.870 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource raise l3.RouterInUse(router_id=id)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource"
0,"I tested a devstack today with libvirt-lxc, and was unable to get a dhcp address in cirros 0.3.2.

The reason is that cirros's udhcpc seems to ignore the response if it doesn't have checksums.

the appropriate mangle rule would be written if /dev/vhost-net , but with newer kernels this is also happening on the lxc network devices.

It seems the sane thing to do at this point is just to drop the protection based on '/dev/vhost-net' presence."
0,"In this BigSwitch plugin router rules support, the plugin only accepts the 'any' keyword. However, it should accept both the 'external' keyword as well, which looks the same to neutron (0.0.0.0/0) but has a different meaning on the backend controller."
1,"The create_thin_pool method is called when thin provisioning is enabled and the pool doesn't exist. At the end of this method, self.vg_thin_pool is set[0] with the full pool_path which makes volume's creation fail since it tries to build the pool_path again.[1]
This is the command it generates:
sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvcreate -T -V 1g -n volume-b0c7232b-4214-4d13-ac20-4894333b627d stack-volumes/stack-volumes/stack-volumes-pool
This stack-volumes/stack-volumes/stack-volumes-pool should be stack-volumes/stack-volumes-pool
[0] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L340
[1] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L353"
1,"Similar to bug #1314313 but this is another failure.
In some tempest runs a test fails to delete firewall within 300 seconds.
That happens because at the point firewall agent sends deleting confirmation to neutron server, firewall object is already updated to a state unexpected by deleting method.
Example of the issue:
http://logs.openstack.org/18/97218/2/gate/gate-tempest-dsvm-neutron/e03d166/console.html#_2014-06-07_10_33_34_506"
1,"liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
liugya@liugya-ubuntu:~$ nova service-list
/usr/lib/python2.7/dist-packages/gobject/constants.py:24: Warning: g_boxed_type_register_static: assertion `g_type_from_name (name) == 0' failed
  import gobject._gobject
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary | Host | Zone | Status | State | Updated_at | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor | liugya-ubuntu | internal | enabled | up | 2013-11-16T03:06:33.000000 | None |
| nova-compute | liugya-ubuntu | nova | disabled | up | 2013-11-16T03:06:29.000000 | | <<<<<<< Reason is empty now
| nova-cert | liugya-ubuntu | internal | enabled | up | 2013-11-16T03:06:34.000000 | None |
| nova-network | liugya-ubuntu | internal | enabled | up | 2013-11-16T03:06:26.000000 | None |
| nova-scheduler | liugya-ubuntu | internal | enabled | up | 2013-11-16T03:06:31.000000 | None |
| nova-consoleauth | liugya-ubuntu | internal | enabled | up | 2013-11-16T03:06:32.000000 | None |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+"
0,"If a driver raises an exception during retype, only a generic message is printed by the manager. It would help debugging if the original exception was printed as well."
1,"In the case of a failed migration the volume's status and the migration status are both set to error state. We have the ability to use reset-state to get the volume back to a usable state, however the migration-status isn't updated which makes it impossible to do things like delete the volume or perhaps retry the migration.
We should have the reset-state command clean up outliers like this as well."
1,"I don't know why the gate isn't catching these, but I got the latest code this morning and ran a clean tox pep8 which resulted in these failures:

http://paste.openstack.org/show/47427/

Using blame, here is at least one patch that added code which is causing one of the failures but it wasn't caught in the gate:

https://review.openstack.org/#/c/47901/"
0,"Passing a fake url to `get_transport` makes it ignore the configured `transport_url`. This blocks any chance of using the new transport_url configuration.
https://github.com/openstack/glance/blob/master/glance/notifier.py#L81"
0,"Currently the Evacuate operation (enabled by the os-evacuate) extension always returns a generated (or user supplied) password, although not all hypervisors support password injection.
For server create and rebuild and rescue operations the configuration option ""enable_instance_password=False"" can be used to suppress returning a meaningless and confusing password, and the evacuate operation should also honor this setting"
1,"After unshelved a shelved server(status: ACTIVE), it fails to suspend the server like the following:

$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | SHELVED_OFFLOADED | None | Shutdown | private=10.0.0.2 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
$ nova unshelve vm01
$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | ACTIVE | None | Running | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+
$ nova suspend vm01
ERROR: Unable to process the contained instructions (HTTP 422) (Request-ID: req-5c8edaf3-abb7-4d00-af96-e8a6d9777910)
$"
0,NSX: nicira_models should import model_base directly
1,"cinder/db/sqlalchemy/api.py

service_get_all_by_topic includes the filter disabled=False. This means it doesn't get all and in fact gets only services that are not disabled.

This method is used four times in the codebase

cinder/backup/api.py: API._is_backup_service_enabled
this method filters out disabled services already so is not affected.

cinder/scheduler/host_manager.py: HostManager.get_all_host_states
this method tries to log messages for disabled services, but can't because they've already been filtered out

cinder/volume/api.py: API.list_availability_zones
list_availability_zones is not able to show disabled services

cinder/volume/api.py: API.migrate_volume
not currently filtering out disabled hosts - would require patching"
0,"Update all of the rpc client API classes to include a version alias
for the latest version implemented in Juno. This alias is needed when
doing rolling upgrades from Juno to Kilo. With this in place, you can
ensure all services only send messages that both Juno and Kilo will
understand."
1,"If passing invalid parameter to ""create flavor_extraspecs"" API, an internal error happens and Traceback is written in log file.
Nova should return BadRequest response instead of internal error.
$ curl -i 'http://10.21.42.109:8774/v2/fd283c7ef47b4f46899403e9ebb1e2ed/flavors/6e05eb08-ef1a-4183-9eb9-5c175060247a/os-extra_specs' [..] -d '{""foo"": {""key01"": ""value01""}}'
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-e8b5120c-e585-4095-a387-d3eed52b7415
Date: Thu, 26 Dec 2013 11:06:46 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}}
$
** The log of nova-api **
2013-12-26 20:06:46.026 ERROR nova.api.openstack [req-e8b5120c-e585-4095-a387-d3eed52b7415 admin demo] Caught error: 'NoneType' object has no attribute 'keys'
2013-12-26 20:06:46.026 TRACE nova.api.openstack Traceback (most recent call last):
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 121, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return req.get_response(self.application)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-12-26 20:06:46.026 TRACE nova.api.openstack application, catch_exc_info=False)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-12-26 20:06:46.026 TRACE nova.api.openstack app_iter = application(self.environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return self.app(env, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack response = self.app(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack resp = self.call_func(req, *args, **self.kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-12-26 20:06:46.026 TRACE nova.api.openstack return self.func(req, *args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 930, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack content_type, body, accept)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 992, in _process_stack
2013-12-26 20:06:46.026 TRACE nova.api.openstack action_result = self.dispatch(meth, request, action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1073, in dispatch
2013-12-26 20:06:46.026 TRACE nova.api.openstack return method(req=request, **action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/compute/contrib/flavorextraspecs.py"", line 76, in create
2013-12-26 20:06:46.026 TRACE nova.api.openstack specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/api.py"", line 1487, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack extra_specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 130, in wrapper
2013-12-26 20:06:46.026 TRACE nova.api.openstack return f(*args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 4483, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack filter(models.InstanceTypeExtraSpecs.key.in_(specs.keys())).\
2013-12-26 20:06:46.026 TRACE nova.api.openstack AttributeError: 'NoneType' object has no attribute 'keys'"
1,"The exception occurs when trying to create/delete an instance that is using a network that is not owned by the admin tenant. This prevents the deletion of the instance.
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp pass
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1616, in run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp do_run_instance()
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1615, in do_run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 965, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp notify(""error"", msg=unicode(e)) # notify that build failed
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 949, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp instance, image_meta, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1078, in _build_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp filter_properties, bdms, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1122, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp self._log_original_error(exc_info, instance_uuid)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1117, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp bdms, requested_networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1642, in _shutdown_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp network_info = self._get_instance_nw_info(context, instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 879, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 455, in get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp result = self._get_instance_nw_info(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 463, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp nw_info = self._build_network_info_model(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 1009, in _build_network_info_model
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp subnets)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 962, in _nw_info_build_network
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp label=network_name,
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp UnboundLocalError: local variable 'network_name' referenced before assignment"
0,"When Neutron WSGI is running in SSL mode, it requires a separate cert file and key file. However, there are cases where these may be combined into one file and neutron currently does not support this mode of operation even though the underlying SSL library does[1].

1. https://docs.python.org/2/library/ssl.html#ssl.wrap_socket"
1,"When doing backup-restore operation, it needs volume's size is not less than backup's size.
But in backup-restore routine, when volume's size is bigger than backup's size, it logs the
the info of volume's size and backup's size to warn level, which I think it should log the info
to info level.
This bug fix it."
0,"The store module is removed from glance project and new glance_store module is created, but the glance project code was not updated properly for the required changes.

There are few cases in v1 api which needs to be addressed:

1. _get_from_store method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError.

2. _get_size method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError.."
1,"Change Ia0ebd674345734e7cfa31ccd400fdba93646c554 traded one race condition for another. By ignoring all mkdir() calls that would otherwise fail because an instance directory already exists, two nodes racing to create a single image will corrupt or lose data, or fail in a strange way. This call should fail in that case, but doesn't after the recent patch was merged:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L350"
1,"Works fine: neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1
Generates:
    [{u'ip_address': u'10.0.0.1'}]
neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1 --request-format xml
Generates:
    {'allowed_address_pair': {'ip_address': '10.0.0.1'}}
2013-09-24 14:10:34.021 19471 ERROR neutron.api.v2.resource [-] update failed
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 460, in update
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource allow_bulk=self._allow_bulk)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 589, in prepare_request_body
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource attr_vals['validate'][rule])
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/extensions/allowedaddresspairs.py"", line 53, in _validate_allowed_address_pairs
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource raise AllowedAddressPairsMissingIP()
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource AllowedAddressPairsMissingIP: AllowedAddressPair must contain ip_address
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource"
0,"Passing mutable objects as default args is a known Python pitfall.
We'd better avoid this.
This is an example show the pitfall:
http://docs.python-guide.org/en/latest/writing/gotchas/"
1,"When uploading a volume to an image with an unsupported disk type (anything other than vmdk), an orphan image stuck in ""Queued"" state gets left behind.
Steps to reproduce:
1. cinder create 1
2. cinder upload-to-image --disk raw <volume id> <name>
The result is an orphan image in Glance that gets left behind. This image should not exist."
1,After deletion of a volume which was created by cloning Nexenta backend still contains snapshot used for cloning.
1,"Given the expense of sudo (at scale) and rootwrap calls, agents should not be using root for commands that don't need it. Listing namespaces is one of those.
(I could have sworn I already fixed this which is why I didn't fix it until today)"
1,Volume migration API does not convert string to bool for the force_host_copy parameter.
1,"the comments of nova.scheduler.filters.type_filter.py:
class TypeAffinityFilter(filters.BaseHostFilter):
...
def host_passes(self, host_state, filter_properties):
        """"""Dynamically limits hosts to one instance type
        Return False if host has any instance types other then the requested
        type. Return True if all instance types match or if host is empty.
        """"""
...
'other then' in the next-to-last line should be 'other than'"
0,"The unit test for the NetApp E-Series volume driver (cinder/tests/test_netapp_eseries_iscsi.py) is missing cleanup operations to ensure other tests using the python-requests library are not affected by its setup.
By assigning a mock object to requests.Session during setup and not restoring the original during teardown, the test causes any following code trying to invoke requests.Session.send to fail with the message:
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Attached is a test case (test_requests_mock.py) that when run alone will pass but fails when being executed after the NetApp E-Series test in the same run:
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock
[...]
Ran 1 test in 5.007s
OK
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock cinder.tests.test_netapp_eseries_iscsi
[...]
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Ran 15 tests in 5.030s
FAILED (failures=1)"
0,"Currently, nova doesn't enable pae setting for Xen or KVM guest in its libvirt driver. Windows(Win7 in my enviroment) guests would not boot successful in such case. This patch adds pae setting in libvirt driver for Xen or KVM guest, which would fix this problem."
0,"When running 'tox -epy27 services.loadbalancer' the following failures are encountered:
Traceback (most recent call last):
  File ""/home/eugene/quantum/neutron/tests/unit/services/loadbalancer/drivers/haproxy/test_agent.py"", line 32, in setUp
    cfg.CONF.register_opts(agent.OPTS)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1540, in __inner
    result = f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1673, in register_opts
    self.register_opt(opt, group, clear_cache=False)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1544, in __inner
    return f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1662, in register_opt
    if _is_opt_registered(self._opts, opt):
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 486, in _is_opt_registered
    raise DuplicateOptError(opt.name)
DuplicateOptError: duplicate option: periodic_interval"
1,"When creating a logic router using plugin NvpAdvancedPlugin with service_router=True, the Edge service router status sometimes show PENDING_CREATE even when the Edge is created successfully.

Also, if the plugin is restarted, deleting an logic router does not delete Edge router."
1,"l3 agent dies after MessagingTimeout error, attached is the trace.
I am not sure about the root cause of the MessagingTimeout either but that seems a different problem, I think agent should survive the timeout and retry maybe."
1,"Steps are as following:
1) Create one VM
2) Attach volume to the VM
3) pause the VM
4) detach the volume
5) unpause the VM
6) re-attch the VM to same device, nova compute throw exception
2013-10-20 23:21:22.520 DEBUG amqp [-] Channel open from (pid=19728) _open_ok /usr/local/lib/python2.7/dist-packages/amqp-1.0.12-py2.7.egg/amqp/channel.py:420
2013-10-20 23:21:22.520 ERROR nova.openstack.common.rpc.amqp [req-5f0d786e-1273-4611-b0a5-a787754c6bc8 admin admin] Exception during message handling
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp pass
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 3649, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp context, instance, mountpoint)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 3644, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp mountpoint, instance)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 3690, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp connector)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 3680, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp encryption=encryption)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1107, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp raise exception.DeviceIsBusy(device=disk_dev)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp DeviceIsBusy: The supplied device (vdb) is busy.
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp
^C2013-10-20 23:21:24.871 INFO nova.openstack.common.service [-] Caught SIGINT, exiting"
1,"In my case, triggered by running neutron-usage-audit. Fix underway.
Traceback (most recent call last):
  File ""~/bin/neutron-usage-audit"", line 10, in <module>
    sys.exit(main())
  File ""~/neutron/cmd/usage_audit.py"", line 37, in main
    plugin = manager.NeutronManager.get_plugin()
  File ""~/neutron/manager.py"", line 211, in get_plugin
    return cls.get_instance().plugin
  File ""~/neutron/manager.py"", line 206, in get_instance
    cls._create_instance()
  File ""~/neutron/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""~/neutron/manager.py"", line 200, in _create_instance
    cls._instance = cls()
  File ""~/neutron/manager.py"", line 112, in __init__
    plugin_provider)
  File ""~/neutron/manager.py"", line 140, in _get_plugin_instance
    return plugin_class()
  File ""~/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 263, in __init__
    db.sync_network_states(self.network_vlan_ranges)
  File ""~/neutron/plugins/linuxbridge/db/l2network_db_v2.py"", line 87, in sync_network_states
    'physical_network': physical_network})
UnboundLocalError: local variable 'physical_network' referenced before assignment"
0,"Currently, the network cache assumes neutron is the source of truth for which interfaces are actually attached to an instance. This is not actually correct as nova is really the source of truth here. In order to demonstrate this issue if one creates multiple ports in neutron that match the same device_id/instance_id as instances in nova those ports will show up in nova list even though they are not part of the instance."
1,"I tried live-migration against VM with multipath access to FC bootable volume and FC data volume.
After checking the code, I found the reason is that
1. /dev/dm-<NUM> is used, which is subject to change in the destination Compute Node since it is not unique across nodes
2. multipath_id in connnection_info is not maintained properly and may be lost during connection refreshing
The fix would be
1. Like iSCSI multipath, use /dev/mapper/<multipath_id> instead of /dev/dm-<NUM>
2. Since multipath_id is unique for a volume no matter where it is attached, add logic to preserve this information."
1,"Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOnRlbXBlc3Quc2NlbmFyaW8udGVzdF9sb2FkX2JhbGFuY2VyX2Jhc2ljLlRlc3RMb2FkQmFsYW5jZXJCYXNpYy50ZXN0X2xvYWRfYmFsYW5jZXJfYmFzaWMgQU5EIG1lc3NhZ2U6RkFJTEVEIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3OTE1MTA0MTQwfQ=="
0,"Investigating some tempest failures in this area led me to this issue. devstack currently has a bug re: configuration of fwaas. This leads the service to be enabled, but the agent does get passed the relevant config files /w fwaas config. On firewall creation, the following traceback appears while the firewall stays in PENDING_CREATE:

neutron.services.firewall.agents.l3reference.firewall_l3_agent [req-2b7a801e-7358-418e-b4e7-95b7b27aefc2 None] FWaaS RPC failure in create_firewall for fw: f24bd240-04d5-49f1-971c-8ae95e666ef0
neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
neutron.services.firewall.agents.l3reference.firewall_l3_agent File ""/opt/stack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 133, in _invoke_driver_for_plugin_api
neutron.services.firewall.agents.l3reference.firewall_l3_agent self.fwaas_driver.__getattribute__(func_name)(
neutron.services.firewall.agents.l3reference.firewall_l3_agent AttributeError: 'VPNAgent' object has no attribute 'fwaas_driver'
neutron.services.firewall.agents.l3reference.firewall_l3_agent"
1,"Recently merged patch - https://review.openstack.org/#/c/42090 - adds an ability to update status of a member according to health statistics coming from lbaas agent so that if a member stops responding it is marked as INACTIVE. Such members however are not passed to the agent on next reload_pool operation (pool update, new member, new monitor, etc) so remain INACTIVE even if corresponding instance is repaired."
0,"The change to use InstanceActionEvent objects in compute.utils.EventReporter changed the order of how things are done. Before, traceback info were converted to strings before being sent to the conductor. Now, since the object method being used remotes itself, the order becomes the opposite and any captured tracebacks are sent as is, resulting in errors during messaging.

See http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVmFsdWVFcnJvcjogQ2lyY3VsYXIgcmVmZXJlbmNlIGRldGVjdGVkXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTk2MjYzMjYwODZ9"
1,"The netapp_server_port setting in cinder.conf is ignored by both the NetApp NFS and iSCSI drivers. Ironically it is marked as a required flag.
NetAppDirectNfsDriver._get_client() creates a client from NaServer but no port argument is passed to init(). NaServer.set_transport_type() sets the port based on the configured transport_type (URI scheme), http=80 and https=443. netapp_server_port appears to be largely unused.
A quick work-around is to set the client port from self.configuration.netapp_server_port in NetAppDirectNfsDriver._get_client() immediately after creating the client. This may not be the cleanest solution but it solves the immediate problem I had.
There is a similar problem in NetAppDirectISCSIDriver._create_client().
It seems to me that a better solution would be to add a port argument to NaServer.init() and fall back to the scheme-based defaults if no arg is supplied. That may have ramifications that I am unaware of."
0,"Check N310 can return ""N310 timeutils.now() must be used instead of datetime.now()"", but timeutils.now() does not exist. Only utcnow() does."
0,"When booting an instance if the network_uuid is not of a valid network the following error is returned:

ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-f86b297f-bbec-4ca1-93f9-f495250f1a3f) (This doesn't really tell the user what resource is not found which should be improved).

In addition http response code should be 400 not 404 to align with other resources i.e security_groups:
ERROR: Unable to find security_group with name 'asdfasdf' (HTTP 400) (Request-ID: req-ff8b528a-50cf-4ca5-9598-b9ed1a69482d)"
0,Tags: low-hanging-fruit
1,"Currently a sparse disk based image is copied as a flat file which is one of the causes of the following bug:
https://bugs.launchpad.net/nova/+bug/1255317
Also, 'vmdk_type' extra spec property of the cinder volume is ignored.
The image should be copied, and then converted to appropriate type based on 'vmdk_type'."
1,"A FloatingIP can be not associated with an FixedIP, which will cause its fixed_ip field in the database model to be None. Currently, FloatingIP's _from_db_object() method always assumes it's non-None and thus tries to load a FixedIP from None, which fails."
0,"cfg.CONF.state_path is set to a random temporary directory in neutron.tests.base:BaseTestCase.setUp. This value was then over written in neutron.tests.unit.__init__. Tests that need to read or otherwise use cfg.CONF.state_path were getting the directory from which the tests were running and not the temporary directory specially created for the current test run.
Note that the usage of state_path to set lock_path, dhcp state path and the likes was working as expected, and was not affected by this bug."
1,"Accounts have historically reported three numbers:
                container_count INTEGER,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0,
When we broke out stats per-policy for some reason we only did two of the three:
            CREATE TABLE policy_stat (
                storage_policy_index INTEGER PRIMARY KEY,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0
            );
We should track container_count in POLICY_STAT_TRIGGER_SCRIPT"
1,"The metering iptables driver doesn't read the root_helper param, thus it uses the default value."
0," from dnsmasq(8):

      -h, --no-hosts
              Don't read the hostnames in /etc/hosts.

I reliably get bit by this during certain kinds of deployments, where my nova-network/dns host has an entry in /etc/hosts such as:

127.0.1.1 hostname.example.com hostname

I keep having to edit /etc/hosts on that machine to use a real IP, because juju gets really confused when it looks up certain openstack hostnames and gets sent to its own instance!"
1,"The validate_ec2_id() method is used to validate both the Instance ID as well as Volume ID for valid EC2 ID format.
However the exception class raised in both cases, if the respective ID were invalid is ""InvalidInstanceIDMalformed"".
This is ambiguous and needs to be fixed such that a clearer exception is seen in the stack trace.

The exception class InvalidInstanceIDMalformed in exception.py could be changed to something like InvalidEC2IDMalformed."
0,"The nova-cells service looks up instances locally before passing them to the local compute api, and only converts them to objects if the compute api method is explicitly listed in the run_compute_api method. There is in fact a FIXME around this process, but it appears to not have been addressed yet :)
2014-03-10 17:27:59.881 30193 ERROR nova.cells.messaging [req-3e27c8c0-6b3c-482d-bb9b-d638933ec949 10226892 5915610] Error processing message locally: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1290, in _process_message_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 706, in run_compute_api_method
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 2988, in update_instance_metadata
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging orig = dict(instance.metadata)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging"
1,"While creating Baremetal Node by `nova baremetal-node-create' with --pm_password '' [1] and starting a deployment, IPMI power manager hangs with showing prompt ""Password:"" in nova-compute process. IPMI power manager creates an empty file and specifies it as the password file in the ipmitool command line, but ipmitool ignores that file [2].
This is an uncommon case that an administrator set password empty, but this is not a low importance bug due to it stops the thread.
I think we can avoid this bug by writing '\0' into the password file [3], since ipmitool checks return value of fgets() is not NULL which means the file is not start with EOF and no error had occurred.
[1] e.g.:
    $ nova baremetal-node-create --pm_address 192.0.2.200 --pm_user admin --pm_password '' service-host 1 1000 10000 00:11:22:33:44:55
[2] In ipmitool manpage:
    -f <password_file>
        Specifies a file containing the remote server password. If this option is absent, or if password_file is empty, the password will default to NULL.
[3] I checked that ipmitool works with a file containing '\0';
    # touch a
    # ipmitool -I lanplus -H 192.0.2.94 -f a -U administrator power status
    Unable to read password from file a
    Unable to read password from file a
    Password: <-- Enter
    Chassis Power is off
    # echo -ne '\0' > b
    # ipmitool -I lanplus -H 192.0.2.94 -f b -U administrator power status
    Chassis Power is off"
1,"The method which gets VM related information can fail if the VM is in an intermediary state such as ""Shutting down"".
The reason is that some of the Hyper-V specific vm states are not defined as possible states.
This will result into a key error as shown bellow:
http://paste.openstack.org/show/90015/"
0,"I bulk created subnets and ports by neutron api.
Using these ports boot instance. I found that the vms could not get ip address.
Then I saw the source code and found that _send_dhcp_notification function only handle ['network', 'subnet', 'port'] resource.
I think we need to handle collections such as ['networks', 'subnets', 'ports'] here."
1,"when parallel tempest tests are enabled, the nicira plugin shows erros when deleting routers.
Parallel operations indeed cause the usual eventlet/mysql deadlock in delete_router as the nvp operation is nested within the db transaction.
The root cause for the deadlock is that the nvp api client uses eventlet to dispatch requests.
while a solution might be to rework the API client, an easier, backportable solution would be to move the NVP operation out of the transaction and ensuring consistency in case of failure.
note: in the same delete_router routine also the metada access network handling should be moved out of the transaction."
0,"Some config options(interface_driver, use_namespaces, periodic_interval) are defined in multiple sources in ad-hoc way.
It may cause DuplicateOptError exception when using those module at the same time.
Right now the exception is avoided in ad-hoc way by each executables.
Those definition/registering should be consolidated.

This is the blocker for BP of l3 agent consolidation.
https://blueprints.launchpad.net/neutron/+spec/l3-agent-consolidation"
1,"In horizon:
1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot using a size greater than the original.
4. Horizon shows the volume with Status=Error
c-vol logs:
2014-02-12 11:11:20.785 ERROR cinder.volume.flows.manager.create_volume [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Volume e07e57c6-7cc1-4b60-8050-8187d99e0006: create failed
2014-02-12 11:11:20.786 DEBUG cinder.openstack.common.lockutils [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Released file lock ""00844d93-9233-4b14-a238-e94916936807-delete_snapshot"" at /opt/stack/data/cinder/cinder-00844d93-9233-4b14-a238-e94916936807-delete_snapshot for method ""_run_flow_locked""... from (pid=28714) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
2014-02-12 11:11:20.786 ERROR cinder.openstack.common.rpc.amqp [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Exception during message handling
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp **args)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 355, in create_volume
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp _run_flow_locked()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 350, in _run_flow_locked
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp _run_flow()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 346, in _run_flow
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp flow_engine.run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 110, in run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp self._run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in _run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp self._revert(misc.Failure())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 75, in _revert
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp misc.Failure.reraise_if_any(failures.values())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 390, in reraise_if_any
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp failures[0].reraise()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 397, in reraise
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp six.reraise(*self._exc_info)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/task_action.py"", line 96, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp result = self._task.execute(**kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp **volume_spec)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 398, in _create_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp snapshot_ref)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 145, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp snapshot)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 797, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp raise exception.InvalidInput(reason=err)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp InvalidInput: Invalid input received: You cannot change size of the volume. It must
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp"
1,"Any errors after the XenServer migrate command completes currently can cause the users VM to be deleted.
While there should be some cleanup performed, deleting the VM does not make sense for the XenAPI driver."
1,"The following rule is unable to be installed:
$ neutron security-group-rule-create --direction ingress default
409-{u'NeutronError': {u'message': u'Security group rule already exists. Group id is 29dc1837-75d3-457a-8a90-14f4b6ea6db9.', u'type': u'SecurityGroupRuleExists', u'detail': u''}}
The reason for this is when the db query is done it passes this in as a filter:
{'tenant_id': [u'577a2f0c78fb4e36b76902977a5c1708'], 'direction': [u'ingress'], 'ethertype': ['IPv4'], 'security_group_id': [u'0fb10163-81b2-4538-bd11-dbbd3878db51']}
and the remote_group_id is wild carded thus it matches this rule:
[ {'direction': u'ingress',
  'ethertype': u'IPv4',
  'id': u'8d5c3429-f4ef-4258-8140-5ff3247f9dd6',
  'port_range_max': None,
  'port_range_min': None,
  'protocol': None,
  'remote_group_id': None,
  'remote_ip_prefix': None,
  'security_group_id': u'0fb10163-81b2-4538-bd11-dbbd3878db51',
  'tenant_id': u'577a2f0c78fb4e36b76902977a5c1708'}]"
1,"in nova-network , when allocate_fixed_ip failed for some unknown reason
it will add
cleanup.append(fip.disassociate)
to cleanup the stuffs it did when handle exception
but the function is following in objects/fixed_ips.py
def disassociate(self, context):
so the cleanup function will not be executed correctly
try:
                        f()
                    except Exception:
                        LOG.warn(_('Error cleaning up fixed ip allocation. '
                                   'Manual cleanup may be required.'),
                                 exc_info=True)"
0,"cinder.scheduler.host_manager.HostManager doesn't have get_host_list()
and get_service_capabilities() method.

But I look like which cinder.scheduler.driver.Scheduler is trying to call them.
Are they not being used? I think they can be deleted."
1,"Operation details are as following:
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE | None | Running | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova reboot lxc1 --hard
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | HARD_REBOOT | rebooting_hard | Running | private=10.0.0.2 |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE | None | Running | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize lxc1 m1.lxc
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+---------------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | VERIFY_RESIZE | None | Running | private=10.0.0.2 |
+--------------------------------------+------+---------------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize-confirm lxc1
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE | None | Running | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ vir list
Id Name State
----------------------------------------------------
9312 instance-00000052 running
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk (deleted))
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)"
0,"I'm seeing something like this in many unrelated changes:

2014-06-26 15:55:57.452 | FAIL: glance.tests.unit.test_quota.TestImageLocationQuotas.test_add_too_many_image_locations
2014-06-26 15:55:57.452 | tags: worker-0
2014-06-26 15:55:57.452 | ----------------------------------------------------------------------
2014-06-26 15:55:57.452 | Traceback (most recent call last):
2014-06-26 15:55:57.452 | File ""glance/tests/unit/test_quota.py"", line 568, in test_add_too_many_image_locations
2014-06-26 15:55:57.452 | self.assertIn('Attempted: 2, Maximum: 1', str(exc))
2014-06-26 15:55:57.452 | File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-06-26 15:55:57.452 | raise UnicodeError(msg)
2014-06-26 15:55:57.452 | UnicodeError: Message objects do not support str() because they may contain non-ascii characters. Please use unicode() or translate() instead."
1,"It appears a regression was introduced in:
https://review.openstack.org/#/c/36363/
Where the API cell is now always doing a _local_delete()... before telling child cells to delete the instance. There's at least a couple of bad side effects of this:
1) The instance disappears immediately from API view, even though the instance still exists in the child cell. The user does not see a 'deleting' task state. And if the delete fails in the child cell, you have a sync issue until the instance is 'healed'.
2) Double delete.start and delete.end notifications are sent. 1 from API cell, 1 from child cell.
The problem seems to be that _local_delete is being called because the service is determined to be down... because the compute service does not run in the API cell."
1,"I am able to set a flavor-key but not unset it. devstack sha1=fdf1cffbd5d2a7b47d5bdadbc0755fcb2ff6d52f
ubuntu@d8:~/devstack$ nova help flavor-key
usage: nova flavor-key <flavor> <action> <key=value> [<key=value> ...]
Set or unset extra_spec for a flavor.
Positional arguments:
  <flavor> Name or ID of flavor
  <action> Actions: 'set' or 'unset'
  <key=value> Extra_specs to set/unset (only key is necessary on unset)
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny set foo=bar
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property | Value |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled | False |
| OS-FLV-EXT-DATA:ephemeral | 0 |
| disk | 1 |
| extra_specs | {""foo"": ""bar""} |
| id | 1 |
| name | m1.tiny |
| os-flavor-access:is_public | True |
| ram | 512 |
| rxtx_factor | 1.0 |
| swap | |
| vcpus | 1 |
+----------------------------+----------------+
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny unset foo
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property | Value |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled | False |
| OS-FLV-EXT-DATA:ephemeral | 0 |
| disk | 1 |
| extra_specs | {""foo"": ""bar""} |
| id | 1 |
| name | m1.tiny |
| os-flavor-access:is_public | True |
| ram | 512 |
| rxtx_factor | 1.0 |
| swap | |
| vcpus | 1 |
+----------------------------+----------------+"
0,"""is not"" operator compares two objects are identical.
When comparing values, ""!="" should be used."
0,Any quota request made to neutron fails with the bigswitch plugin. Enabling the quota extension will fix this.
1,"EC2 describe images crashes on volume backed instance snapshot which has several volumes:
$ euca-describe-images
euca-describe-images: error (KeyError): Unknown error occurred.
Steps to reproduce
1 Create bootable volume
$ cinder create --image <image-id> <size>
2 Boot instance from volume
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=<volume_id>:::1 inst
3 Create empty volume
$ cinder create 1
4 Attach the volume to the instance
$ nova volume-attach inst <empty-volume-id> /dev/vdd
5 Create volume backed snapshot
$ nova image-create inst sn-in
6 Describe EC2 images
$ euca-describe-images"
1,"auth token is exposed in meter http.request
# curl -i -X GET -H 'X-Auth-Token: 258ab6539b3b4eae8b3af307b8f5eadd' -H 'Content-Type: application/json' -H 'Accept: application/json' -H 'User-Agent: python-ceilometerclient' http://0.0.0.0:8777/v2/meters/http.request
-----------
snip..
{""counter_name"": ""http.request"", ""user_id"": ""0"", ""resource_id"": ""ip-9-37-74-33:8774"", ""timestamp"": ""2014-05-16T17:42:16.851000"", ""recorded_at"": ""2014-05-16T17:42:17.039000"", ""resource_metadata"": {""request.CADF_EVENT:initiator:host:address"": ""9.44.143.6"", ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"", ""request.RAW_PATH_INFO"": ""/v2/9af97e383dad44969bd650ebd55edfe0/servers/060c76a5-0031-430d-aa1e-01f9b3db234b"", ""request.REQUEST_METHOD"": ""DELETE"", ""event_type"": ""http.request"", ""request.HTTP_X_TENANT_ID"": ""9af97e383dad44969bd650ebd55edfe0"", ""request.CADF_EVENT:typeURI"": ""http://schemas.dmtf.org/cloud/audit/1.0/event"", ""request.HTTP_X_PROJECT_NAME"": ""ibm-default"", ""host"": ""nova-api"", ""request.SERVER_PORT"": ""8774"", ""request.REMOTE_PORT"": ""55258"", ""request.HTTP_X_USER_ID"": ""0"", ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478"", ""request.CADF_EVENT:action"": ""delete"", ""request.CADF_EVENT:target:typeURI"": ""service/compute/servers/server"", ""request.HTTP_USER_AGENT"": ""Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0"",
snip...
auth token is masked in ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"".
But it is exposed in ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478"""
0,"currently, most neutron codes name method name following pep8 recommendation while some code in test_ovs_tunnel.py not, this is found while review https://review.openstack.org/#/c/45725/
ref: pep8 recommendation: http://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables"
1,Attempting to block iptables across the bridge via iptables rules is not working. The iptables rules are never hit. blocking dhcp traffic from exiting the node will need to use ebtables instead.
1,"Locally to an OVS agent, when the last port of a network disappears the local VLAN isn't reclaim."
1,"During instance spawn, Ironic attempts to unplug any plugged VIFs from ports associated with an instance. If there are no associated VIFs to unplug, instance spawn fails with a nova-compute errror:
2014-03-14 21:15:35.907 16640 TRACE nova.openstack.common.loopingcall HTTPBadRequest: Couldn't apply patch '[{'path': '/extra/vif_port_id', 'op': 'remove'}]'. Reason: u'vif_port_id'
The driver should be only attempt to unplug VIFs from ports that actually have them associated."
1,"While setting the bridge up, if the network interface has a dynamic address, the 'dynamic' flag will be displayed in the ""ip addr show"" command:
[fedora@dev1 devstack]$ ip addr show dev eth0 scope global
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:00:00:01 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.2/24 brd 192.168.122.255 scope global dynamic eth0
       valid_lft 2225sec preferred_lft 2225sec
When latter executing ""ip addr del"" with the IPv4 details, the 'dynamic' flag is not accepted, causes the command to crash and leaves the bridge half configured."
0,"in the post_live_migration_at_destination method, the to_xml method is called but the return value never be used, and the comment say that the reason is 'the uuid is not included in to_xml() result'.
but I think the uuid is already included in to_xml() result now, so we may refactor post_live_migration_at_destination method and change it to use the correct way to get the xml of migrated instance.

# In case of block migration, destination does not have
# libvirt.xml
disk_info = blockinfo.get_disk_info(CONF.libvirt_type,
                                    instance)
self.to_xml(instance, network_info, disk_info,
            block_device_info, write_to_disk=True)
# libvirt.xml should be made by to_xml(), but libvirt
# does not accept to_xml() result, since uuid is not
# included in to_xml() result.
dom = self._lookup_by_name(instance[""name""])
self._conn.defineXML(dom.XMLDesc(0))"
0,"Nova quota-classes request should be in the format like:
body = {'quota_class_set': {'instances': 50, 'cores': 50,
                            'ram': 51200, 'floating_ips': 10,
                            'fixed_ips': -1, 'metadata_items': 128,
                            'security_groups': 10,
                            'security_group_rules': 20,
                            'key_pairs': 100,
                            }}
But in some unit test cases, the id parameter is found in the body, it should be deleted.
 def test_quotas_update_as_admin(self):
     body = {'quota_class_set': {'instances': 50, 'cores': 50,
                                 'ram': 51200, 'floating_ips': 10,
                                 'fixed_ips': -1, 'metadata_items': 128,
                                 鈽?id': 'test_class', 鈽?                                 'security_groups': 10,
                                 'security_group_rules': 20,
                                 'key_pairs': 100}}"
0,"When running libvirt tests via testtools.run, there are a number of cases which fail due to lockutils setup
$ .venv/bin/python -m testtools.run nova.tests.virt.libvirt.test_driver
..snip...
======================================================================
ERROR: nova.tests.virt.libvirt.test_driver.IptablesFirewallTestCase.test_multinic_iptables
----------------------------------------------------------------------
pythonlogging:'': {{{INFO [nova.network.driver] Loading network driver 'nova.network.linux_net'}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_driver.py"", line 10182, in test_multinic_iptables
    self.fw.prepare_instance_filter(instance_ref, network_info)
  File ""nova/virt/firewall.py"", line 184, in prepare_instance_filter
    self.refresh_provider_fw_rules()
  File ""nova/virt/firewall.py"", line 474, in refresh_provider_fw_rules
    self._do_refresh_provider_fw_rules()
  File ""nova/openstack/common/lockutils.py"", line 267, in inner
    with lock(name, lock_file_prefix, external, lock_path):
  File ""/usr/lib64/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""nova/openstack/common/lockutils.py"", line 231, in lock
    ext_lock = external_lock(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 180, in external_lock
    lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 171, in _get_lock_path
    raise cfg.RequiredOptError('lock_path')
RequiredOptError: value required for option: lock_path
The tox.ini / run_tests.sh work around this problem by using ""-m nova.openstack.common.lockutils"" but this is somewhat tedious to remember to add. A simple mock addition to the tests in question can avoid the issue in the first place."
0,"I am trying to use VMware hypervisor ESXi 5.0.0 as a compute resource in OpenStack 2013.1.1. The driver being used is ""?vmwareapi.VMwareVCDriver"". The vCenter is contains a single cluster (two nodes) running ESXi version 5.0.

In the compute node, I am seeing the below error"
1,"The lvm driver uses positional arguments to create a new instance for the LVM brick. Some of those arguments are being passed in the wrong position. Kwargs should be used in this case instead.
http://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py#n711"
0,"Network manager methods add_fixed_ip_to_instance() and remove_fixed_ip_from_instance() both return with updated nw_info models. The corresponding network API methods however returns nothing, which has the following effect:
Both API methods have the @refresh_cache decorator that tries to update instance info cache from the decorated method's return value. In absence of a return value, it will make a new rpc call to to get the missing nw_info model. By changing the two API methods so that they return the models that they in fact already get, these extra calls can be avoided altogether.
In addition, having the API methods return updated nw_info models make it possible to further improve as in compute manager, calls to these methods are immediately followed by calls to get updated nw_info."
0,"The unit tests in nova.tests.image.test_glance unfortunately make use of a faked glance image service in nova.tests.image.fakes. What this does is actually mask a number of problems and makes it harder to assert for specific behavior in the real glanceclient client classes.
The unit tests should be rewritten to simply mock out the very specific code boundaries where tested calls interact with the glanceclient client classes, and that's it. Unit tests should just test one little unit of code, not giant swathes of dependent code."
0,"The tempfile.mktemp() function has been deprecated since Python 2.3 due to security issues. There are more secure alternatives available, such as tempfile.TemporaryFile(). There are more details on this in the Python tempfile documentation.

Swift is using tempfile.mktemp() in a few locations in the profiling middleware:

  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/html_viewer.py
  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/profile_model.py

These should be modified to use a secure method of temporary file creation for security hardening reasons."
1," nova host-update --status enable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-e4a1d1cc-9799-4748-b065-6bb938126134 None None] Updating compute service status to: disabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628 << This should be ""updating to enabled""
 nova host-update --status disable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-90b2cb09-e995-4676-8a1e-7774bb8a0f12 admin admin] Updating compute service status to: enabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628 << This should be ""updating to disabled"""
0,The server manager component of the BigSwitch plugin needs more unit tests to exercise some functions that are currently mocked out as part of the existing unit tests.
0,"Add new behavior to control update and delete operations for the cisco-network-profile resource extension.
The new behavior is to allow update and delete operations only if there are no neutron networks associated with a particular cisco-network-profile instance."
0,"The replication mode on switches and routers should have been configurable
to use use source replication if one did not want to deploy service node(s)."
0,"In a few points throughout the codebase, mutable lists and mutable dicts are being used as default function/method arguments.

In Python, this is an issue since functions are treated as objects that can maintain state between calls. As a result, this only gets set once, and it's possible for it to stack list values over time in cases when you might expect them to be empty. Depending on use, this can cause incredibly complex and yet very subtle bugs in code that reads just fine. In Glance's case, since a few instances of this are in several ACL-related methods in glance.store.*, there is *potential* for security concern (not confirmed).

Here's some additional information illustrating and explaining this behavior in Python:
http://effbot.org/zone/default-values.htm
http://stackoverflow.com/questions/1132941/least-astonishment-in-python-the-mutable-default-argument

There are no comments in the code I've seen that indicate this usage is meant specifically to take advantage of this subtlety in the language. We'd definitely want to document that if it is the case.

Wanted to create this as a discussion point if needed, and as a courtesy to attach it to the patch I'm going to push in a few minutes. The full test suites seem to pass locally, so will be curious what Jenkins has to say."
1,"1.create n/w,subnet
2.create a dvr and attach the subnet
3/create external network and attach the router gateway
4.now boot a vm in that subnet
5.ping to external network -successful
6.create a new network,subnet attach it to router created in step 2.
7.boot a vm and ping to external network -fails
8.try to ping to external network using vm created in step 4 -fails
Reason:
=======
when new subnet is added ,all the sg ports inside snat namespace are updated with default gateway of subnet added
say i had subnet 4.4.4.0/24 already attached to router its sg port had ip 4.4.4.2,now when i add new subnet say 5.5.5.0/24 this router
sg port of 4.4.4.0/24 becomes 5.5.5.1 also sg ip of 5.5.5.0/24 also becomes 5.5.5.1 (even though 5.5.5.1 has device owner =network:router_interface_distributed and 5.5.5.2 has device owner as network:router_centralized_snat)"
0,"""Flags.monkey_patch"" should be ""CONF.monkey_patch"" in the docstring of monkey_patch function in nova/utils.py"
1,"In Midonet plugin, when a VM terminates, floating IP does not get diassociated properly. This left the floating IP associated with a terminated instance and required you to explicitly diassociate in a separate step. This also left MidoNet resources dangling around when the VM terminates."
0,"List of known misspellings
http://paste.openstack.org/show/54354
Generated with:
  pip install misspellings
  git ls-files | grep -v locale | misspellings -f -"
0,"For now there are two cases I got:
1. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_status_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/2b0dea1/console.html#_2014-02-13_06_27_19_371
2. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_default_created_at_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/348ad19/console.html#_2014-02-11_18_42_13_276
It is the same reason as https://bugs.launchpad.net/glance/+bug/1272136 fixed."
1,"I am testing with the templates in https://review.openstack.org/#/c/97366/
I can create a stack. I can use `curl` to hit the webhooks to scale up and down the old-style group and to scale down the new-style group; those all work. What fails is hitting the webhook to scale up the new-style group. Here is a typescript showing the failure:
$ curl -X POST 'http://10.10.0.125:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3A39675672862f4bd08505bfe1283773e0%3Astacks%2Ftest4%2F3cd6160b-d8c5-48f1-a527-4c7df9205fc3%2Fresources%2FNewScaleUpPolicy?Timestamp=2014-06-06T19%3A45%3A27Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=35678396d987432f87cda8e4c6cdbfb5&SignatureVersion=2&Signature=W3aJQ6SR7O5lLOxLEQndbzNB%2FUhefr1W7qO9zNZ%2BHVs%3D'
<ErrorResponse><Error><Message>The request processing has failed due to an internal error:Remote error: ResourceFailure Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)
[u'Traceback (most recent call last):\n', u' File ""/opt/stack/heat/heat/engine/service.py"", line 61, in wrapped\n return func(self, ctx, *args, **kwargs)\n', u' File ""/opt/stack/heat/heat/engine/service.py"", line 911, in resource_signal\n stack[resource_name].signal(details)\n', u' File ""/opt/stack/heat/heat/engine/resource.py"", line 879, in signal\n raise failure\n', u""ResourceFailure: Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)\n""].</Message><Code>InternalFailure</Code><Type>Server</Type></Error></ErrorResponse>
The original sin looks like this in the heat engine log:
2014-06-06 17:39:20.013 28692 DEBUG urllib3.connectionpool [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 ] ""GET /v2/39675672862f4bd08505bfe1283773e0/os-networks HTTP/1.1"" 200 16 _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-06-06 17:39:20.014 28692 ERROR heat.engine.resource [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 None] CREATE : Server ""my_instance"" Stack ""test1-new_style-qidqbd5nrk44-43e7l57kqf5w-4t3xdjrfrr7s"" [20523269-0ebb-45b8-ad59-75f55607f3bd]
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource Traceback (most recent call last):
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource File ""/opt/stack/heat/heat/engine/resource.py"", line 383, in _do_action
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource handle())
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource File ""/opt/stack/heat/heat/engine/resources/server.py"", line 493, in handle_create
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource nics = self._build_nics(self.properties.get(self.NETWORKS))
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource File ""/opt/stack/heat/heat/engine/resources/server.py"", line 597, in _build_nics
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource network = self.nova().networks.find(label=label_or_uuid)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource File ""/opt/stack/python-novaclient/novaclient/base.py"", line 194, in find
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource raise exceptions.NotFound(msg)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource NotFound: No Network matching {'label': u'private'}. (HTTP 404)
Private debug logging reveals that in the scale-up case, the call to ""GET /v2/{tenant-id}/os-networks HTTP/1.1"" returns with response code 200 and an empty list of networks. Comparing with the corresponding call when the stack is being created shows no difference in the calls --- because the normal logging omits the headers --- even though the results differ (when the stack is being created, the result contains the correct list of networks). Turning on HTTP debug logging in the client reveals that the X-Auth-Token headers differ."
1,"%neutron security-group-list
+--------------------------------------+-----------------------------+--------------------------------+
| id | name | description |
+--------------------------------------+-----------------------------+--------------------------------+
| 0163fc21-e0d2-4219-9efc-47c71e102ba8 | default | default |
| 1023c62b-029f-4faf-9257-97ffc870fffd | default | default |
| 7c096113-2e5d-4fc3-b3ba-8462ec5e6964 | default | default |
| b9fc7dc0-c22b-45ee-a811-d2a11b7e864f | security-tempest-1342522857 | description-tempest-1430856972 |
| dfc6bff9-b412-4f47-b38b-6d0079412a4d | default | default |
+--------------------------------------+-----------------------------
%neutron security-group-delete b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
500-{u'NeutronError': {u'message': u'An unknown exception occurred.', u'type': u'NeutronException', u'detail': u''}}
neutron:/var/log/neutron.log
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6997] Completed request 'POST https://x.x.x.x:443//ws.v1/security-profile': 201 (0.06 seconds)
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774691'
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6997] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6997] Completed request 'POST /ws.v1/security-profile': 201
2013-11-14 05:08:05,039 (neutron.plugins.nicira.nvplib): DEBUG nvplib create_security_profile Created Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quan
tum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'log
ical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group.create.end on notifications.info
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 3221f6dd937e4657993532db6910d7fc.
2013-11-14 05:08:05,069 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:05,070 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,071 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,072 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""POST /v2.0/security-groups.json HTTP/1.1"" 201 954 0.180143
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,283 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,285 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:05,288 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:05,303 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 200 949 0.023315
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Matched POST /security-group-rules.json
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Route path: '/security-group-rules.:(format)', defaults: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>}
2013-11-14 05:08:05,315 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>, 'format': u'json'}
2013-11-14 05:08:05,316 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.start on notifications.info
2013-11-14 05:08:05,317 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 66fa31f3dc8d4f2eb9cd7fb561a17f49.
2013-11-14 05:08:05,324 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,325 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.client): DEBUG client acquire_connection [6998] Acquired connection https://x.x.x.x:443. 8 connection(s) available.
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Issuing - request PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Setting X-Nvp-Wait-For-Config-Generation request header: '37774691'
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:05Z into {'tz_sign': None, 'second_fraction': None, 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'05', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'second' with default None
2013-11-14 05:08:05,401 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_put Storing 81224fd97e0fca9805338cbbb08a3900 token in memcache
2013-11-14 05:08:05,402 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: service-user with project_id : csi-tenant-admin and roles: csi-tenant-admin,csi-role-admin
2013-11-14 05:08:05,404 (routes.middleware): DEBUG middleware __call__ No route matched for GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Matched GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>}
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>, 'format': u'json'}
2013-11-14 05:08:05,423 (neutron.wsgi): INFO log write 10.140.129.30,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/ports.json?network_id=07490058-67bd-4038-9ae2-bcc7aa973b02&device_owner=network%3Adhcp HTTP/1.1"" 200 136 0.039706
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Completed request 'PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200 (0.06 seconds)
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774694'
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6998] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6998] Completed request 'PUT /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200
2013-11-14 05:08:05,458 (neutron.plugins.nicira.nvplib): DEBUG nvplib update_security_group_rules Updated Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quantum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 22, u'port_range_min': 22, u'protocol': 6}, {u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'logical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.end on notifications.info
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is d7b3139142a04511867d57090f7e22cf.
2013-11-14 05:08:05,497 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:12,001 (neutron.api.v2.resource): ERROR resource resource show failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 290, in show
    parent_id=parent_id),
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 258, in _item
    obj = obj_getter(request.context, id, **kwargs)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 180, in get_security_group
    context, id), fields)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 194, in _get_security_group
    raise ext_sg.SecurityGroupNotFound(id=id)
SecurityGroupNotFound: Security group b9fc7dc0-c22b-45ee-a811-d2a11b7e864f does not exist
2013-11-14 05:08:12,002 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:12] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 404 277 0.032987
* Delete
2013-11-14 18:50:04,498 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [8422] Completed request 'DELETE /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Received error code: 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Server Error Message: Security Profile 'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f' not registered.
2013-11-14 18:50:04,498 (neutron.plugins.nicira.nvplib): ERROR nvplib delete_security_profile Error. Unknown exception: An unknown exception occurred.. locals=[{'path': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'e': NotFound(u'An unknown exception occurred.',), 'spid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'cluster': <neutron.plugins.nicira.nvp_cluster.NVPCluster object at 0x2d96f10>}]
2013-11-14 18:50:04,499 (neutron.api.v2.resource): ERROR resource resource delete failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
    obj_deleter(request.context, id, **kwargs)
    security_group['id'])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/nicira/nvplib.py"", line 1148, in delete_security_profile
    raise exception.NeutronException()
NeutronException: An unknown exception occurred.
2013-11-14 18:50:04,500 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 18:50:04] ""DELETE /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 500 248 0.071508"
1,"Upload volume to image fails with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 108, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 311, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 309, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 177, in call
    raise exception.GlanceConnectionFailed(reason=e)
GlanceConnectionFailed: Connection to glance failed: Error communicating with http://10.20.72.32:9292 timed out
When this happens, the volume gets stuck in 鈥淯ploading鈥?state and never completes."
0,"ESeries filers require appropriate multipath/DMMP configuration on the host running cinder volume process for volume attach and image transfers to work reliably and efficiently, and the cinder.conf option 'use_multipath_for_image_xfer' should be set to True.

Here we will commit a fix to log a warning on driver startup if Eseries backend is being used and 'use_multipath_for_image_xfer' is not set to True."
1,"When the Cisco nexus plugin is configured on DevStack, the recently added tempest Neutron API test test_port_list_filter_by_router_id fails with the following error:
     ParseError: no element found: line 1, column 0
These failures occur for the following classes:
    NetworksIpV6TestXML
    NetworksTestXML"
1,"Currently when specifying NUMA cell memory via flavor extra_specs or image properties, MiB units are used. According to the libvirt xml domain format documentation (http://libvirt.org/formatdomain.html) , cell memory should be specified in KiB.
In this example, we use the following extra_specs:
""hw:numa_policy"": ""strict"", ""hw:numa_mem.1"": ""2048"", ""hw:numa_mem.0"": ""6144"", ""hw:numa_nodes"": ""2"", ""hw:numa_cpus.0"": ""0,1,2"", ""hw:numa_cpus.1"": ""3""
The flavor has 8192 MB of ram and 4 vcpus.
When using qemu 2.1.0, the following will be seen in the n-cpu logs when booting a machine with NUMA specs.
""libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: total memory for NUMA nodes (8388608) should equal RAM size (200000000)""
Please note that the 200000000 is 8388608 KiB in bytes and hex (simply an issue with the qemu error message). The error shows that 8192 KiB is being requested rather than 8192 MiB. Because the RAM size does not equal the total memory size, the machine fails to boot.
When using versions of qemu lower than 2.1.0 the issue is not obvious, as machines with NUMA specs boot, but only because of a bug (that has since been resolved) in qemu. This is because the check to ensure that RAM size equals the NUMA node total memory does not happen in versions lower than 2.1.0
In short, we should be using KiB units for NUMA cell memory, or at least be converting from MiB to KiB before creating the xml. Otherwise, NUMA placement will not behave as intended.
To be fair, I haven't had the chance to look at the memory placement in a guest booted using qemu 2.0.0 or lower, though I suspect the memory placement would be incorrect.. If anyone has the chance to look, it would be greatly appreciated.
I am currently investigating the appropriate fix for this alongside Tiago Mello. We made a quick fix in /nova/virt/libvirt/config.py on line 495:
                cell.set(""memory"", str(self.memory * 1024))
Mutiplying by 1024 allowed the machine to properly boot, but it is probably a bit too quick and dirty. Just thought it would be worth mentioning.
Sys-info:
x86_64 machine
Virt-info:
qemu version 2.1.0
libvirt version 1.2.2
Kenerl-info:
3.13.0-35-generic #62-Ubuntu SMP Fri Aug 15 01:58:42 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
OS-info:
Distributor ID: Ubuntu
Description: Ubuntu 14.04.1 LTS
Release: 14.04
Codename: trusty"
1,"We should always activate the pool at initialization time since we are going to use it anyway.
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/service.py"", line 205, in _child_process
2013-12-13 10:05:14.095 TRACE cinder.service launcher.run_server(server)
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/service.py"", line 96, in run_server
2013-12-13 10:05:14.095 TRACE cinder.service server.start()
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/service.py"", line 388, in start
2013-12-13 10:05:14.095 TRACE cinder.service self.manager.init_host()
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/volume/manager.py"", line 286, in init_host
2013-12-13 10:05:14.095 TRACE cinder.service self.publish_service_capabilities(ctxt)
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/volume/manager.py"", line 912, in publish_service_capabilities
2013-12-13 10:05:14.095 TRACE cinder.service self._report_driver_status(context)
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/volume/manager.py"", line 904, in _report_driver_status
2013-12-13 10:05:14.095 TRACE cinder.service volume_stats = self.driver.get_volume_stats(refresh=True)
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 345, in get_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service self._update_volume_stats()
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 358, in _update_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service self.vg.update_volume_group_info()
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 396, in update_volume_group_info
2013-12-13 10:05:14.095 TRACE cinder.service self.vg_thin_pool)
2013-12-13 10:05:14.095 TRACE cinder.service File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 149, in _get_thin_pool_free_space
2013-12-13 10:05:14.095 TRACE cinder.service consumed_space = float(data[0]) / 100 * (float(data[1]))
2013-12-13 10:05:14.095 TRACE cinder.service ValueError: empty string for float()
2013-12-13 10:05:14.095 TRACE cinder.service
2013-12-13 10:05:14.109 INFO cinder.service [-] Child 5052 exited with status 2
2013-12-13 10:05:14.110 INFO cinder.service [-] _wait_child 1
2013-12-13 10:05:14.110 INFO cinder.service [-] wait wrap.failed True
$ sudo lvchange -a n stack-volumes/stack-volumes-pool
$ sudo lvs -o size,data_percent --separator : stack-volumes/stack-volumes-pool
  LSize:Data%
  9.00g:"
0,"Discussed with the thread starting with
http://lists.openstack.org/pipermail/openstack-dev/2014-July/040644.html

Neutron context isn't populated with auth token unlike nova, glance.
Since there are several (potential) users for it. servicevm project, routervm implementation(cisco csr1kv, vyatta vrouter),
it makes sense for neutron context to include auth token."
0,"hypervisors API are Admin API and unit tests should test those accordingly. But All the V2 hypervisors Unit tests (https://github.com/openstack/nova/blob/master/nova/tests/api/openstack/compute/contrib/test_hypervisors.py) tests those as a non Admin API.
Issue is in fake_policy.py where V2 hypervisors API is not marked as Admin role unlike V3. https://github.com/openstack/nova/blob/master/nova/tests/fake_policy.py#L221"
1,"I saw this in a recent CI overcloud run: http://logs.openstack.org/66/74866/8/check-tripleo/check-tripleo-overcloud-precise/aa490f1/console.html
2014-03-12 20:01:46.509 | Timing out after 300 seconds:
2014-03-12 20:01:46.509 | COMMAND=ping -c 1 192.0.2.46
2014-03-12 20:01:46.509 | OUTPUT=PING 192.0.2.46 (192.0.2.46) 56(84) bytes of data.
2014-03-12 20:01:46.509 | From 192.0.2.46 icmp_seq=1 Destination Host Unreachable
It appears as though everything ran fine up until it tried to ping the booted overcloud instance. I'm fairly certain it has nothing to do with my change, so I wanted to open a bug to track it in case anyone else runs into a similar problem."
0,"""The volume metadata API controller for the OpenStack API."" is inappropriate description for snapshot metadata controller."
1,"Description of problem
---------------------------
Live migration fails.
libvirt says ""XML error: CPU feature `wdt' specified more than once""
Version
---------
ii libvirt-bin 1.2.2-0ubuntu2 amd64 programs for the libvirt library
ii python-libvirt 1.2.2-0ubuntu1 amd64 libvirt Python bindings
ii nova-compute 1:2014.1~b3-0ubuntu2 all OpenStack Compute - compute node base
ii nova-compute-kvm 1:2014.1~b3-0ubuntu2 all OpenStack Compute - compute node (KVM)
ii nova-cert 1:2014.1~b3-0ubuntu2 all OpenStack Compute - certificate management
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=14.04
DISTRIB_CODENAME=trusty
DISTRIB_DESCRIPTION=""Ubuntu Trusty Tahr (development branch)""
NAME=""Ubuntu""
VERSION=""14.04, Trusty Tahr""
Test env
----------
A two node openstack havana on ubuntu 14.04. Migrating a instance to other node.
Steps to Reproduce
------------------
 - Migrate the instance
And observe /var/log/nova/compute.log and /var/log/libvirt.log
Actual results
--------------
/var/log/nova-conductor.log
2014-04-04 13:42:17.128 3294 ERROR oslo.messaging._drivers.common [-] ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\n incoming.message))\n', ' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\n return self._do_dispatch(endpoint, method, ctxt, args)\n', ' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\n result = getattr(endpoint, method)(ctxt, **new_args)\n', ' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner\n return func(*args, **kwargs)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 668, in migrate_server\n block_migration, disk_over_commit)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 769, in _live_migrate\n raise exception.MigrationError(reason=ex)\n', 'MigrationError: Migration error: Remote error: libvirtError XML error: CPU feature `wdt\' specified more than once\n[u\'Traceback (most recent call last):\\n\', u\' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\\n incoming.message))\\n\', u\' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\\n return self._do_dispatch(endpoint, method, ctxt, args)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\\n result = getattr(endpoint, method)(ctxt, **new_args)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped\\n payload)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n six.reraise(self.type_, self.value, self.tb)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped\\n return f(self, context, *args, **kw)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 272, in decorated_function\\n e, sys.exc_info())\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n six.reraise(self.type_, self.value, self.tb)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 259, in decorated_function\\n return function(self, context, *args, **kwargs)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 4159, in check_can_live_migrate_destination\\n block_migration, disk_over_commit)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4094, in check_can_live_migrate_destination\\n self._compare_cpu(source_cpu_info)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4236, in _compare_cpu\\n LOG.error(m, {\\\'ret\\\': ret, \\\'u\\\': u})\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n six.reraise(self.type_, self.value, self.tb)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4232, in _compare_cpu\\n ret = self._conn.compareCPU(cpu.to_xml(), 0)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit\\n result = proxy_call(self._autowrap, f, *args, **kwargs)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call\\n rv = execute(f,*args,**kwargs)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker\\n rv = meth(*args,**kwargs)\\n\', u\' File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 3191, in compareCPU\\n if ret == -1: raise libvirtError (\\\'virConnectCompareCPU() failed\\\', conn=self)\\n\', u""libvirtError: XML error: CPU feature `wdt\' specified more than once\\n""].\n']
2014-04-04 13:52:18.161 3295 ERROR nova.conductor.manager [req-471d2933-354a-4417-af50-c48399e19663 42fab7a8b7434bfc8473767c01e8378d b1cf6337c229491c96ad6e0a96e82979] Migration of instance 47d1fe7d-b812-4588-85eb-aa813267fc82 to host c2 unexpectedly failed.
/var/log/libvirtd.log
2014-03-27 18:23:17.141+0000: 2659: info : libvirt version: 1.2.2
2014-03-27 18:23:17.141+0000: 2659: error : virCPUDefParseXML:413 : XML error: CPU feature `wdt' specified more than once
Expected results
----------------
Successful migration
Additional info
----------------
Related with: https://bugs.launchpad.net/nova/+bug/1267191
On the file /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py, the list info['features'] have the duplicate feature."
1,"2013-09-10 07:48:36,767.767 5769 ERROR nova.virt.baremetal.deploy_helper [req-f649058f-8b8f-4392-a221-ca55a96178b0 None None] StdOut :
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)
Logged from file baremetal_deploy_helper.py, line 217

Program stdout can be arbitrary bytes; we need to convert it to unicode before trying to log it."
1,"Volumes cannot be attach to any LXC instances
since it's root device cannot be parsed properly.
This later causes a failure in device name generation in get_next_device_name(),
when attempting to generate a name for the attached volume.
The generated device name, will not be recognized by get_dev_prefix_for_disk_bus()
when trying to select a disk bus, nor libvirt will be able to attach the volume
with an unrecognized device name.
When creating a LXC instance, the /dev/nbd1 or /dev/loop0 devices will be saved as
instance root device in _create_domain()
Later, when attaching the volume, block_device.match_device will be called from compute_utils.get_next_device_name(),
The formed device will be named as /dev/na (for /dev/nbdX)
Which will not be recognized in blockinfo.get_dev_prefix_for_disk_bus()
Even if it will be recognized, libvirt wont be able to attach a volume named /dev/na"
0,"Base test case class has a fixture, that overrides CONF.lock_path value, which means that every test case will have CONF.lock_path set to its own temporary dir path. This makes interprocess locks unusable in unit tests, which is likely to break tests when they are run concurrently using testr (e.g. tests using MySQL/PostgreSQL might want to be run exclusively)."
1,"when detach a pci device from instance
the method _detach_pci_devices will check if pci device detached
> for hdev in [d for d in guest_config.devices
> if d.type == 'pci']:
guest_config.devices will have more device not only pci, like disk
in LibvirtConfigGuestDisk has no attribute type
  File ""/home/xiaoding/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 684, in test_detach_pci_devices
    conn._detach_pci_devices(FakeDomain(), pci_devices)
  File ""/home/xiaoding/nova/nova/virt/libvirt/driver.py"", line 2774, in _detach_pci_devices
    if d.type == 'pci']:
AttributeError: 'LibvirtConfigGuestDisk' object has no attribute 'type'
https://review.openstack.org/#/c/93383/"
0,RHEL6.5 has back ported vxlan support feature into its kernel 2.6 but check_vxlan_support still give NOT result.
0,https://review.openstack.org/#/c/56263 removes the need to explicitly call release_lease on this class. It should be removed.
0,"In neutron/auth.py, X_TENANT_ID and X_ROLE are used to extract context information.
They are now deprecated and X_PROJECT_ID and X_ROLES are recommended instead."
1,"This is to fix the log messges as per Jay's comment in https://review.openstack.org/#/c/51900/ - ""I think it is important that we be doing all LOG.* messages in a sentence format that ends with a '.' . This makes it easier to read through the messages when debugging."""
1,"In PLUMgrid plugin, the update_floatingip is missing id as shown above:
def update_floatingip(self, net_db, floating_ip):
        self.plumlib.update_floatingip(net_db, floating_ip, id)"
0,"The Tempest test ""tempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern"" fails on Hyper-V.
The cause is related to the fact that the root device is ""sda"" and not ""vda""."
0,"Today Nova does not pass a specific machine type in the libvirt XML configuration file, resulting in the use of a default machine type. In some cases the operator may want to use a non-default machine type. For example, with ARM by default the machine type is integratorcp, but users need to use the virt model or others such as vexpress-a15 with KVM"
0,"In Havana time, in the execute method of processutils the condition (os.geteuid() != 0) was added in line 130
    if run_as_root and os.geteuid() != 0:
        if not root_helper:
            raise NoRootWrapSpecified(
                message=('Command requested root, but did not specify a root '
                         'helper.'))
        cmd = shlex.split(root_helper) + list(cmd)
That provokes the next error in Windows:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp (stdout, stderr) = processutils.execute(*cmd, **kwargs)
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp File ""C:\Users\Pedro\dev\cinder\cinder\openstack\common\processutils.py"", line 130, in execute
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp if run_as_root and os.geteuid() != 0:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'module' object has no attribute 'geteuid'
This should be removed in order to this code be used in nova and cinder windows-based drivers"
1,"Updating router routes takes long time when increasing a number of routes.
The critical problem is that it is CPU bound task of neutron-server and neutron-server can not reply for other request.
I show below a measurement example of neutron-server's CPU usage.
Setting routes to a router. (0 to N)
100 routes: 1 sec
1000 routes: 5 sec
10000 routes: 51 sec
I found validation check of parameter is inefficient. The following example support it too.
No change but just specify same routes to a router. (N to N, DB is not changed)
100 routes: <1 sec
1000 routes: 4 sec
10000 routes: 52 sec
Remove routes from a router. (N to 0)
100 routes: <1 sec
1000 routes: 8 sec
10000 routes: 750 sec
I found handling of record deletion is bad. It takes N**2 order."
0,"Currently, on an unhandled exception, the error message logged through sys.excepthook is not very helpful.
Currently it prints only the exception_value.
https://github.com/openstack/oslo-incubator/blob/master/openstack/common/log.py#L396
Fix:
Make the log message print both the exception_type and exception_value of the unhandled exception.
PS: Currently the traceback is printed only when the VERBOSE is ON."
1,"Due to re-scheduler mechanism, when a user tries to
 create (in error) an instance using a volume
 which is already in use by another instance,
the error is correctly detected, but the recovery code
 will incorrectly affect the original instance.
Need to raise exception directly when the situation above occurred.
------------------------
------------------------
We can create VM1 with BDM-volumes (for example, one volume we called it 鈥淰ol-1鈥?.
But when the attached-volume (Vol-1..) involved in BDM parameters to create a new VM2, due to VM re-scheduler mechanism, the volume will change to attach on the new VM2 in Nova & Cinder, instead of raise an 鈥淚nvalidVolume鈥?exception of 鈥淰ol-1 is already attached on VM1鈥?
In actually, Vol-1 both attached on VM1 and VM2 on hypervisor. But when you operate Vol-1 on VM1, you can鈥檛 see any corresponding changes on VM2鈥?I reproduced it and wrote in the doc. Please check the attachment for details~
-------------------------
I checked on the Nova codes, the problem is caused by VM re-scheduler mechanism:
Now Nova will check the state of BDM-volumes from Cinder now [def _setup_block_device_mapping() in manager.py]. If any state is 鈥渋n-use鈥? this request will fail, and trigger VM re-scheduler.
According to existing processes in Nova, before VM re-scheduler, it will shutdown VM and detach all BDM-volumes in Cinder for rollback [def _shutdown_instance() in manager.py]. As the result, the state of Vol-1 will change from 鈥渋n-use鈥?to 鈥渁vailable鈥?in Cinder. But, there鈥檙e nothing detach-operations on the Nova side鈥?Therefore, after re-scheduler, it will pass the BDM-volumes checking in creating VM2 on the second time, and all VM1鈥檚 BDM-volumes (Vol-1) will be possessed by VM2 and are recorded in Nova & Cinder DB. But Vol-1 is still attached on VM1 on hypervisor, and will also attach on VM2 after VM creation success鈥?---------------
Moreover, the problem mentioned-above will occur when 鈥渄elete_on_termination鈥?of BDMs is 鈥淔alse鈥? If the flag is 鈥淭rue鈥? all BDM-volumes will be deleted in Cinder because the states are already changed from 鈥渋n-use鈥?to 鈥渁vailable鈥?before [def _cleanup_volumes() in manager.py].
(P.S. Success depends on the specific implementation of Cinder Driver)
Thanks~"
1,"When performing a nova snapshot, the vmware driver takes a VM snapshot and copies out the snapshotted disk. After the operation, the VM snapshot should be deleted."
0,"When the QPID broker is restarted, RPC servers attempt to re-connect. This re-connection process is not done correctly for fanout subscriptions - two subscriptions are established to the same fanout address.
This problem is compounded by the fix to bug#1178375 https://bugs.launchpad.net/oslo/+bug/1178375
With this bug fix, when topology version 2 is used, the reconnect attempt uses a malformed subscriber address.
For example, I have a simple RPC server script that attempts to service ""my-topic"". When it initially connects to the broker using topology-version 1, these are the subscriptions that are established:
(py27)[kgiusti@t530 work (master)]$ ./my-server.py --topology=1 --auto-delete server-02
Running server, name=server-02 exchange=my-exchange topic=my-topic namespace=my-namespace
Using QPID topology version 1
Enable auto-delete
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_489a3178fc704123b0e5e2fbee125247""}}
When I restart the qpid broker, the server reconnects using the following subscriptions
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
^^^^--- Note: subscribing twice to the same exclusive address! (Bad!)
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
When using topology=2, the failure case is a bit different. On reconnect, the fanout addresses are lacking proper topic names:
Recevr amq.topic/topic/openstack/my-topic ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/topic/openstack/my-topic.server-02 ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Note again - two subscriptions to fanout, and 'my-topic' is missing (it should be after that trailing /)
FYI - my test RPC server and client can be accessed here: https://github.com/kgiusti/oslo-messaging-clients"
1,"From: http://logs.openstack.org/31/105031/14/gate/gate-tempest-dsvm-full/c05b927/console.html
2014-08-05 02:54:01.131 | Log File Has Errors: n-cond
2014-08-05 02:54:01.132 | *** Not Whitelisted *** 2014-08-05 02:25:47.799 ERROR nova.quota [req-19feeaa2-e1d4-419b-a7bb-a19bb7000b1d AggregatesAdminTestJSON-2075387658 AggregatesAdminTestJSON-270189725] Failed to commit reservations [u'ceaa6ce7-db8d-4ba6-871a-b29c59f4a338', u'10d7550d-d791-44dd-8396-2fa6eaea7c20', u'e7a322e2-948d-45f7-892f-7ea4d9aa0e7c']
There are a number of errors happening in that file that arent whitelisted.
This one *seems* to be a possible cause of others.as there is then a number of InstanceNotFound errors."
1,"Start qpid, nova-api, nova-scheduler, and nova-conductor, and nova-compute.

There are orphan direct exchanges in qpid. Checked using qpid-config exchanges. The exchanges continue to grow, presumably, whenever nova-compute does a periodic update over AMQP.

Moreover, the direct and topic exchanges are by default durable which is a problem. We want the ability to turn on/off the durable option just like Rabbit options."
1,"The API samples of scheduler-hints contains ""hypervisor"" and ""near"" attributes like the following:
{
    ""server"" : {
        ""name"" : ""new-server-test"",
        ""image_ref"" : ""http://glance.openstack.example.com/openstack/images/70a599e0-31e7-49b7-b260-868f441e862b"",
        ""flavor_ref"" : ""http://openstack.example.com/openstack/flavors/1"",
        ""os-scheduler-hints:scheduler_hints"": {
            ""hypervisor"": ""xen"",
            ""near"": ""48e6a9f6-30af-47e0-bc04-acaed113bb4e""
        }
    }
}
However the attributes do not exist in the scheduler-hints parameter of ""create a server"" API."
0,"If the iscsi target port IP is set incorrectly, the driver will get a None value. Here the dirver should raise an exception.
The log:
2013-09-26 00:39:41.832 ERROR cinder.openstack.common.rpc.amqp [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Exception during message handling
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp **args)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 561, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp conn_info = self.driver.initialize_connection(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/huawei/huawei_hvs.py"", line 78, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp return self.common.initialize_connection_iscsi(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 518, in initialize_connection_iscsi
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp (iscsi_iqn, target_ip) = self._get_iscsi_params(connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1287, in _get_iscsi_params
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp target_iqn = self._get_tgt_iqn(target_ip)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1087, in _get_tgt_iqn
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp split_list = ip_info.split(""."")
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'split'
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp
2013-09-26 00:39:41.833 ERROR cinder.openstack.common.rpc.common [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Returning exception 'NoneType' object has no attribute 'split' to caller"
0,"The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type."
0,"From the initial commit of NEC plugin, network_id of packetfilters table is nullable=False [1],
but in folsom_initial db migration script nullable is set to True.

nullable=False is just a more strict constraint than nullable=True,
so NEC plugin works but it is better to be fixed in the migration.

I will add the migration to network_id to nullable=False both upgrade and downgrade
to make sure nullable=False in any revision."
1,"GET:
http://9.12.27.148:9292/v1/images/82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r
would return 500 instead of 400.
The root cause is as below:
2013-12-23 07:43:15.387 27806 INFO glance.wsgi.server [687bccf6-ed9e-4efa-988e-a05faafb6c4b 51336fdd98e449ef911f57ad3d03c818 d9f056c22dac4022bf2ebb1ed70fd25e] Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/wsgi.py"", line 384, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 571, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 368, in __call__
    response = req.get_response(self.application)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 620, in __call__
    request, **action_args)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 646, in dispatch
    return method(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/registry/api/v1/images.py"", line 312, in show
    image = self.db_api.image_get(req.context, id)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 315, in image_get
    force_show_deleted=force_show_deleted)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 343, in _image_get
    raise e
DataError: (DataError) ibm_db_dbi::DataError: Statement Execute Failed: [IBM][CLI Driver] CLI0109E String data right truncation. SQLSTATE=22001 SQLCODE=-99999 'SELECT images.created_at AS images_created_at, images.updated_at AS images_updated_at, images.deleted_at AS images_deleted_at, images.deleted AS images_deleted, images.id AS images_id, images.name AS images_name, images.disk_format AS images_disk_format, images.container_format AS images_container_format, images.size AS images_size, images.status AS images_status, images.is_public AS images_is_public, images.checksum AS images_checksum, images.min_disk AS images_min_disk, images.min_ram AS images_min_ram, images.owner AS images_owner, images.protected AS images_protected, image_properties_1.created_at AS image_properties_1_created_at, image_properties_1.updated_at AS image_properties_1_updated_at, image_properties_1.deleted_at AS image_properties_1_deleted_at, image_properties_1.deleted AS image_properties_1_deleted, image_properties_1.id AS image_properties_1_id, image_properties_1.image_id AS image_properties_1_image_id, image_properties_1.name AS image_properties_1_name, image_properties_1.""value"" AS image_properties_1_value, image_locations_1.created_at AS image_locations_1_created_at, image_locations_1.updated_at AS image_locations_1_updated_at, image_locations_1.deleted_at AS image_locations_1_deleted_at, image_locations_1.deleted AS image_locations_1_deleted, image_locations_1.id AS image_locations_1_id, image_locations_1.image_id AS image_locations_1_image_id, image_locations_1.""value"" AS image_locations_1_value, image_locations_1.meta_data AS image_locations_1_meta_data \nFROM images LEFT OUTER JOIN image_properties AS image_properties_1 ON images.id = image_properties_1.image_id LEFT OUTER JOIN image_locations AS image_locations_1 ON images.id = image_locations_1.image_id \nWHERE images.id = ?' ('82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r',)"
1,"https://review.openstack.org/#/c/78194/ changed tempest to clear image_ref for some BFV tests - in particular the test_volume_boot_pattern
This now results in a ""KeyError: 'disk_format'"" exception from Nova when using the XenAPI driver.
http://paste.openstack.org/show/73733/ is a nicer format of the below - but might disappear!
2014-03-18 11:20:07.475 ERROR nova.compute.manager [req-82096fe0-921a-4bc1-9c41-d0aafad4c923 TestVolumeBootPattern-581093620 TestVolumeBootPattern-1800543246] [instance: 2b047f24-675c-4921-8cf3-85584097f106] Error: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] Traceback (most recent call last):
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/compute/manager.py"", line 1306, in _build_instance
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] set_access_ip=set_access_ip)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/compute/manager.py"", line 394, in decorated_function
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] return function(self, context, *args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/compute/manager.py"", line 1708, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] LOG.exception(_('Instance failed to spawn'), instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/compute/manager.py"", line 1705, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/driver.py"", line 236, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] admin_password, network_info, block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 357, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] network_info, block_device_info, name_label, rescue)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 526, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] undo_mgr.rollback_and_reraise(msg=msg, instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/utils.py"", line 812, in rollback_and_reraise
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] self._rollback()
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 501, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] disk_image_type = determine_disk_image_type_step(undo_mgr)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 146, in inner
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] rv = f(*args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 414, in determine_disk_image_type_step
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] return vm_utils.determine_disk_image_type(image_meta)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] File ""/opt/stack/nova/nova/virt/xenapi/vm_utils.py"", line 1647, in determine_disk_image_type
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] disk_format = image_meta['disk_format']
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] KeyError: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]
I've confirmed that running without https://review.openstack.org/#/c/78194/ passes the tests (although there is a race condition, which is why test_volume_boot_pattern is disabled in the XenServer CI system) but will always fail with https://review.openstack.org/#/c/78194/ applied."
1,"When creating cold snapshots we first stop the instance, create a snapshot of that instance, extract the snapshot to a file, delete the snapshot and bring the instance back up.
If the instance is stopped, then there's no need to create a snapshot because there's no concurrent writer, so the snapshot can be extracted directly and save us from the two unnecessary steps (creation and deletion of a snapshot)."
0,"The method assertNotEquals has been deprecated.
In Python 3, a deprecated warning is raised when using assertNotEquals
therefore we should use assertNotEqual instead."
1,"This commit https://github.com/openstack/cinder/commit/beecd769af02ba5915be827d28a7b46d970e41b0 has changed the flow of volume creation, so Cinder does not create an iSCSI target anymore when a volume is created. Even so, the Windows Cinder driver tries to remove the according iSCSI target every time a volume is deleted, resulting into an error when the target has not been created. The fix consists in checking if the target exists before attempting to delete it.
Trace: http://paste.openstack.org/show/73912/"
0, fixed in oslo some time ago in Ib544be1485823f6c619312fdee5a04031f48bbb4. All direct and indirect (lockutils and rpc) usages of strong_store might be potentially affected.
1,"v2 api returns 200 with blank response (no image data) for download_image policy
If you have enabled download_image policy in policy.json to ""role:admin"" then it should return 403 error if user other admin role is calling image-download api.
Presently it is returning 200 with blank response (no image data). If you enable cache filter, then it returns 403 error correctly.
Steps to reproduce:
1. Ensure following flavor is set in glance-api.conf
   [paste-deploy]
   flavor = keystone+cachemanagement
2. Disable cache
   a. Open /etc/glance/glance-api-paste.ini file.
   b. Remove cahce from following sections.
     [pipeline:glance-api-caching]
     [pipeline:glance-api-cachemanagement]
     [pipeline:glance-api-keystone+caching]
     [pipeline:glance-api-keystone+cachemanagement]
     [pipeline:glance-api-trusted-auth+cachemanagement]
   c. Save and exit from file.
   d. Restart the g-api (glance-api) service.
3. Ensure that 'download_image' policy is set in policy.json
   ""download_image"": ""role:admin""
4. Download image using v2 api for role other than admin
   a. source openrc normal_user normal_user
   b. glance --os-image-api-version 2 image-download <image-id>
   Output:
   -------
   ''
   glance-api screen log:
   ----------------------
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] Traceback (most recent call last):
   File ""/usr/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 395, in handle_one_response
  for data in result:
   File ""/mnt/stack/glance/glance/notifier.py"", line 228, in get_data
  for chunk in self.image.get_data():
   File ""/mnt/stack/glance/glance/api/policy.py"", line 233, in get_data
  self.policy.enforce(self.context, 'download_image', {})
   File ""/mnt/stack/glance/glance/api/policy.py"", line 143, in enforce
  exception.Forbidden, action=action)
   File ""/mnt/stack/glance/glance/api/policy.py"", line 131, in _check
  return policy.check(rule, target, credentials, *args, **kwargs)
   File ""/mnt/stack/glance/glance/openstack/common/policy.py"", line 183, in check
  raise exc(*args, **kwargs)
 Forbidden: You are not authorized to complete this action.
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] 10.146.146.4 - - [05/Jun/2014 12:45:00] ""GET /v2/images/63826dea-e281-4ffe-821b-f598c747ba54/file HTTP/1.1"" 200 0 0.062499"
0,in H it was decided that the support would be removed in I
1,"step to reproduce:
 *execute ""glance-cache-manage"" command as fallows
  -""glance-cache-manage queue-image 1fde5cfe-10be-4f38-86e7-9e54710d34f2 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1""
result:
 *messeage is displayed as follows
  -Queue image 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1 for caching? [y/N]
expected result:
   If the number of argument isn't one, commad fails."
1,"revoke_cert may throw ProcessExecutionError.
https://github.com/openstack/nova/blob/master/nova/crypto.py#L164
This is bad pattern.
(See Policy of Exception Handling
https://blueprints.launchpad.net/openstack-qa/+spec/nova-exception-policy)"
1,"http://logs.openstack.org/74/128974/1/check/check-grenade-dsvm/6b7d6f3/logs/new/screen-n-api.txt.gz#_2014-10-21_07_12_30_741
https://review.openstack.org/#/c/127057/ caused the issue."
1,"Try get console as below:
curl -i http://cloudcontroller:8774/v3/servers/570a0058-3094-4201-b313-a337a984f773/consoles -X GET
Then get error in nova-api as below:
2013-09-10 14:59:46.107 ERROR nova.api.openstack.extensions [req-8f981cd0-96ef-4c95-b2b5-2d42a666deed admin admin] Unexpected exception in API method
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 98, in index
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions for console in consoles])
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 30, in _translate_keys
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions pool = cons['pool']
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/openstack/common/db/sqlalchemy/models.py"", line 59, in __getitem__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions return getattr(self, key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 168, in __get__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions return self.impl.get(instance_state(instance),dict_)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 453, in get
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions value = self.callable_(state, passive)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/strategies.py"", line 481, in _load_for_state
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions (mapperutil.state_str(state), self.key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions DetachedInstanceError: Parent instance <Console at 0x635d5d0> is not bound to a Session; lazy load operation of attribute 'pool' cannot proceed
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions"
1,"""revertResize/confirmResize"" server actions does not work for v2.1 API

Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
0,"I have a suspend vm with an attached volume, if I detached volume while instance is in suspend state it can't be resumed properly.
It happens with both Windows and Linux vm's.

LibVirt error:
2494: error : qemuMonitorIORead:502 : Unable to read from monitor: Connection reset by peer

Packets versioning in Ubuntu 12.04:
ii libvirt-bin 1.0.2-0ubuntu11.13.04.2~cloud0 programs for the libvirt library
ii libvirt-dev 1.0.2-0ubuntu11.13.04.2~cloud0 development files for the libvirt library
ii libvirt0 1.0.2-0ubuntu11.13.04.2~cloud0 library for interfacing with different virtualization systems
ii python-libvirt 1.0.2-0ubuntu11.13.04.2~cloud0 libvirt Python bindings
ii kvm 1:84+dfsg-0ubuntu16+1.0+noroms+0ubuntu14.10 dummy transitional package from kvm to qemu-kvm
ii qemu-common 1.0+noroms-0ubuntu14.10 qemu common functionality (bios, documentation, etc)
ii qemu-kvm 1.0+noroms-0ubuntu14.10 Full virtualization on i386 and amd64 hardware
ii nova-common 1:2013.1.2-0ubuntu1~cloud0 OpenStack Compute - common files
ii nova-compute 1:2013.1.2-0ubuntu1~cloud0 OpenStack Compute - compute node
ii nova-compute-kvm 1:2013.1.2-0ubuntu1~cloud0 OpenStack Compute - compute node (KVM)
ii python-nova 1:2013.1.2-0ubuntu1~cloud0 OpenStack Compute Python libraries
ii python-novaclient 1:2.13.0-0ubuntu1~cloud0 client library for OpenStack Compute API"
0,"In neutron/tests/unit/test_api_v2_resource.py,
there are tests which expects HTTP success response but expect_errors=True is specified.
There are no direct negative impact now, but it is better to specify expect_errors=False (default value) to avoid a confusion for test authors."
1," the message """"An unknown exception occurred."""" was obtained."""
1,"Neutron floatingip-delete ""id"" does not delete the associated ""fip_agent_gw_port""."
1,"code in type_tunnel.py type_vlan.py incorrectly throws exception class instead of exception class instance.

Some other places need to be fixed as well."
0,"This has been noticed before, it just came up again with comments in https://bugs.launchpad.net/neutron/+bug/1250168

[12:23] <dims> salv-orlando, for 1250168, where are the keystone calls being made from? (you indicated a large percent of api calls are keystone in the last comment)
[12:35] <salv-mobile> It looks like all calls in admin context trigger a POST to keystone as the admon token is not cachex
[12:35] <salv-mobile> Cached
[12:36] <salv-mobile> I think this was intentional even if I know do not recall the precise reason
[12:40] <dims> salv-mobile, which file? in python-neutronclient?
[12:41] <salv-mobile> I am afk i think it's either nova.network.neutronv2.api or nova.network.neutronv2
[12:41] <dims> salv-mobile, thx, will take a peek
[12:42] <salv-mobile> The latter would be __init_.py of course"
0,Misc typos in scheduler code (comments)
1,"For any invalid request format we should return 400, not 422, we should fix this case for v2 security_groups API. That is also good for sharing the unittest between v2.1 and v2.
To change the API that follow the rule: https://wiki.openstack.org/wiki/APIChangeGuidelines"
0,"2014-03-20 14:51:23.246 29247 ERROR neutron.openstack.common.rpc.amqp [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Exception during message handling
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp **args)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/common/agent_driver_base.py"", line 99, in get_logical_device
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp raise n_exc.Invalid(_('Expected active pool'))
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Invalid: Expected active pool
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp
2014-03-20 14:51:23.247 29247 ERROR neutron.openstack.common.rpc.common [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Returning exception Expected active pool to caller"
0,"Currently, there is no easy way to get the Swift server version. If I'm not wrong, the only way to do this is to run the following on the swift server/s.
python -c ""import swift; print swift.__version__""
Instead there should be some way for a client to request for the server version."
1,"Because resize and cold migrate share common code, some of the migrate code shows through in the error handling code in that when a valid host can't be found when performing a resize operation, the user sees:
""No valid host found for cold migrate""
This can be confusing, especially when a number of different actions can be going in parallel."
0,"supports_thin_provisioning fails on customised builds; RHEL for instance reports version as:

2.02.100(2)-RHEL6

2013-10-10 14:58:56.928 32460 TRACE cinder.service File ""/usr/lib/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 142, in supports_thin_provisioning
2013-10-10 14:58:56.928 32460 TRACE cinder.service version_tuple = tuple(map(int, version.split('.')))
2013-10-10 14:58:56.928 32460 TRACE cinder.service ValueError: invalid literal for int() with base 10: '100-RHEL6'"
0,Plugins using libvirt_ovs_bridge config are affected due to changes in nova's VIF plugging code. Fix port crud in the Cisco N1kv Neutron plugin by extending Port Bindings Extension.
0,"A dhcp resync can be triggered at a number of points, but the actual resync is done asynchronously by a helper thread. This means by the time the resync happens, it's hard to establish what actually caused it.
I've seen a number of problems in production systems that cause excessive resyncs. One is a ipv6/dnsmasq issue (rhbz#1077487) and another is db corruption with duplicate entries [1]. The resync triggers a whole lot of logs itself, so it becomes very unclear how to establish any causality.
What I propose is to keep track of what triggered the resync with some helpful information.
[1] The logs will contain output like ""DBDuplicateEntry
(IntegrityError) (1062, ""Duplicate entry
'6d799c6a-7a09-4c1e-bb63-7d30fd052c8a-d3e3ac5b-9962-428a-a9f8-6b2' for
key 'PRIMARY'"") ..."" in this case"
0,"Improve unit test coverage for ...
quantum/plugins/cisco/models/virt_phy_sw_v2 193 34 0 31 5 78%"
0,"""CreateVM_Task"", vm_folder_ref,
config=config_spec, pool=res_pool_ref)
specifies a vm_folder_ref that has no relationship to the datastore.
This may lead to VM construction and placement errors.
NOTE: code selects the 0th datacenter"
1,"This bug was fixed by https://bugs.launchpad.net/nova/+bug/1064524 previously but broken by version 46922068ac167f492dd303efb359d0c649d69118.

Instead of iterating the already ordered port list, the new code iterates the list from neutron and the result is random ordering."
1,"The os-interface:show method in the v2/v3 compute API is catching a NotFound(NovaException):
http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/contrib/attach_interfaces.py?id=2014.2.rc1#n67
But when using the neutronv2 API, if you get a port not found it's going to raise up a PortNotFoundClient(NeutronClientException), which won't be handled by the NotFound(NovaException) in the compute API since it's not the same type of exception.
http://git.openstack.org/cgit/openstack/nova/tree/nova/network/neutronv2/api.py?id=2014.2.rc1#n584
This bug has two parts:
1. The neutronv2 API show_port method needs to return nova exceptions, not neutron client exceptions.
2. The os-interfaces:show v2/v3 APIs need to handle the exceptions (404 is handled, but neutron can also raise Forbidden/Unauthorized which the compute API isn't handling)."
1,Currently script which loads metadeta definitions to database uses try except block with open(file) inside. There is better solution in Python for file streams - use 'with open(file) as...'. With this approach we will be sure that every resource is cleaned up when the code finishes running.
1,"VMware mine sweeper for Neutron (*) recently showed a 100% failure rate on tempest.api.compute.v3.servers.test_server_actions
Logs for two instances of these failures are available at [1] and [2]
The failure manifested as an instance unable to go active after a rebuild.
A bit of instrumentation and log analysis revealed no obvious error on the neutron side - and also that the instance was actually in ""running"" state even if its task state was ""rebuilding/spawning""
N-API logs [3] revealed that the instance spawn was timing out on a missed notification from neutron regarding VIF plug - however the same log showed such notification was received [4]
It turns out that, after rebuild, the instance network cache had still 'active': False for the instance's VIF, even if the status for the corresponding port was 'ACTIVE'. This happened because after the network-vif-plugged event was received, nothing triggered a refresh of the instance network info. For this reason, the VM, after a rebuild, kept waiting for an even which obviously was never sent from neutron.
While this manifested only on mine sweeper - this appears to be a nova bug - manifesting in vmware minesweeper only because of the way the plugin synchronizes with the backend for reporting the operational status of a port.
A simple solution for this problem would be to reload the instance network info cache when network-vif-plugged events are received by nova. (But as the reporter knows nothing about nova this might be a very bad idea as well)
[1] http://208.91.1.172/logs/neutron/98278/2/413209/testr_results.html
[2] http://208.91.1.172/logs/neutron/73234/34/413213/testr_results.html
[3] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=WARNING#_2014-06-06_01_46_36_219
[4] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-06-06_01_41_31_767
(*) runs libvirt/KVM + NSX"
0,"In nova.image.glance:
def get_remote_image_service(context, image_href):
    """"""Create an image_service and parse the id from the given image_href.
    The image_href param can be an href of the form
    'http://example.com:9292/v1/images/b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3',
    or just an id such as 'b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3'. If the
    image_href is a standalone id, then the default image service is returned.
    :param image_href: href that describes the location of an image
    :returns: a tuple of the form (image_service, image_id)
    """"""
    # Calling out to another service may take a while, so lets log this
    LOG.debug(_(""fetching image %s from glance"") % image_href)
    #NOTE(bcwaldon): If image_href doesn't look like a URI, assume its a
    # standalone image ID
    if '/' not in str(image_href):
        image_service = get_default_image_service()
        return image_service, image_href
    try:
        (image_id, glance_host, glance_port, use_ssl) = \
            _parse_image_ref(image_href)
        glance_client = GlanceClientWrapper(context=context,
                host=glance_host, port=glance_port, use_ssl=use_ssl)
    except ValueError:
        raise exception.InvalidImageRef(image_href=image_href)
    image_service = GlanceImageService(client=glance_client)
    return image_service, image_id
Clearly the LOG.debug() message above is incorrect. The method does not fetch an image at all. It just returns an ImageService object."
1,"A rescue of an image that is not linked clone will leave the rescue image disk in the original VM folder. The disk will not be cleaned up properly.
This leads to a number of problems:
1. usage of unnecessary disk space
2. additional rescues for the same VM may fail"
0,"For 'extended_ips'/'extended_ips_mac' extension, there are difference between V2 and V3 server show/index & server address index API response listed below-

'address' field of V2->V3 server API response-

""OS-EXT-IPS:type"" -> ""type""
""OS-EXT-IPS-MAC:mac_addr"" -> ""mac_addr""

Above attribute needs to be fixed in V2.1 to make it backward compatible with V2."
0,"Retype fails with

^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m host)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m return f(*args, **kwargs)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py"", line 1434, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m new_profile)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 263, in is_datastore_compliant
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m profile_id = self.get_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 59, in get_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m profile_id = self._vops.retrieve_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1361, in retrieve_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m for profile in self.get_all_profiles():
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1339, in get_all_profiles
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m profile_manager = pbm.service_content.profileManager
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mAttributeError: 'NoneType' object has no attribute 'service_content'"
0,"Using the libvirt driver on Juno RC2 code, trying to create an instance with unicode name:

""\uff21\uff22\uff23\u4e00\u4e01\u4e03\u00c7\u00e0\u00e2\uff71\uff72\uff73\u0414\u0444\u044d\u0628\u062a\u062b\u0905\u0907\u0909\u20ac\u00a5\u5642\u30bd\u5341\u8c79\u7af9\u6577""

Blows up:

http://paste.openstack.org/show/121560/

The libvirt config code shouldn't be casting values to str(), it should be using six.text_type."
1,"NoopQuotasDriver.get_settable_quotas() tries to call update() on non-existing dictionary entry. While NoopQuotasDriver is not really useful, we still want it to be working."
0,"Once https://bugs.launchpad.net/neutron/+bug/1290550 is closed, calling stopall on mock.patch from individual unit tests is no longer required. Calls to stopall in existing code should be removed to prevent duplication."
0,"The new cinder pools submission breaks any cinder operation that involves <host> as input parameter. I tested cinder manage/unmanage feature taking the latest code. cinder manage breaks if host not given in the form host@backend#pool. For eg the following breaks:

cinder manage --source-name vol1 openstack1@iscsi

but this succeeds

cinder manage --source-name vol1 openstack1@iscsi#pool1

The knowledge of pool is only limited to the backend and hence any operation that involves <host> as input should not be burdened to also include pool name with it. This should be looked into at high priority."
1,"Plugin: Cisco plugin with N1KV.
n1kv:profile_id is mandatory and hence the network and port creation dont go through if no n1kv:profile_id parameter is passed.
This raises a network binding/ port binding error.
2013-03-15 12:45:47 DEBUG [routes.middleware] Match dict: {'action': u'index', 'controller': wsgify(quantum.api.v2.resource.resource, RequestClass=<class 'quantum.api.v2.resource.Request'>), 'format': u'json'}
2013-03-15 12:45:47 ERROR [quantum.api.v2.resource] index failed
Traceback (most recent call last):
  File ""/opt/stack/quantum/quantum/api/v2/resource.py"", line 95, in resource
    result = method(request=request, **args)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 198, in index
    return self._items(request, True)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 168, in _items
    obj_list = obj_getter(request.context, **kwargs)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 427, in get_networks
    self._extend_network_dict_provider(context, net)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 159, in _extend_network_dict_provider
    network[provider.NETWORK_TYPE] = binding.network_type
AttributeError: 'NoneType' object has no attribute 'network_type'
2013-03-15 12:45:47 DEBUG [eventlet.wsgi.server] 10.0.2.15 - - [15/Mar/2013 12:45:47] ""GET //v2.0/networks.json?router%3Aexternal=True HTTP/1.1"" 500 215 0.052251"
1,"nova compute is crashing with the below error when nova compute is started

2014-10-01 14:50:26.854 ^[[00;32mDEBUG nova.virt.libvirt.driver [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mUpdating host stats^[[00m ^[[00;33mfrom (pid=9945) update_status /opt/stack/nova/nova/virt/libvirt/driver.py:6361^[[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 449, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 58, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 167, in _do_send
    waiter.switch(result)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 207, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 490, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 181, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1152, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5946, in update_available_resource
    nodenames = set(self.driver.get_available_nodes())
  File ""/opt/stack/nova/nova/virt/driver.py"", line 1237, in get_available_nodes
    stats = self.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5771, in get_host_stats
    return self.host_state.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 470, in host_state
    self._host_state = HostState(self)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6331, in __init__
    self.update_status()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6387, in update_status
    numa_topology = self.driver._get_host_numa_topology()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4828, in _get_host_numa_topology
    for cell in topology.cells])
TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'
2014-10-01 14:50:26.989 ^[[01;31mERROR nova.openstack.common.threadgroup [^[[00;36m-^[[01;31m] ^[[01;35m^[[01;31munsupported operand type(s) for /: 'NoneType' and 'int'^[[00m

Seems like the commit https://github.com/openstack/nova/commit/6a374f21495c12568e4754800574e6703a0e626f
is the cause."
1,"If the blockrebase(swap volume) fails, the status of the attached volume remains of detaching.
Tried Commit ID:b037993984229bb698050f20e8719b8c06ff2be3
1.Before Swap Volume
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | in-use | vol01 | 1 | None | False | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available | vol02 | 1 | None | False | |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
2.Swap volume running
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching | vol01 | 1 | None | False | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | attaching | vol02 | 1 | None | False | |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
3.An error occurs銆€in swap volume
For example, cancel a blockrebase job.
libvirtError: virDomainGetBlockJobInfo() failed
4.After an error occurs
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching | vol01 | 1 | None | False | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available | vol02 | 1 | None | False | |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+"
1,When nova compute restarts the running instances on the hypervisor are queried. None of the instances would be matched - this would prevent the instance states being in sync with the state in the database. See _destroy_evacuated_instances (https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L531)
0,The security groups db queries are loading extra data from the ports table that is unnecessarily hindering performance.
1,"In order to set up VPNaaS, a user needs to know his router's external IP (to configure it as endpoint).
PROBLEM : When a user is not admin, the external IP of a router is not visible:
source openrc demo demo
neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id | name | external_gateway_info |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| 2bd1f015-6c98-4861-a078-5a69256ca7b0 | router1 | {""network_id"": ""8ae6890d-5bb5-4f07-9059-77499628048c"", ""enable_snat"": true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e | | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""} |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
It's visible only as admin:
source openrc admin demo
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e | | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""} |
| fd56a686-480d-4ede-b021-010253c3de42 | | fa:16:3e:a5:d2:92 | {""subnet_id"": ""29f5737c-417f-4aa9-a95e-2bef3a04729e"", ""ip_address"": ""192.168.57.226""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
Since users need to know the external IP of their router in order to set up VPNaaS this is quite blocking because it requires users to be admin in order to use this feature. It's not an issue for a private cloud, but a big issue for public clouds."
1,"I'm implementing nvp network gateway to heat.
  https://blueprints.launchpad.net/heat/+spec/resource-type-nvp-network-gateway
Heat would check resource existence by resource-show after resource deleted.
However, show_network_gateway returns 500 instead of 404 when nvp network gateway isn't exist.
The result of this situation is that Heat is unable to delete nvp network gateway.
I think, neutron should return 404 when resource isn't exist.
Curl:
$ curl -i http://172.23.56.142:9696/v2.0/network-gateways/b5afd4a9-eb71-4af7-a082-8fc625a35b61 -X GET -H ""X-Auth-Token: 56c136ee847f476f9f0ba4c2ca78ae4b"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""User-Agent: python-neutronclient""
HTTP/1.1 500 Internal Server Error
Content-Type: application/json; charset=UTF-8
Content-Length: 88
Date: Mon, 18 Nov 2013 10:30:44 GMT
{""NeutronError"": ""Request Failed: internal server error while processing your request.""}
Log:
2013-11-18 18:18:03.315 25570 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 54012987ac014457b9a0a8bcc10928ae with project_id : ed684e101d3243a69db07e744acad6f2 and roles: admin _build_user_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-11-18 18:18:03.316 25570 DEBUG routes.middleware [-] Matched GET /network-gateways/3fe90063-9e96-45be-8989-335f582962be.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Route path: '/network-gateways/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>, 'id': u'3fe90063-9e96-45be-8989-335f582962be', 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-11-18 18:18:03.324 25570 ERROR neutron.api.v2.resource [-] show failed
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 290, in show
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource parent_id=parent_id),
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 258, in _item
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource obj = obj_getter(request.context, id, **kwargs)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1951, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource id, fields)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 248, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource gw_db = self._get_network_gateway(context, id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 133, in _get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource return self._get_by_id(context, NetworkGateway, gw_id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 145, in _get_by_id
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource return query.filter(model.id == id).one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2190, in one
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource raise orm_exc.NoResultFound(""No row was found for one()"")
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource NoResultFound: No row was found for one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_msg_id': u'3ce98f89ac844d2a8d38e0ae231ff447', u'_context_read_deleted': u'no', u'_reply_q': u'reply_70a7a3f33d8f4417b5225404d435d264', u'_context_tenant_id': None, u'args': {u'devices': [u'tap3fc478a3-07', u'tapfa59a846-8b', u'tapba4de5a8-85']}, u'namespace': None, u'_unique_id': u'44f59887e1084f9898cdcc57d3bcff78', u'_context_is_admin': True, u'version': u'1.1', u'_context_project_id': None, u'_context_timestamp': u'2013-11-14 10:51:59.225787', u'_context_user_id': None, u'method': u'security_group_rules_for_devices'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'is_admin': True, 'timestamp': u'2013-11-14 10:51:59.225787', 'project_id': None, 'read_deleted': u'no'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276"
1,"In ae6b7642e8d32ef5fa75cdcfe55be23c052fd547 we added a key manager with a static key.

This key manager (enabled by default) repeated logs the following WARNING message to the Cinder api.log file:

2013-10-07 15:10:17.714 553 WARNING cinder.keymgr.conf_key_mgr [-] This key manager is insecure and is not recommended for production deployments

-----

There are actually two issues here. Logging tons of warning messages by default is not ideal... and should be avoided, especially since at this time there is no ""production ready"" key manager implementation which an end user could configure."
1,"~/swift$ git log | head -n1
commit 429cb8b7453662e325d6120457638827527483cd
$ python -c 'import swift; print swift.__version__'
1.10.0.47.g429cb8b
Uploading a new object returns last-modified header
 $ curl -i -XPUT --data-binary '1' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8f69afe6ec99d' http://127.0.0.
1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 201 Created
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Content-Length: 0
Etag: c4ca4238a0b923820dcc509a6f75849b
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:35:45 GMT
Then try to GET the object with if-modified-since header value from last-modified returns 200, not 304
$ curl -i -H'if-modified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8
f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 200 OK
Content-Length: 1
Accept-Ranges: bytes
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Etag: c4ca4238a0b923820dcc509a6f75849b
X-Timestamp: 1383795345.50550
Content-Type: application/x-www-form-urlencoded
Date: Thu, 07 Nov 2013 03:36:15 GMT
This leads the unnecessary download for most cases including the GET request from most of web browsers
Moreover, a GET request if-unmodified-since value returns 412, not 200
$ curl -i -H'if-unmodified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381
a8f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 412 Precondition Failed
Content-Length: 92
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:36:30 GMT
<html><h1>Precondition Failed</h1><p>A precondition for this request was not met.</p></html>
For the ranged-GET request, the if-unmodified-since headers check the rest of contents are not modified and valid to continue the downloading, but fails always.
The reason the if-(un)modified-since header not working is last-modified header get the time value from x-timestamp value (floating type) and truncate it to integer value. So the last-modified header value always earlier than x-timestamp's one."
0,"The downloading code in xenserver requires an instance argument. However in practice this argument is not used for debugging or any other purpose. Downloading in fact can occur without an instance being involved at all. We should remove this unused argument.

This is in preparation to separate out the download code and leverage it for precaching blueprint work. This should be treated as a bug on it's own as it's a code refactor/cleanup work."
1,"With the following set in /etc/nova/nova.conf:
security_group_api=neutron
You can view security groups and rules that have been created in Neutron with nova secgroup-* commands.
If you create a Neutron Security Group rule with a different protocol though, nova secgroup-* calls fail with a 500 and a lot of stack trace in /var/log/nova/nova-api-os-compute.log:
<snip>
014-03-18 20:23:46.599 25278 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/security_groups.py"", line 215, in _format_security_group_rule
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack sg_rule['from_port'] = rule['from_port']
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack KeyError: 'from_port'
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack
2014-03-18 20:23:46.600 25278 INFO nova.api.openstack [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] http://10.240.0.100:8774/v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups returned with HTTP 500
2014-03-18 20:23:46.601 25278 INFO nova.osapi_compute.wsgi.server [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] 10.240.0.100 ""GET /v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups HTTP/1.1"" status: 500 len: 335 time: 0.0474379
To recreate:
# Test nova secgroup-list works
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id | Name | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default | default |
+--------------------------------------+-------------+-------------+
# Add rule with a different protocol
neutron security-group-rule-create --direction ingress --protocol 50 --remote-ip-prefix 0.0.0.0/0 ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field | Value |
+-------------------+--------------------------------------+
| direction | ingress |
| ethertype | IPv4 |
| id | d98e83cf-2aab-4eec-89ed-f9aa4d00d57b |
| port_range_max | |
| port_range_min | |
| protocol | 50 |
| remote_group_id | |
| remote_ip_prefix | 0.0.0.0/0 |
| security_group_id | ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 |
| tenant_id | 1be2c0f9589d4822856a9ac2e16f0406 |
+-------------------+--------------------------------------+
# Test
neutron security-group-list # works
nova secgroup-list # now errors
# Delete rule
neutron security-group-rule-delete d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
Deleted security_group_rule: d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
# Test nova again
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id | Name | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default | default |
+--------------------------------------+-------------+-------------+"
1,"When SIGHUP signal is send to nova-api service, it stops all the nova-api processes and while restarting the nova-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
2014-06-24 15:52:55.185 CRITICAL nova [-] AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova Traceback (most recent call last):
2014-06-24 15:52:55.185 TRACE nova File ""/usr/local/bin/nova-api"", line 10, in <module>
2014-06-24 15:52:55.185 TRACE nova sys.exit(main())
2014-06-24 15:52:55.185 TRACE nova File ""/opt/stack/nova/nova/cmd/api.py"", line 56, in main
2014-06-24 15:52:55.185 TRACE nova launcher.launch_service(server, workers=server.workers or 1)
2014-06-24 15:52:55.185 TRACE nova File ""/opt/stack/nova/nova/openstack/common/service.py"", line 340, in launch_service
2014-06-24 15:52:55.185 TRACE nova self._start_child(wrap)
2014-06-24 15:52:55.185 TRACE nova File ""/opt/stack/nova/nova/openstack/common/service.py"", line 324, in _start_child
2014-06-24 15:52:55.185 TRACE nova launcher.restart()
2014-06-24 15:52:55.185 TRACE nova File ""/opt/stack/nova/nova/openstack/common/service.py"", line 145, in restart
2014-06-24 15:52:55.185 TRACE nova self.services.restart()
2014-06-24 15:52:55.185 TRACE nova File ""/opt/stack/nova/nova/openstack/common/service.py"", line 478, in restart
2014-06-24 15:52:55.185 TRACE nova restart_service.reset()
2014-06-24 15:52:55.185 TRACE nova AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova
Steps to reproduce:
1. Run nova-api service as daemon.
2. Send SIGHUP signal to nova-api service
   kill -1 <parent_process_id_of_nova_api>"
0,MidonetInterfaceDriver should use the mm-ctl script to bind ports. Currently it uses Midonet API calls. This bug is to change MidonetInterfaceDriver to correctly use the mm-ctl script.
1,"If the server port is misconfigured, the plugin dies with an unfriendly error when starting the BigSwitch plugin.
e.g. ValueError: invalid literal for int() with base 10: 'a'"
0,"L3 agent uses deprecated root_helper option through self.conf.root_helper in neutron.agent.l3_agent.L3NATAgent._update_routing_table instead of using self.root_helper (result of neutron.agent.common.config.get_root_helper.
It implies if AGENT/root_helper option is configured and root_helper is not configured, extra routes are not pushed in routers."
0,"The call to l3plugin.disassociate_floatingips() trigggers several events
that could cause a timeout to occur trying to query the db for the port
therefore this patch changes the code to query first for the port."
1,"The instance will be ERROR when booting instance from volume, if the volume quota is not enough. And there is even no useful error message to show to the user. Following is the related nova-compute.log:
2014-07-27 17:56:19.372 17060 ERROR nova.compute.manager [req-4e876b97-be8a-486b-98e2-7d707266755d 98fa3fd418914a9288b5560e1bb6944e 5254621adfd949a9a3b975f68119e269] [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Instance failed block device setup
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Traceback (most recent call last):
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1690, in _prep_block_device
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] self.driver, self._await_block_device_map_created))
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 363, in attach_block_devices
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] map(_log_and_attach, block_device_mapping)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 361, in _log_and_attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] bdm.attach(*attach_args, **attach_kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 311, in attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] '', '', image_id=self.image_id)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/nova/volume/cinder.py"", line 303, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] item = cinderclient(context).volumes.create(size, **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/cinderclient/v1/volumes.py"", line 187, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] return self._create('/volumes', body, 'volume')
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/cinderclient/base.py"", line 153, in _create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] resp, body = self.api.client.post(url, body=body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 209, in post
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] return self._cs_request(url, 'POST', **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 173, in _cs_request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 156, in request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] raise exceptions.from_response(resp, body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] OverLimit: VolumeLimitExceeded: Maximum number of volumes allowed (10) exceeded (HTTP 413) (Request-ID: req-07dcc4c4-182f-4d73-b054-806f31cb7e71)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]
2014-07-27 17:56:19.693 17060 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return func(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher payload)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return f(self, context, *args, **kw)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 282, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher pass
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 268, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 335, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 311, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher e, sys.exc_info())
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 298, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2077, in run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher do_run_instance()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher return f(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2076, in do_run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1209, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher notify(""error"", fault=e) # notify that build failed
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1193, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher instance, image_meta, legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1347, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher LOG.exception(msg, instance=instance)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1304, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher context, instance, bdms)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1707, in _prep_block_device
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher raise exception.InvalidBDM()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher InvalidBDM: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher"
1,"In the current code, the configuration option 'gpfs_storage_pool' is assigned with default value of 'None'
And in the do_setup(), it line self._storage_pool = pool or system, it get assigned with ""None"" if the user has not specified any value.
Following is seen in the c-vol log.
2014-09-04 17:43:44.039 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:43:44.039 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 250, in init_host
2014-09-04 17:43:44.039 TRACE cinder.volume.manager self.driver.do_setup(ctxt)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:43:44.039 TRACE cinder.volume.manager return f(*args, **kwargs)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/gpfs.py"", line 341, in do_setup
2014-09-04 17:43:44.039 TRACE cinder.volume.manager raise exception.VolumeBackendAPIException(data=msg)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Invalid storage pool None specificed.
""gpfs_storage_pool"" option needs to assigned by default with ""system"" instead of ""None"""
0,"http://logs.openstack.org/22/112422/1/check/gate-nova-python26/621e0ae/console.html
This is probably a latent bug in the nova unit tests for postgresql in stable/havana, or it's due to slow nodes for the py26 jobs.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRVJST1I6ICBzb3VyY2UgZGF0YWJhc2UgXFxcInRlbXBsYXRlMVxcXCIgaXMgYmVpbmcgYWNjZXNzZWQgYnkgb3RoZXIgdXNlcnNcIiBBTkQgdGFnczpcImNvbnNvbGVcIiBBTkQgYnVpbGRfYnJhbmNoOlwic3RhYmxlL2hhdmFuYVwiIEFORCBidWlsZF9uYW1lOlwiZ2F0ZS1ub3ZhLXB5dGhvbjI2XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDc4NjA5ODg1MjMsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=
3 hits in 7 days, check queue only but multiple changes and all failures."
1,"I am getting with the latest version of the trunk:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
    if instance.task_state == task_states.DELETING:
AttributeError: 'dict' object has no attribute 'task_state'
Removing descriptor: 6
2014-03-04 13:59:14.304 ERROR nova.openstack.common.threadgroup [-] 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup x.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup return self.thread.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup return self._exit_event.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup return hubs.get_hub().switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup return self.greenlet.switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup result = function(*args, **kwargs)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup service.start()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/service.py"", line 180, in start
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup self.manager.init_host()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup self._init_instance(context, instance)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup if instance.task_state == task_states.DELETING:
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup AttributeError: 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup
I get this exception when creating a snapshot, put a breakpoint in the virt layer (in this case vmware_images/snapshot), stop nova-cpu, restart nova-cpu, delete the snapshot.
nova-cpu crashes."
1,Using log decorator from neutron.common.log causes creating new logger and not respecting logging level of decorated methods.
1,"I faced a bug in latest nova network:
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.0.0/24 --vlan 500
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.1.0/24 --vlan 501
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.2.0/24 --vlan 502
[root@host awasilyev]# nova-manage network list
id IPv4 IPv6 start address DNS1 DNS2 VlanID project uuid
64 172.26.0.0/24 None 172.26.0.3 8.8.4.4 None 500 None 225c8cbf-89bb-4171-b405-0047012a7803
65 172.26.1.0/24 None 172.26.1.3 8.8.4.4 None 502 None d461b285-d9c6-4a8c-ae39-5a657bb5926a
66 172.26.2.0/24 None 172.26.2.3 8.8.4.4 None 504 None 4c5a5d5b-24c8-4833-8bd0-6dcca11acb68
I try to create 3 networks, specifying exact vlan number for each network. But nova-manage creates networks using wrong vlan id's.
My previous openstack install (it was 3-4 monthes ago) does not have this bug."
1,"It should have been 324 and not 324

commit 243879f5c51fc45f03491bcb78765945ddf76be8 was bad"
0,"from nova/scheduler/filters/disk_filter.py

    def host_passes(self, host_state, filter_properties):
        """"""Filter based on disk usage.""""""
        instance_type = filter_properties.get('instance_type')
        requested_disk = 1024 * (instance_type['root_gb'] +
                                 instance_type['ephemeral_gb'])

This should take into account swap, which is stored in MB, as well."
1,"Found during review https://review.openstack.org/40296
There is a comma missing in the migration_for_plugins list
https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/4ca36cfc898c_nsx_router_mappings.py#L30"
1,"When update quota-class-set without the parameter ""quota_class_set"",the server throws a 500 error:
 ""The server has either erred or is incapable of performing the requested operation.""
Because the server doesn't check whether the parameter ""quota_class_set"" is in request body,the KeyError is not catched.
   for key in body['quota_class_set'].keys():
       ......
I think we should catch the KeyError and transfer the KeyError to 400(HTTPBadRequest) instead of 500.
Update quota-set has the same problem."
1,"    -container-5.2..com: May 22 22:49:25 container-server ERROR __ll__ error with DELETE /d84/58775/AUTH_benchmark/bench_000078/tiny_020610 :
    Traceback (most recent ll last):
      File ""/opt//lib/python2.7/site-packages/swift/container/server.py"", line 474, in __ll__
        res = method(req)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 1458, in wrapped
        return func(*a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 480, in _timing_stats
        ctrl.logger.timing_since(method + '.timing', start_time)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 610, in wrapped
        return func(self.logger.statsd_client, *a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 464, in timing_since
        sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 460, in timing
        return self._send(metric, timing_ms, 'ms', sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 445, in _send
        return sock.sendto('|'.join(parts), self._target)
      File ""/opt//lib/python2.7/site-packages/eventlet/greenio.py"", line 290, in sendto
        return self.fd.sendto(*args)
    error: [Errno 1] Operation not permitted (txn: tx60ec62fa962d4d96ba5dace5d81eede2)
Just need to audit the error handling, I think this happened under high load and the network was saturated maybe unable to send new udp packets or something..."
1,"When attaching volumes from NFS or GlusterFS backends, nova mounts the share in a temporary directory, this is reused when attaching another volume from the same share so there is no need to mount it several times.
On the other hand, when disconnecting those volumes nova doesn't even try to unmount the share which may remain mounted and unused there. To clean up after detach, nova could at least try to umount the shares."
1,"We assign VNC ports to VM instances with the following method:
def _get_vnc_port(vm_ref):
    """"""Return VNC port for an VM.""""""
    vm_id = int(vm_ref.value.replace('vm-', ''))
    port = CONF.vmware.vnc_port + vm_id % CONF.vmware.vnc_port_total
    return port
the vm_id is a simple counter in vSphere which increments fast and there is a chance to get the same port number if the vm_ids are equal modulo vnc_port_total (10000 by default).
A report was received that if the port number is reused you may get access to the VNC console of another tenant. We need to fix the implementation to always choose a port number which is not taken or report an error if there are no free ports available."
1," this config option was accidentally dropped in https://review.openstack.org/#/c/67657/20/cinder/service.py"""
1,"In migration folsom_initial in cisco_upgrade function table nexusport_bindings create column vlan_id with incorrect type Integer(255). It causes the following exception:
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 87, in upgrade
    upgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 455, in upgrade_cisco
    sa.Column('vlan_id', sa.Integer(255)),
TypeError: object() takes no parameters"
1,"Test Steps:
1. Run Cinder CI with ibm storwize storage, it was found much more ERRORs than before:
10:11:45 *** Not Whitelisted *** 2014-09-04 14:49:54.821 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 438e620d-8b04-49c0-9968-820c2bf8590f
10:11:45 *** Not Whitelisted *** 2014-09-04 14:50:54.769 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 3c9e9a38-e606-47f7-bbea-c3e84205bd79
10:11:45 *** Not Whitelisted *** 2014-09-04 14:52:54.799 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume fedb9af6-d7dc-4f0d-ad7b-eb28127ac507
2. Manually Create volumes without replication on IBM storwize v7000u, the volume creation successfully.
3. Check c-vol.log, it was found the below logs every 1 minutes:
2014-09-04 17:07:11.515 ERROR cinder.volume.manager [-] Error checking replication status for volume d2ffa93d-4140-4f1e-bcb7-c70af51781a3
2014-09-04 17:07:11.515 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.515 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager ctxt, vol)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.515 TRACE cinder.volume.manager return f(*args, **kwargs)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.515 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume d9791267-bb7a-448c-938e-c98dff38bcf5
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume fedd3b21-d60e-497e-b309-b125d81b1bcd
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
3. Expected result:
There is no replication status update Error for those volumes without replication."
1,"when there is a Datacenter in the vCenter with no datastore associated with it, nova boot fails even though there are data-centers configured properly.

The log error trace

Error from last host: devstack (node domain-c162(Demo-1)): [u'Traceback (most recent call last):\n', u' File ""/opt/stack/nova/nova/compute/manager.py"", line 1322, in _build_instance\n set_access_ip=set_access_ip)\n', u' File ""/opt/stack/nova/nova/compute/manager.py"", line 399, in decorated_function\n return function(self, context, *args, **kwargs)\n', u' File ""/opt/stack/nova/nova/compute/manager.py"", line 1734, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', u' File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__\n six.reraise(self.type_, self.value, self.tb)\n', u' File ""/opt/stack/nova/nova/compute/manager.py"", line 1731, in _spawn\n block_device_info)\n', u' File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 619, in spawn\n admin_password, network_info, block_device_info)\n', u' File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 211, in spawn\n dc_info = self.get_datacenter_ref_and_name(data_store_ref)\n', u' File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1715, in get_datacenter_ref_and_name\n self._update_datacenter_cache_from_objects(dcs)\n', u' File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1693, in _update_datacenter_cache_from_objects\n datastore_refs = p.val.ManagedObjectReference\n', u""AttributeError: 'Text' object has no attribute 'ManagedObjectReference'\n""]
2014-04-04 03:05:41.629 WARNING nova.scheduler.driver [req-cc690e5a-2bf3-4566-a697-30ca882df815 nova service] [instance: f0abb23a-943a-475d-ac63-69d2563362cb] Setting instance to ERROR state."
0,"We have 14 hits of tests using mock.assert_not_called() which is not a real method for mocks:
nova
nova
tests
api
openstack
test_common.py
395: href_link_mock.assert_not_called()
compute
test_compute_utils.py (2 matches)
764: mock_log.warning.assert_not_called()
775: mock_log.warning.assert_not_called()
image
test_glance.py (10 matches)
548: img.assert_not_called()
556: img.assert_not_called()
690: trans_from_mock.assert_not_called()
708: is_avail_mock.assert_not_called()
709: trans_from_mock.assert_not_called()
754: trans_from_mock.assert_not_called()
791: is_avail_mock.assert_not_called()
792: trans_from_mock.assert_not_called()
848: trans_from_mock.assert_not_called()
920: trans_from_mock.assert_not_called()
virt
baremetal
test_nova_baremetal_deploy_helper.py
339: self.m_mp.assert_not_called()
https://code.google.com/p/mock/issues/detail?id=159
We should be using self.assertFalse(mock.called) instead."
1,"Filter scheduler and create_volume flow both pass request_specs and filter_properties as optional arguments to create_volume in volume_rpcapi. This does not fit how the call is defined.

Also, the create_volume call in the migration code in manager.py should not allow rescheduling."
1,"db.sqlalchemy.models.ComputeNodeStat.stats field is not properly named: since it is a relation that maps to db.sqlalchemy.models.ComputeNode, it should be named 'compute_node', not 'stats'. Besides, the call to the relationship() method lacks the foreign_keys parameter."
0,"In the BrcdFCSanLookupService, when initialize. The ssh client should be allowed to load customized host key file instead of only load from the OS host key file. Also for the host key missing policy, the code should also allow to be customized by parsing the kwargs instead of hard-code the ""missing policy"". The ""MIssing Policy"" will not stop the man in the middle attack if the known_hosts is not a match, it should allow customized policy being configured in different scenarios to fit the security need."
0,"If we modify test_xiv_ds8k.py properly,
VolumeNotFoundForInstance class can be removed."
1,"First use vcenter driver to spawn some instances to one of the datastores that esxi is binding to. Later this datastore became unavailable due to certain reason(power off or network problem). Then when restart nova-compute, found that compute service will exit with errors. This will openstack compute not usable.
2014-07-03 01:38:13.961 3634 DEBUG nova.compute.manager [req-11bc0618-8696-464d-8820-7565db8f44c3 None None] [instance: 9428cf95-5
37f-48f6-b79e-faa981f6066d] NV-AC7AA80 Checking state _get_power_state /usr/lib/python2.6/site-packages/nova/compute/manager.py:10
54
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1037, in init_host
    self._init_instance(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 865, in _init_instance
    try_reboot, reboot_type = self._retry_reboot(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 963, in _retry_reboot
    current_power_state = self._get_power_state(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1056, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/driver.py"", line 862, in get_info
    return _vmops.get_info(instance)
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/vmops.py"", line 1376, in get_info
    max_mem = int(query['summary.config.memorySizeMB']) * 1024
KeyError: 'summary.config.memorySizeMB'"
0,"The ceilometer central agent pull and pubulish floatingip samples or other types of samples .but it cannot get valid samples of floatingip.
The reason is ceilometer floatingip poster call nova API ""list"" metod of nova.api.openstack.compute.contrib.floating_ips.FloatingIPController, this API get floatingips filtered by context.project_id.

The current context.project_id is the id of tenant ""service"".So,the result is {""floatingips"": []}

the logs of nova-api-os-compute is:

http://paste.openstack.org/show/55285/

Here,ceilometer invoke novaclient to list floatingips,and novaclient call nova API,then,the nova API will call nova network API or neutron API with:
    client.list_floatingips(tenant_id=project_id)['floatingips']

Novaclient can not list other tenant's floatingip but only the tenant of current context.

So, I think we should modify the nova API with adding a parameter like ""all_tenant"" which accessed by admin role.

This should be confirmed?"
1,"We have developed an extended Nova API, the API query disks at first, then add a disk to an instance.
After querying, if disk has non-english disk name, unicode will be converted to str in nova/api/openstack/wsgi.py line 451
""node = doc.createTextNode(str(data))"", then unicode encoding error exists."
1,"We recently landed a patch to make the metadata API's base.py use the FixedIP object for address lookups.

This causes problems when using the metadata API with neutron because the Nova FixedIP object class does not yet appear to support neutron.

When using the metadata server today (with Neutron) you'll see the following:

Apr 14 15:36:40 localhost nova-api[3373]: 2014-04-14 15:36:40.035 3505 ERROR nova.api.metadata.handler [-] Failed to get metadata for ip: 172.19.0.5"
1,"When an HP 3par host in host set, the cinder volume detach will hang. Looking at the log files, the exception shows the following exception after deleting all the vluns, the host entry is a pre-existing one since the host storage is using the same 3par system:
cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
in master/cinder/volume/drivers/san/hp/hp_3par_common.py
def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self._get_vlun(volume_name, hostname)
        if vlun is not None:
            # VLUN Type of MATCHED_SET 4 requires the port to be provided
            if self.VLUN_TYPE_MATCHED_SET == vlun['type']:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname,
                                       vlun['portPos'])
            else:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        try:
            self._delete_3par_host(hostname)
            self._remove_hosts_naming_dict_host(hostname)
 >> except hpexceptions.HTTPConflict as ex:
 >> # host will only be removed after all vluns
 >> # have been removed
 >> if 'has exported VLUN' in ex.get_description():
 >> pass
 >> else:
 >> raise
Yes, the problem occurred in Havana. I believe the problem is in Icehouse as well, but I have not tested it yet.
Log info:
ccontrol': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.698 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.699 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/hosts/kvm1-4 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.866 22459 DEBUG hp3parclient.http [-] RESP:{'connection': 'close',
 'content-type': 'application/json',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'server': 'hp3par-wsapi',
 'status': '409'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.867 22459 DEBUG hp3parclient.http [-] RESP BODY:{""code"":77,""desc"":""host is a member of a set""}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.868 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/credentials/272d1-c810c89243580a0047fec3b7c713b650-a1d66353 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.887 22459 DEBUG hp3parclient.http [-] RESP:{'cache-control': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.888 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.889 22459 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Disconnect from 3PAR client_logout /usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py:201
2014-05-02 17:31:23.890 22459 DEBUG cinder.openstack.common.lockutils [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Released file lock ""3par"" at /var/lib/cinder/tmp/cinder-3par for method ""terminate_connection""... inner /usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py:239
2014-05-02 17:31:23.891 22459 ERROR cinder.openstack.common.rpc.amqp [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Exception during message handling
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp **args)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 658, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp self.driver.terminate_connection(volume_ref, connector, force=force)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp retval = f(*args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 233, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp wwn=connector['wwpns'])
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1099, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp self.delete_vlun(volume, hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 546, in delete_vlun
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp self._delete_3par_host(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 408, in _delete_3par_host
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp self.client.deleteHost(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/hp3parclient/client.py"", line 388, in deleteHost
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp reponse, body = self.http.delete('/hosts/%s' % name)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 327, in delete
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp return self._cs_request(url, 'DELETE', **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 231, in _cs_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 205, in _time_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp resp, body = self.request(url, method, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 199, in request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp raise exceptions.from_response(resp, body)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp
2014-05-02 17:31:23.897 22459 ERROR cinder.openstack.common.rpc.common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Returning exception Conflict (HTTP 409) 77 - host is a member of a set to caller"
0,NSX: newly created ports status should be DOWN
1,"Account-reaper works at account-server with the first account replica, and reaps accounts with ""deleted"" status.
When swift fails to delete some account replicas, account-replicator doesn't replicate ""deleted"" status, but only *_timestamp.
So when swift fails to delete the first account replica, account-reaper will never reap it because its first replica will never have ""deleted"" status.
(If replica count is set as 3, swift returns 204, success return code to delete request in this situation)"
0,"Mellanox Neutron agent is configuredby default to contact Eswitch Daemon using port 5001 which is used by keystone.
It should be changed to use port 60001."
1,"In DB API code we have a notion of 'public' and 'private' methods. The former are conceptually executed within a *single* DB transaction and the latter can either create a new transaction or participate in the existing one. The whole point is to be able to roll back the results of DB API methods easily and be able to retry method calls on connection failures. We had a bp (https://blueprints.launchpad.net/nova/+spec/db-session-cleanup) in which all DB API have been re-factored to maintain these properties.
instance_create() is one of the methods that currently violates the rules of 'public' DB API methods and creates a concurrent transaction implicitly."
0,Currently the metering test case includes setUp code as well as the tests themselves. Splitting up the test case class into two allows other test cases to inherit the metering plugin class without inherting (And implicitly re-running) the tests themselves.
0,"Need to remove unnecessary 'context' parameter from a couple of methods in
neutron.services.loadbalancer.agent.agent_device_driver.AgentDeviceDriver"
1,"I've found that port delete fails if there are a number of port delete/create operations happening at once.
It seems to depend on database load but I've seen as few as about 8 parallel port deletes begin to timeout waiting for the appropriate locks inside of the _recycle_ip method in neutron/db/db_base_plugin_v2.py."
1,"Prior to a rebuild, nova issues a compute.instance.exists notification to capture usage for the period between the beginning of the day and the rebuild.
It should have the info for the instance prior to the rebuild, so if you are rebuilding from Centos 6.0 to Ubuntu 10.04, it should list a os_distribution of centos and an os_version of 6.0 Instead it's listing a distribution of Ubuntu and an os_version of 10.04 (i.e. the info for after the rebuild)."
1,The Big Switch full topology synchronization function isn't including all of the information about a network. It's only including the ports and floating IP addresses. The subnets are missing as well as the tenant of the network.
1,"When VMwareVCDriver is being used, detaching a volume will fail. The following error shows up in the n-cpu log:
Traceback (most recent call last):
 File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
   **args)
 File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
   result = getattr(proxyobj, method)(ctxt, **kwargs)
 File ""/opt/stack/nova/nova/exception.py"", line 89, in wrapped
   payload)
 File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
   return f(self, context, *args, **kw)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
   pass
 File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
   e, sys.exc_info())
 File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3720, in detach_volume
   self._detach_volume(context, instance, bdm)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3692, in _detach_volume
   self.volume_api.roll_detaching(context, volume_id)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3685, in _detach_volume
   encryption=encryption)
ypeError: detach_volume() got an unexpected keyword argument 'encryption'
Full log of detach operation here:
http://paste.openstack.org/show/47179/
The solution is to update the VMwareVCDriver method ""detach_volume"" to take an ""encryption"" argument."
1,"In Icehouse we marked the v2 API XML support as deprecated. The log message says it *will* be removed, but should be updated to be more accurate and say *may* be removed, pending finalizing the discussion around it."
1,"Requests for many resources in Glance v2 will return a 404 if the request is using an unsupported HTTP verb for that resource. For example, the /v2/images resource does exist but a 404 is returned when attempting a DELETE on that resource. Instead, this should return an HTTP 405 MethodNotAllowed response."
0,"How to reproduce:
  See the test cases in the following patch."
1,netaddr raises ValueError in 0.7.10 and AddrFormatError in 0.7.11
1,"Image snapshot through the VC cluster driver may fail if, within the datacenter containing the cluster managed by the driver, there are one or more hosts in maintenance mode with access to the datastore containing the disk image snapshot.
A sign that this situation has occurred is the appearance in the nova compute log of an error similar to the following:
2013-08-02 07:10:30.036 WARNING nova.virt.vmwareapi.driver [-] Task [DeleteVirtualDisk_Task] (returnval){
value = ""task-228""
_type = ""Task""
} status: error The operation is not allowed in the current state.
What this means is that even if all hosts in cluster are running fine in normal mode, a host outside of the cluster going into maintenance mode may
lead to snapshot failure.
The root cause of the problem is due to an issue in VC's handler of the VirtualDiskManager.DeleteVirtualDisk_Task API, which may incorrectly pick a host in maintenance mode to service the disk deletion even though such an operation will be rejected by the host under maintenance."
0,"Everything in module py3kcompat is ready in six > 1.4.0, we don't need this module now . It was removed from oslo-incubator recently, see https://review.openstack.org/#/c/71591/. This make us don't need maintain this module any more, use six directly."
1,"If DevStack is configured for the Cisco Nexus plugin with the latest DevStack, the following
infinite recursion error is observed:
Exception RuntimeError: 'maximum recursion depth exceeded' in <bound method
ConnectionContext.__del__ of <neutron.openstack.common.rpc.amqp.ConnectionContext
object at 0x403a3d0>> ignored
An investigation shows that this failure triggered when the DB base plugin's
_is_native_pagination_supported method is called. The infinite recursion begins
when this exception is raised:
             raise AttributeError(
                 _(""'%(model)s' object has no attribute '%(name)s'"") %
                {'model': self._model, 'name': name})
in the PluginV2.__getattr__ method in
neutron/plugins/cisco/network_plugin.py. The problem is that self._model
object is being % mod'd as a string in the unicode message, and this
causes many levels of recursion into deepcopy (a deepcopy for all objects
embedded in this object)."
1,"This is weird - Ironic has used the mac from a different node (which quite naturally leads to failures to boot!)
nova list | grep spawn
| 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | ci-overcloud-NovaCompute3-zmkjp5aa6vgf | BUILD | spawning | NOSTATE | ctlplane=10.10.16.137 |
 nova show 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | grep hyperv
 | OS-EXT-SRV-ATTR:hypervisor_hostname | b07295ee-1c09-484c-9447-10b9efee340c |
 neutron port-list | grep 137
 | 272f2413-0309-4e8b-9a6d-9cb6fdbe978d | | 78:e7:d1:23:90:0d | {""subnet_id"": ""a6ddb35e-305e-40f1-9450-7befc8e1af47"", ""ip_address"": ""10.10.16.137""} |
ironic node-show b07295ee-1c09-484c-9447-10b9efee340c | grep wait
 | provision_state | wait call-back |
ironic port-list | grep 78:e7:d1:23:90:0d # from neutron
| 33ab97c0-3de9-458a-afb7-8252a981b37a | 78:e7:d1:23:90:0d |
ironic port-show 33ab97c0-3de9-458a-afb7-8252a981
+------------+-----------------------------------------------------------+
| Property | Value |
+------------+-----------------------------------------------------------+
| node_uuid | 69dc8c40-dd79-4ed6-83a9-374dcb18c39b | # Ruh-roh, wrong node!
| uuid | 33ab97c0-3de9-458a-afb7-8252a981b37a |
| extra | {u'vif_port_id': u'aad5ee6b-52a3-4f8b-8029-7b8f40e7b54e'} |
| created_at | 2014-07-08T23:09:16+00:00 |
| updated_at | 2014-07-16T01:23:23+00:00 |
| address | 78:e7:d1:23:90:0d |
+------------+-----------------------------------------------------------+
ironic port-list | grep 78:e7:d1:23:9b:1d # This is the MAC my hardware list says the node should have
| caba5b36-f518-43f2-84ed-0bc516cc89df | 78:e7:d1:23:9b:1d |
# ironic port-show caba5b36-f518-43f2-84ed-0bc516cc
+------------+-----------------------------------------------------------+
| Property | Value |
+------------+-----------------------------------------------------------+
| node_uuid | b07295ee-1c09-484c-9447-10b9efee340c | # and tada right node
| uuid | caba5b36-f518-43f2-84ed-0bc516cc89df |
| extra | {u'vif_port_id': u'272f2413-0309-4e8b-9a6d-9cb6fdbe978d'} |
| created_at | 2014-07-08T23:08:26+00:00 |
| updated_at | 2014-07-16T19:07:56+00:00 |
| address | 78:e7:d1:23:9b:1d |
+------------+-----------------------------------------------------------+"
1,"Tested Environment
--------------------------
OS: Ubuntu 14.04 LST
Cinder NFS driver:
volume_driver=cinder.volume.drivers.nfs.NfsDriver

Error description
--------------------------
I used NFS as the cinder storage backend and successfully attached multiple volumes to nova instances.
However, when I tried to detach one them, I found following error on nova-compute.log.

-----------Error log------
2014-07-07 17:48:46.175 3195 ERROR nova.virt.libvirt.volume [req-a07d077f-2ad1-4558-91fa-ab1895ca4914 c8ac60023a794aed8cec8552110d5f12 fdd538eb5dbf48a98d08e6d64def73d7] Couldn't unmount the NFS share 172.23.58.245:/NFSThinLun2
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Traceback (most recent call last):
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/volume.py"", line 675, in disconnect_volume
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume utils.execute('umount', mount_path, run_as_root=True)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume File ""/usr/local/lib/python2.7/dist-packages/nova/utils.py"", line 164, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume return processutils.execute(*cmd, **kwargs)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume File ""/usr/local/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 193, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume cmd=' '.join(cmd))
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume ProcessExecutionError: Unexpected error while running command.
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Command: sudo nova-rootwrap /etc/nova/rootwrap.conf umount /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Exit code: 16
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stdout: ''
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stderr: 'umount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\numount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\n'
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume

-----------End of the Log--

For NFS volumes, every time you detach a volume, nova tries to umount the device path.
/nova/virt/libvirt/volume.py in
Line 632: class LibvirtNFSVolumeDriver(LibvirtBaseVolumeDriver):
Line 653: def disconnect_volume(self, connection_info, disk_dev):
Line 661: utils.execute('umount', mount_path, run_as_root=True)

This works when the device path is not busy.
If the device path is busy (or in use), it should output a message to log and continue.
The problem is, Instead of output a log message, it raise exception and that cause the above error.

I think the reason is, the ‘if’ statement at Line 663 fails to catch the device busy massage from the content of the exc.message. It looking for the ‘target is busy’ in the exc.message, but umount error code returns ‘device is busy’.
Therefore, current code skip the ‘if’ statement and run the ‘else’ and raise the exception.

How to reproduce
--------------------------
(1) Prepare a NFS share storage and set it as the storage backend of you cinder
(refer http://docs.openstack.org/grizzly/openstack-block-storage/admin/content/NFS-driver.html)
In cinder.conf
volume_driver=cinder.volume.drivers.nfs.NfsDriver
nfs_shares_config=<path to your nfs share list file>
(2) Create 2 empty volumes from cinder
(3) Create a nova instance and attach above 2 volumes
(4) Then, try to detach one of them.
You will get the error in nova-compute.log “Couldn't unmount the NFS share <your NFS mount path on nova-compute>”

Proposed Fix
--------------------------
I’m not sure about any other OSs who outputs the ‘target is busy’ in the umount error code.
Therefore, first fix comes to my mind is fix the ‘if’ statement to:
Before fix;
if 'target is busy' in exc.message:
After fix;
if 'device is busy' in exc.message:"
0,In the cfg module default=None is set as the default value. It's not necessary to set it again when defining config options.
1,"When you query the ec2 api, and it returns non-ascii data from the database, It throws something like this
2014-04-11 10:05:19.874 ERROR nova.api.ec2 [req-bc73aa28-0faa-437c-8fb1-e61188658c03 user project] Unexpected error raised: 'ascii' codec can't encode character u'\xe4' in position 1: ordinal not in range(128)
Tha cause seems to be the str() calls when calling xml.createTextNode in the /api/ec2/apirequest.py file.
This should porbably be made safe for non-ascii characters."
0,"Description of problem:
When you setting user_total_quota size is in bytes, unit should be in KB\MB\GB
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch"
0," the metering service plugin doesn't respect anymore the l3 agent binding. So instead of using the cast rpc method it uses the fanout_cast method."""
1,"my use case is as follows:
1.delete dhcp port
2.restart dhcp-agent
then, there will be two dhcp ports."
0,"After the changes in the block device mappings introduced for Havana, if we try to create an snapshot of a volume-backed instance the resulting image cannot be used to boot a new instance due to conflicts with the bootindex between the block_device_mapping stored in the image properties and the current image.
The steps to reproduce are:
$ glance image-create --name f20 --disk-format qcow2 --container-format bare --min-disk 2 --is-public True --min-ram 512 --copy-from http://download.fedoraproject.org/pub/fedora/linux/releases/test/20-Alpha/Images/x86_64/Fedora-x86_64-20-Alpha-20130918-sda.qcow2
$ cinder create --image-id <uuid of the new image> --display-name f20 2
$ nova boot --boot-volume <uuid of the new volume> --flavor m1.tiny test-instance
$ nova image-create test-instance test-snap
This will create an snapshot of the volume and an image in glance with a block_device_mapping containing the snapshot_id and all the other values from the original block_device_mapping (id, connection_info, instance_uuid, ...):
| Property 'block_device_mapping' | [{""instance_uuid"": ""989f03dc-2736-4884-ab66-97360102d804"", ""virtual_name"": null, ""no_device"": null, ""connection_info"": ""{\""driver_volume_type\"": \""iscsi\"", \""serial\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""data\"": {\""access_mode\"": \""rw\"", \""target_discovered\"": false, \""encrypted\"": false, \""qos_spec\"": null, \""device_path\"": \""/dev/disk/by-path/ip-192.168.122.2:3260-iscsi-iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7-lun-1\"", \""target_iqn\"": \""iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_portal\"": \""192.168.122.2:3260\"", \""volume_id\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_lun\"": 1, \""auth_password\"": \""wh5bWkAjKv7Dy6Ptt4nY\"", \""auth_username\"": \""oPbN9FzbEPQ3iFpPhv5d\"", \""auth_method\"": \""CHAP\""}}"", ""created_at"": ""2013-10-30T13:18:57.000000"", ""snapshot_id"": ""f6a25cc2-b3af-400b-9ef9-519d28239920"", ""updated_at"": ""2013-10-30T13:19:08.000000"", ""device_name"": ""/dev/vda"", ""deleted"": 0, ""volume_size"": null, ""volume_id"": null, ""id"": 3, ""deleted_at"": null, ""delete_on_termination"": false}] |
When we try latter to use this image to boot a new instance, the API won't let us because both, the device in the image bdm and the image (which is empty) are considered to be the boot device:
$ nova boot --image test-snap --flavor m1.nano test-instance2
ERROR: Block Device Mapping is Invalid: Boot sequence for the instance and image/block device mapping combination is not valid. (HTTP 400) (Request-ID: req-3e502a29-9cd3-4c0c-8ddc-a28d315d21ea)
If we check the internal flow we can see that nova considers the image to be the boot device even thought the image itself doesn't define any local disk but only a block_device_mapping pointing to the snapshot.
To be able to generate proper images from volume-backed instances we should:
 1. copy only the relevant keys from the original block_device_mapping to prevent duplicities in DB
 2. prevent nova from adding a new block device for the image if this one doesn't define any local disk"
1,"If multiple resources beyond the quotas , the headroom information about the resources beyond the quotas is incomplete(quota.py).
for example:
If the lens of path and content of the injected files exceed the limits, the above situation will appear.
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] overs: ['injected_file_content_bytes', 'injected_file_path_bytes']
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] headroom: {'injected_file_path_bytes': 255}"
1,"If the specific error message is required, we must set explanation='message' to initialize subclass of webob.exc.WSGIHTTPException(). There is some code specify the message to 'reason' incorrectly.
Ex.
OK: raise webob.exc.HTTPBadRequest(explanation=_('What?'))
WRONG: raise webob.exc.HTTPBadRequest(reason=_(""What's up?""))"
1,"The HP3PAR FC and iSCSI drivers are not synchronizing entry into methods ""extend_volume""."
1,"The ML2 plugin cannot raise NoResultFound exception because it does not use the correct sqlalchemy library.
'from sqlalchemy import exc as ...' instead of 'from sqlalchemy.orm import exc as ...'"
0,"psutil is currently version locked at psutil>=0.6.1,<1.0
These versions are not in PyPI, which is not liked by the new pip.
For further information see the comments in https://review.openstack.org/65209"
0,"neutron.services.vpn.service_drivers.ipsec has typo in its comment.
""""""Retuns the vpnservices on the host."""""" - should be """"""Returns the vpnservices on the host."""""""
1,"This change: https://review.openstack.org/42877 introduced the ability to pass os_type property to glance when we are doing a snapshot, but the change doesn't manage correctly the None value for this property.
The result of that is that we have a porperty called 'os_type': None associated with the snapshotted image.
That is a problem as this property is used when we create the backing file for the ephemeral disks.
From nova/virt/libvirt/driver/py:
 # Lookup the filesystem type if required
 os_type_with_default = instance['os_type']
 if not os_type_with_default:
     os_type_with_default = 'default'
and then we create the ephemeral file name using this rule:
fname = ""ephemeral_%s_%s"" % (ephemeral_gb, os_type_with_default)
Now consider an instance with a flavor with an ephemeral disk of 50 G and without the os_type defined, the resulting backing file will be:
""ephemeral_50_default""
At this point if we create a snapshot from that instance and we boot from it we will have another backing file created and called:
""ephemeral_50_None""
That is bad for at least two reasons:
1 performance issue: we need to create a new backing file which is not actually necessary
2 resources leaking: those backing files consume disk capacity on the host"
0,"Input parameter 'context' of _extend_snapshot method in cinder/api/extended_snapshot_attributes.py is not being used, so remove this 'context' parameter."
0,oslo.config.cfg.CONF.reset is added to cleanup in BaseTestCase.setUp(). No need for individual test classes to do it.
0,"I found we can not attach one volume to one instance(this instance is in hyperV compute node).
Test Env:
Cinder using Storwizedriver, and nova compute node is hyperV env.
When I want to attach one available volume to an instance which in hyper compute node, after ran ""nova volume-attach"" command, the volume status still is available(available-> attaching->availble), and from hyper compute node, in compute.log. I got the following error message:
2013-09-05 19:54:30.273 2204 AUDIT nova.compute.manager [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] [instance: 1b892ca0-3b5f-4792-a394-e9de661b429a] Attaching volume a053f7f3-8ffd-4392-b278-3791c2cd29bd to /dev/sdb
2013-09-05 19:54:30.305 2204 INFO nova.virt.hyperv.basevolumeutils [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] The ISCSI initiator name can't be found. Choosing the default one
2013-09-05 19:54:30.336 2204 INFO urllib3.connectionpool [-] Starting new HTTP connection (1): 9.123.137.71
2013-09-05 19:54:31.618 2204 ERROR nova.virt.hyperv.volumeops [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] Attach volume failed: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops Traceback (most recent call last):
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 113, in attach_volume
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops self._login_storage_target(connection_info)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 100, in _login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops target_portal)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeutilsv2.py"", line 53, in login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops TargetPortalPortNumber=target_port)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 431, in __call__
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops handle_com_error ()
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 241, in handle_com_error
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops raise klass (com_error=err)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops x_wmi: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops"
1,"This stracktrace has been observed:

http://logs.openstack.org/29/112229/5/gate/gate-tempest-dsvm-neutron-pg/db8ad58/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-07_15_49_00_061

Since change:

https://review.openstack.org/#/c/73234/

A unique constraint has been introduced. This prevents the race condition to cause multiple entries in the table, however concurrent changes can still occur, hence the DBDuplicateEntry can still occur. That said, it looks like the transaction is not cleaned up properly."
0,"Now that the DB healing https://review.openstack.org/96438 is merged, the DB migrations need unit tests."
0,"Sometimes after restarting the openvswitch-agent the veth pair that connects the physical bridge with the integration bridge doesn't come up correctly. (Which of cause disconnects any running VM instance from the network)
# /etc/init.d/openstack-neutron-openvswitch-agent restart
# ip addr show
[..]
83: phy-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether 3a:6c:d6:a4:1c:89 brd ff:ff:ff:ff:ff:ff
84: int-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether a2:12:2a:e5:b8:e4 brd ff:ff:ff:ff:ff:ff
[..]
I was able to reproduce this problem on openSUSE 12.3 and SLES 11. Ubuntu seems to be unaffected by this.
Doing a manual ""ip link set up dev <device>"" on both ends of the veth pair fixes the problem. (until another restarted might bring it back)
I think I was able to track this down to a race condition between udev (and its network rules) and the ip commands that the openvswitch-agent during startup. Among other things the agent does this during startup:
ip link delete int-br-fixed
ip link add int-br-fixed type veth peer name phy-br-fixed
ip link set int-br-fixed up
ip link set phy-br-fixed up
The ip link delete and ip link add command cause several udev events to be fired. However on my system the processing of the udev rules takes so long that the ""remove"" events are not completely processed before the ip link add command is started. Which causes the interface to be down after the above commands completed.
A possible fix for this is to call ""udevadm settle"" after the ip link delete call.
I will upload a draft patch for review shortly."
0,"Current DB migration script to populate N1kv tables is missing the ondelete=""CASCADE"" option in the ForeignKey constraint for network and port binding tables. This causes the network and port delete calls to fail with Integrity error. This occurs when the database is populated with db migration scripts.
super(N1kvNeutronPluginV2, self).delete_network(context, id)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 1013, in delete_network
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource context.session.delete(network)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource self.commit()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource self._prepare_impl()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource self.session.flush()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource raise exception.DBError(e)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource DBError: (IntegrityError) (1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`cisco_neutron`.`cisco_n1kv_network_bindings`, CONSTRAINT `cisco_n1kv_network_bindings_ibfk_1` FOREIGN KEY (`network_id`) REFERENCES `networks` (`id`))') 'DELETE FROM networks WHERE networks.id = %s' ('12fb857a-cb23-44f0-81a4-9cc8fc669de0',)"
1,One can instantiate subnet and router based on the template provided to inherit most if not all template properties. This is a very useful feature for all Nuage's current and future customers. extension is missed out and needs a fix.
1,An encountered exception in the router rescheduling loop will cause the loop to terminate and not be tried again. Retries should continue at the normal interval.
1,Method translate_struct of NaElement does not work properly in case of unique tags.
1,"RequestContext initialization failed in nova because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call nova api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown."
0,"For downgrade with postgres it is not enough to just drop_table if table contains enums. They should be removed separately after
deleting the table.
This problem takes place in migrations like 52ff27f7567a_support_for_vpnaas, 39cf3f799352_fwaas_havana_2_model, 569e98a8132b_metering, f489cf14a79c_lbaas_havana.
Also if some migration use the same enums in postgres work with
them should be organized differently. In migration 3c6e57a23db4_add_multiprovider if does not use create_type=False downgrade fails with this error http://paste.openstack.org/show/84486/"
0,"Current git version of nova does not fully support ipv6 nameservers despite being able to set them during subnet creation.
This patch adds this support in nova (git) and its interfaces.template. It is currently deployed and used in our infrastructure based on icehouse (Nova 2.17.0)."
1,"There's no catching of exceptions in _cleanup_volumes in compute/manager.py so a raised exception will short circuit cleaning up later volumes.
    def _cleanup_volumes(self, context, instance_uuid, bdms):
        for bdm in bdms:
            LOG.debug(""terminating bdm %s"", bdm,
                      instance_uuid=instance_uuid)
            if bdm.volume_id and bdm.delete_on_termination:
                self.volume_api.delete(context, bdm.volume_id)"
0,"In migrations 52ff27f7567a_support_for_vpnaas.py and 338d7508968c_vpnaas_peer_address_.py different class names are set: neutron.services.vpn.plugin.VPNDriverPlugin and neutron.services.vpn.plugin.VPNPlugin.
This cause the following exception:
neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head
No handlers could be found for logger ""neutron.common.legacy""
INFO [alembic.migration] Context impl MySQLImpl.
INFO [alembic.migration] Will assume non-transactional DDL.
INFO [alembic.migration] Running upgrade None -> folsom, folsom initial database
INFO [alembic.migration] Running upgrade folsom -> 2c4af419145b, l3_support
INFO [alembic.migration] Running upgrade 2c4af419145b -> 5a875d0e5c, ryu
INFO [alembic.migration] Running upgrade 5a875d0e5c -> 48b6f43f7471, DB support for service types
INFO [alembic.migration] Running upgrade 48b6f43f7471 -> 3cb5d900c5de, security_groups
INFO [alembic.migration] Running upgrade 3cb5d900c5de -> 1d76643bcec4, nvp_netbinding
INFO [alembic.migration] Running upgrade 1d76643bcec4 -> 2a6d0b51f4bb, cisco plugin cleanup
INFO [alembic.migration] Running upgrade 2a6d0b51f4bb -> 1b693c095aa3, Quota ext support added in Grizzly
INFO [alembic.migration] Running upgrade 1b693c095aa3 -> 1149d7de0cfa, initial port security
INFO [alembic.migration] Running upgrade 1149d7de0cfa -> 49332180ca96, ryu plugin update
INFO [alembic.migration] Running upgrade 49332180ca96 -> 38335592a0dc, nvp_portmap
INFO [alembic.migration] Running upgrade 38335592a0dc -> 54c2c487e913, 'DB support for load balancing service
INFO [alembic.migration] Running upgrade 54c2c487e913 -> 45680af419f9, nvp_qos
INFO [alembic.migration] Running upgrade 45680af419f9 -> 1c33fa3cd1a1, Support routing table configuration on Router
INFO [alembic.migration] Running upgrade 1c33fa3cd1a1 -> 363468ac592c, nvp_network_gw
INFO [alembic.migration] Running upgrade 363468ac592c -> 511471cc46b, Add agent management extension model support
INFO [alembic.migration] Running upgrade 511471cc46b -> 3b54bf9e29f7, NEC plugin sharednet
INFO [alembic.migration] Running upgrade 3b54bf9e29f7 -> 4692d074d587, agent scheduler
INFO [alembic.migration] Running upgrade 4692d074d587 -> 1341ed32cc1e, nvp_net_binding
INFO [alembic.migration] Running upgrade 1341ed32cc1e -> grizzly, grizzly
INFO [alembic.migration] Running upgrade grizzly -> f489cf14a79c, DB support for load balancing service (havana)
INFO [alembic.migration] Running upgrade f489cf14a79c -> 176a85fc7d79, Add portbindings db
INFO [alembic.migration] Running upgrade 176a85fc7d79 -> 32b517556ec9, remove TunnelIP model
INFO [alembic.migration] Running upgrade 32b517556ec9 -> 128e042a2b68, ext_gw_mode
INFO [alembic.migration] Running upgrade 128e042a2b68 -> 5ac71e65402c, ml2_initial
INFO [alembic.migration] Running upgrade 5ac71e65402c -> 3cbf70257c28, nvp_mac_learning
INFO [alembic.migration] Running upgrade 3cbf70257c28 -> 5918cbddab04, add tables for router rules support
INFO [alembic.migration] Running upgrade 5918cbddab04 -> 3cabb850f4a5, Table to track port to host associations
INFO [alembic.migration] Running upgrade 3cabb850f4a5 -> b7a8863760e, Remove cisco_vlan_bindings table
INFO [alembic.migration] Running upgrade b7a8863760e -> 13de305df56e, nec_add_pf_name
INFO [alembic.migration] Running upgrade 13de305df56e -> 20ae61555e95, DB Migration for ML2 GRE Type Driver
INFO [alembic.migration] Running upgrade 20ae61555e95 -> 477a4488d3f4, DB Migration for ML2 VXLAN Type Driver
INFO [alembic.migration] Running upgrade 477a4488d3f4 -> 2032abe8edac, LBaaS add status description
INFO [alembic.migration] Running upgrade 2032abe8edac -> 52c5e4a18807, LBaaS Pool scheduler
INFO [alembic.migration] Running upgrade 52c5e4a18807 -> 557edfc53098, New service types framework (service providers)
INFO [alembic.migration] Running upgrade 557edfc53098 -> e6b16a30d97, Add cisco_provider_networks table
INFO [alembic.migration] Running upgrade e6b16a30d97 -> 39cf3f799352, FWaaS Havana-2 model
INFO [alembic.migration] Running upgrade 39cf3f799352 -> 52ff27f7567a, Support for VPNaaS
INFO [alembic.migration] Running upgrade 52ff27f7567a -> 11c6e18605c8, Pool Monitor status field
INFO [alembic.migration] Running upgrade 11c6e18605c8 -> 35c7c198ddea, remove status from HealthMonitor
INFO [alembic.migration] Running upgrade 35c7c198ddea -> 263772d65691, Cisco plugin db cleanup part II
INFO [alembic.migration] Running upgrade 263772d65691 -> c88b6b5fea3, Cisco N1KV tables
INFO [alembic.migration] Running upgrade c88b6b5fea3 -> f9263d6df56, remove_dhcp_lease
INFO [alembic.migration] Running upgrade f9263d6df56 -> 569e98a8132b, metering
INFO [alembic.migration] Running upgrade 569e98a8132b -> 86cf4d88bd3, remove bigswitch port tracking table
INFO [alembic.migration] Running upgrade 86cf4d88bd3 -> 3c6e57a23db4, add multiprovider
INFO [alembic.migration] Running upgrade 3c6e57a23db4 -> 63afba73813, Add unique constraint for id column of TunnelEndpoint
INFO [alembic.migration] Running upgrade 63afba73813 -> 40dffbf4b549, nvp_dist_router
INFO [alembic.migration] Running upgrade 40dffbf4b549 -> 53bbd27ec841, Extra dhcp opts support
INFO [alembic.migration] Running upgrade 53bbd27ec841 -> 46a0efbd8f0, cisco_n1kv_multisegment_trunk
INFO [alembic.migration] Running upgrade 46a0efbd8f0 -> 2a3bae1ceb8, NEC Port Binding
INFO [alembic.migration] Running upgrade 2a3bae1ceb8 -> 14f24494ca31, DB Migration for Arista ml2 mechanism driver
INFO [alembic.migration] Running upgrade 14f24494ca31 -> 32a65f71af51, ml2 portbinding
INFO [alembic.migration] Running upgrade 32a65f71af51 -> 66a59a7f516, NEC OpenFlow Router
INFO [alembic.migration] Running upgrade 66a59a7f516 -> 51b4de912379, Cisco Nexus ML2 mechanism driver
INFO [alembic.migration] Running upgrade 51b4de912379 -> 1efb85914233, allowedaddresspairs
INFO [alembic.migration] Running upgrade 1efb85914233 -> 38fc1f6789f8, Cisco N1KV overlay support
INFO [alembic.migration] Running upgrade 38fc1f6789f8 -> 4a666eb208c2, service router
INFO [alembic.migration] Running upgrade 4a666eb208c2 -> 338d7508968c, vpnaas peer_address size increase
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 145, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 205, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 681, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 48, in upgrade
    type_=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 322, in go
    return fn(*arg, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 300, in alter_column
    existing_autoincrement=existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/mysql.py"", line 42, in alter_column
    else existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ipsec_site_connections' doesn't exist"") 'ALTER TABLE ipsec_site_connections CHANGE peer_address peer_address VARCHAR(255) NULL' ()"
0,"https://review.openstack.org/70079
Dear documentation bug triager. This bug was created here because we did not know how to map the project name ""openstack/nova"" to a launchpad project name. This indicates that the notify_impact config needs tweaks. You can ask the OpenStack infra team (#openstack-infra on freenode) for help if you need to.
commit 73c87a280e77e03d228d34ab4781ca2e3b02e40e
Author: Gary Kotton <email address hidden>
Date: Thu Jan 30 01:44:10 2014 -0800
    VMware: update the default 'task_poll_interval' time
    The original means that each operation against the backend takes at
    least 5 seconds. The default is updated to 0.5 seconds.
    DocImpact
        Updated default value for task_poll_interval from 5 seconds to
        0.5 seconds
    Change-Id: I867b913f52b67fa9d655f58a2e316b8fd1624426
    Closes-bug: #1274439"
1,"_default_block_device_names method of the compute manager, would call the conductor block_device_mapping_update method with the wrong arguments, causing a TypeError and ultimately the instance to fail.
This bug happens only when using a driver that does not provid it's own implementation of default_device_names_for_instance, (currently only the libvirt driver does this).
Also affects havana since https://review.openstack.org/#/c/40229/"
1,"2014-03-26 16:07:51,111 (neutron.plugins.vmware.common.nsx_utils): WARNING nsx_utils get_nsx_router_id Unable to find NSX router for Neutron router e90010ab-f630-4f81-9f6b-ac3da4b29aef
2014-03-26 16:07:51,111 (neutron.plugins.vmware.api_client.base): DEBUG base acquire_connection [0] Acquired connection https://17.176.14.50:443. 8 connection(s) available.
2014-03-26 16:07:51,112 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Issuing - request GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus
2014-03-26 16:07:51,320 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Completed request 'GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus': 200 (0.207953929901 seconds)
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.base): DEBUG base release_connection [0] Released connection https://17.176.14.50:443. 9 connection(s) available.
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.eventlet_request): DEBUG eventlet_request _handle_request [0] Completed request 'GET /ws.v1/lrouter?relations=LogicalRouterStatus': 200
2014-03-26 16:07:51,322 (neutron.plugins.vmware.api_client.client): DEBUG client request Request returns ""<httplib.HTTPResponse instance at 0x3d97b00>""
2014-03-26 16:07:51,324 (neutron.openstack.common.loopingcall): ERROR loopingcall _inner in dynamic looping call
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/openstack/common/loopingcall.py"", line 123, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 606, in _synchronize_state
    scan_missing=scan_missing)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 380, in _synchronize_lrouters
    ctx, router, lrouter and lrouter.get('data'))
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 339, in synchronize_router
    self._nsx_cache.update_lrouter(lrouter)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 134, in update_lrouter
    self._update_resources(self._lrouters, [lrouter])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 86, in _update_resources
    item_id = item['uuid']
KeyError: 'uuid'"
0,"From the comments of update_service_capabilities, it is said that once publish_service_capabilities was removed, then we can begin the process of its removal.
Now publish_service_capabilities has been removed, so we can remove update_service_capabilities now as no one is calling it
def update_service_capabilities(self, context, service_name,
                                    host, capabilities):
        """"""Process a capability update from a service node.""""""
        #NOTE(jogo) This is deprecated, but is used by the deprecated
        # publish_service_capabilities call. So this can begin its removal
        # process once publish_service_capabilities is removed.
        if not isinstance(capabilities, list):
            capabilities = [capabilities]
        for capability in capabilities:
            if capability is None:
                capability = {}
            self.driver.update_service_capabilities(service_name, host,
                                                    capability)"
1,"There are some error cases not handled when we unshelve an instance in the conductor.
   nova/conductor/manager.py#823
if the key 'shelved_image_id' is not defined the current code will raise an KeyError not handled.
Also when the 'shelved_image_id' is set to None (which is not the expected behavior), the error is not correctly handled and the message could be confusing."
1,"recently merged ""RPC additions to support DVR"" change make ofagent CI fail.
https://review.openstack.org/#/c/102332/

http://180.37.183.32/ryuci/32/102332/38/check/check-tempest-dsvm-ofagent/d88f439/logs/screen-q-svc.txt.gz

2014-07-18 03:53:27.765 16087 ERROR oslo.messaging.rpc.dispatcher [req-a13fe10b-aae5-458a-a44f-564563368b56 ] Exception during message handling: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/plugins/ml2/rpc.py"", line 191, in update_device_up
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher l3plugin.dvr_vmarp_table_update(rpc_context, port_id, ""add"")
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 439, in dvr_vmarp_table_update
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher port_dict = self._core_plugin._get_port(context, port_id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 109, in _get_port
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher raise n_exc.PortNotFound(port_id=id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher PortNotFound: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher"
1,"Steps to Reproduce:
Setup onto ICEHouse GA:
Build :2014.1-0ubuntu1~cloud0
Steps to Reproduce:
1. Create a lbaas site. (Pool,member,vip,health monitor)
2. Check the configuration onto the network node under
/var/lib/neutron/lbaas/$Pool_id/
conf pid sock
3. Delete all the lbaas and neutron resource created onto the controller.
4. Check the configuration onto the network node.
Actual Results: Directory structure for the pool id remain as its but the files got delete
Expected Results: all the configuration and files/folder should get deleted after deletion of lbaas resource.
Note: Working in Havana GA release."
1,"when launching multiple VMs at the same time (for example by selecting a higher than one instance count in the dashboard), the action will fail with various errors.
most likely the process goes as follows:
Thread1 + ... + ThreadN at the same time: check if the host is mapped -> it doesn't
Thread1: let's create the host!
...
ThreadN: let's create the host!
Thread1 succeeds
Thread2..N fails
Thread1 doesn't even finish the mapping _most of the time_"
1,"Second firewall creation returns 500.
It is an expected behavior of firewall reference implementation and an internal server error should not be returned.
It is some kind of quota error and 409 looks appropriate."
1,"When we configure this item config_drive_cdrom=True on hyperV, then boot an instance using config drive. A config drive image named 'configdrive.iso' will be created. But when we resize this instance, The config drive image will not preserved in the instance."
0,The base FibreChannelDriver is missing the new accept_transfer method.
1,"If the number of datastores is greater than the maximum, the PropertyCollector returns a token value that should be used to iterate over the entire result set. The get_datastore_ref_and_name function in vm_util.py is aware of tokens but it stops on the first found datastore (line 942) instead searching for the best match over the entire set.
We should improve get_datastore_ref_and_name to:
1) clearly specify how the datastore is selected (the current pydoc is wrong)
2) select the best match by iterating the whole result set"
1,"Established connections via floating ip won't get disconnected after we disassociating that floating ip.
As mentioned in maillist by Vish, we should move clean_conntrack() from migration code to remove_floating_ip.
https://github.com/openstack/nova/blob/master/nova/network/floating_ips.py#L575"
1,"There is a race condition in the remoteFS _ensure_shares_mounted() method:
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/remotefs.py#L141
The list of known mounted shares is cleared before being rebuilt. This can cause a race where an operation attempts to run and finds an empty list, simply because it is in the process of being refreshed."
0,"create_table in downgrade in migration 2a6d0b51f4bb_cisco_plugin_cleanup does not match table's content created by migration folsom_initial, because of this there is a problem with downgrade.

File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/2a6d0b51f4bb_cisco_plugin_cleanup.py"", line 87, in downgrade
    sa.PrimaryKeyConstraint(u'id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 117, in _table
    t = sa_schema.Table(name, m, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 318, in __new__
    table._init(name, metadata, *args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 385, in _init
    self._init_items(*args)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 64, in _init_items
    item._set_parent_with_dispatch(self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/events.py"", line 234, in _set_parent_with_dispatch
    self._set_parent(parent)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2131, in _set_parent
    ""named '%s' is present."" % (table.description, col))
ArgumentError: Can't create ForeignKeyConstraint on table 'portprofile_bindings': no column named 'ports' is present."
0,"Cannot attach or detach volume if use v2 volume endpoint.

How to reproduce this bug:

1, Remove the volume v1 endpoint from keystone, use ""keystone endpoint-delete""
2. Create the v2 volume endpoint via CLI command: keystone endpoint-create
3.export OS_VOLUME_API_VERSION=2
4. run 'cinder list' ok, and create one volume
5. run 'nova volume-attach' or 'nova detach' command will be failed. return 500 error.

After investigated, I found the root cause is nova not support cinderclient v2 yet, we need make some code change to let nova to support cinderclient v2."
0,remove pointless test TestN1kvNonDbTest
0,"A large number of libvirt test classes inherit from the TestCase class, which means they incur the overhead of database setup
nova/tests/virt/libvirt/test_blockinfo.py:class LibvirtBlockInfoTest(test.TestCase):
nova/tests/virt/libvirt/test_blockinfo.py:class DefaultDeviceNamesTestCase(test.TestCase):
nova/tests/virt/libvirt/test_dmcrypt.py:class LibvirtDmcryptTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class CacheConcurrencyTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtConnTestCase(test.TestCase,
nova/tests/virt/libvirt/test_driver.py:class HostStateTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class IptablesFirewallTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class NWFilterTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtUtilsTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtDriverTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeUsageTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtNonblockingTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeSnapshotTestCase(test.TestCase):
nova/tests/virt/libvirt/test_imagebackend.py:class EncryptedLvmTestCase(_ImageTestCase, test.TestCase):
nova/tests/virt/libvirt/test_vif.py:class LibvirtVifTestCase(test.TestCase):
Some of these do not even use the database so can be trivially changed. Others will need significant refactoring work to remove database access before they can be changed to NoDBTestCase"
1,"Port the patch for the following bug of OVS to OFAgent.
https://bugs.launchpad.net/nova/+bug/1240849"
0,"In store/__init__.py, there is two places use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.

The code is:

def check_location_metadata(val, key=''):
    t = type(val)
    if t == dict:
        for key in val:
            check_location_metadata(val[key], key=key)
    elif t == list:
        ndx = 0
        for v in val:
            check_location_metadata(v, key='%s[%d]' % (key, ndx))
            ndx = ndx + 1
    elif t != unicode:
        raise BackendException(_(""The image metadata key %s has an invalid ""
                                 ""type of %s. Only dict, list, and unicode ""
                                 ""are supported."") % (key, str(t)))

def store_add_to_backend(image_id, data, size, store):
    (location, size, checksum, metadata) = store.add(image_id, data, size)
    if metadata is not None:
        if type(metadata) != dict:
            msg = (_(""The storage driver %s returned invalid metadata %s""
                     ""This must be a dictionary type"") %
                   (str(store), str(metadata)))
            LOG.error(msg)
            raise BackendException(msg)"
0,"Multiple constants define linux interface maximum length (15):
* neutron.agent.linux.utils: DEVICE_NAME_LEN in get_interface_mac (=15)
* neutron.agent.linux.ip_lib: VETH_MAX_NAME_LENGTH (=15)
* neutron.plugins.common.constants: MAX_DEV_NAME_LEN (=16 incorrect value)

They should be replaced by a unique constant equals to 15 to ensure consistency."
0,"If you hit the cinder endpoint directly, you get the version list. The generated hrefs all point to /v1.
cory@cfsyn28:~/devstack$ curl localhost:8776 | python -m json.tool
{
    ""versions"": [
        {
            ""id"": ""v1.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-01-04T11:33:21Z""
        },
        {
            ""id"": ""v2.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-11-21T11:33:21Z""
        }
    ]
}"
0,remove dead code _arp_spoofing_rule()
1,"When fields filtering is applied, neutron server prints a trace:
 ERROR neutron.api.v2.resource [-] index failed
 TRACE neutron.api.v2.resource Traceback (most recent call last):
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
 TRACE neutron.api.v2.resource result = method(request=request, **args)
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 273, in index
 TRACE neutron.api.v2.resource return self._items(request, True, parent_id)
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 227, in _items
 TRACE neutron.api.v2.resource obj_list = obj_getter(request.context, **kwargs)
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/servicetype_db.py"", line 63, in get_service_providers
 TRACE neutron.api.v2.resource return self.conf.get_service_providers(filters, fields)
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 162, in get_service_providers
 TRACE neutron.api.v2.resource return self._fields(res, fields)
 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 151, in _fields
 TRACE neutron.api.v2.resource return dict(((key, item) for key, item in resource.items()
 TRACE neutron.api.v2.resource AttributeError: 'list' object has no attribute 'items'"
0,"Regression caused by https://github.com/openstack/nova/commit/23158ad8b340ed5c53fe6ad0fe582f47467c9127
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 6447, in test_get_domain_info_with_more_return
    mock_domain = libvirt.virDomain('qemu:///system', None)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 213, in __init__
    self._def = self._parse_definition(xml)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 220, in _parse_definition
    tree = etree.fromstring(xml)
  File ""lxml.etree.pyx"", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68106)
  File ""parser.pxi"", line 1784, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102444)
ValueError: can only parse strings
I think the domain should be mocked and shouldn't be created directly.
I'm facing this issue with py26 on OEL 6.3, if it matter."
1,"Wrong example of ""nova_endpoint_template"" in cinder.conf
e.g. http://localhost:8774/v2/%(tenant_id)s
should be
e.g. http://localhost:8774/v2/%(project_id)s"
0,"A new config option was introduced recently, enable_security_group. This config option specifies if the agent should have the security group enabled or not.
The Hyper-V agent does not take this config option into account, which means the security groups rules are always applied."
0,There is new table in cinder db volume_admin_metadata with FK relationship with volumes. This admin metadata needs to be handled in the same way as normal volume metadata.
1,"When overcommiting on hard disk usage, the audit logs report negative amounts of free disk space. While technically correct, it may confuse the user, or make the user think there is something wrong with the tracking.
The patch to fix this will be in a similar vein to https://review.openstack.org/#/c/93261/"
1,"String formatting template Issues
Name collision on the _ variable"
1,"See https://review.openstack.org/18689 for some background
We can't use os.wait() to block until a child exited so, instead, we're busy-looping
We should be able to come up with another way of doing this - e.g. using pipes provided to each child to give us a selectable handle we can block on"
0,"Set admin password fails in the child cell service with the following:

2014-06-24 20:45:08.403 29591 DEBUG nova.openstack.common.policy [req- None] Rule compute
:set_admin_password will be now enforced enforce /opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/openstack/common/
policy.py:288
2014-06-24 20:45:08.404 29591 ERROR nova.cells.messaging [req- None] Error processing mes
sage locally: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging Traceback (most recent call last):
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 200, in _process_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 1289, in _process_message_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 692, in run_compute_api_method
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 201, in wrapped
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 191, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 172, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 2712, in set_admin_password
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging instance.task_state = task_states.UPDATING_PASSWORD
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging"
1,"when specifying a volume or an image with a user volume to create a virtual machine, if the virtual machine fails to be created for the first time with the parameter delete_on_termination being set 鈥渢rue鈥? the specified volume or the user volume will be deleted, which causes that the rescheduling fails.
for example:
1. upload a image
| 62aa6627-0a07-4ab4-a99f-2d99110db03e | cirros-0.3.2-x86_64-uec | ACTIVE
2.create a boot volume by the above image
cinder create --image-id 62aa6627-0a07-4ab4-a99f-2d99110db03e --availability-zone nova 1
| b821313a-9edb-474f-abb0-585a211589a6 | available | None | 1 | None | true | |
3. create a virtual machine
nova boot --flavor m1.tiny --nic net-id=28216e1d-f1c2-463b-8ae2-330a87e800d2 tralon_disk1 --block-device-mapping vda=b821313a-9edb-474f-abb0-585a211589a6::1:1
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume b821313a-9edb-474f-abb0-585a211589a6. (HTTP 400) (Request-ID: req-486f7ab5-dc08-404e-8d4c-ac570d4f4aa1)
4. use the ""cinder list"" to find that the volume b821313a-9edb-474f-abb0-585a211589a6 has been deleted
+----+--------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+----+--------+------+------+-------------+----------+-------------+
+----+--------+------+------+-------------+----------+-------------+"
0,"the db_replicator.py is probably one of the more complex pieces of swift code, and one of the least tested. We need more tests, and better tests for this functionality."
1,"When an exception occurs in the VMWare driver, for example when there are no more IP addresses available, then the following exception is returned:
2013-09-22 05:26:22.522 ERROR nova.compute.manager [req-b29710eb-5cb9-4de1-adca-919119b10460 demo demo] [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Error: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Traceback (most recent call last):
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/compute/manager.py"", line 1038, in _build_instance
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] set_access_ip=set_access_ip)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/compute/claims.py"", line 53, in __exit__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] self.abort()
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/compute/claims.py"", line 107, in abort
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] LOG.debug(_(""Aborting claim: %s"") % self, instance=self.instance)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 228, in __mod__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] return copied._save_parameters(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 186, in _save_parameters
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] self.params = copy.deepcopy(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/usr/lib/python2.7/copy.py"", line 285, in _deepcopy_inst
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] return x.__deepcopy__(memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 195, in vim_request_handler
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] _(""Exception in %s "") % (attr_name), excep)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] VimException: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]
2013-09-22 05:37:01.926 DEBUG nova.virt.vmwareapi.driver [req-8c58e23f-3970-4c4c-bfbf-39060c0da3ba demo demo] 'VMwareAPISession' object has no attribute 'vim' from (pid=2206) __del__ /opt/stack/nova/nova/virt/vmwareapi/driver.py:705"
0,"The XenAPI driver reports a hypervisor type of 'xapi' for supported instances. This is confusing the hypervisor type, which should be 'xen', with the management API type which is 'xapi'.

The Baremetal driver reports a hypervisor type of 'baremetal' for supported instances. This is confusing the hypervisor type with the nova driver type. There is no hypervisor concept with the bare metal driver, things just run natively, so the type should be 'native'."
0,"After adding state reporting to metadata agent following tracebacks appeared in py26 unit test console logs:
2013-11-22 15:08:33.914 | ERROR:neutron.agent.metadata.agent:Failed reporting state!
2013-11-22 15:08:33.914 | Traceback (most recent call last):
2013-11-22 15:08:33.914 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/metadata/agent.py"", line 258, in _report_state
2013-11-22 15:08:33.914 | use_call=self.agent_state.get('start_flag'))
2013-11-22 15:08:33.914 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/rpc.py"", line 74, in report_state
2013-11-22 15:08:33.914 | return self.cast(context, msg, topic=self.topic)
2013-11-22 15:08:33.915 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/proxy.py"", line 171, in cast
2013-11-22 15:08:33.915 | rpc.cast(context, self._get_topic(topic), msg)
2013-11-22 15:08:33.915 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/__init__.py"", line 158, in cast
2013-11-22 15:08:33.915 | return _get_impl().cast(CONF, context, topic, msg)
2013-11-22 15:08:33.915 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 166, in cast
2013-11-22 15:08:33.915 | check_serialize(msg)
2013-11-22 15:08:33.916 | File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 131, in check_serialize
2013-11-22 15:08:33.916 | json.dumps(msg)
2013-11-22 15:08:33.916 | File ""/usr/lib64/python2.6/json/__init__.py"", line 230, in dumps
2013-11-22 15:08:33.916 | return _default_encoder.encode(obj)
2013-11-22 15:08:33.916 | File ""/usr/lib64/python2.6/json/encoder.py"", line 367, in encode
2013-11-22 15:08:33.916 | chunks = list(self.iterencode(o))
2013-11-22 15:08:33.917 | File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 | for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.917 | File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.917 | for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.917 | File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 | for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 | File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 | for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.918 | File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.918 | for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 | File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 | for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 | File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.919 | for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.919 | File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.919 | for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 | File ""/usr/lib64/python2.6/json/encoder.py"", line 317, in _iterencode
2013-11-22 15:08:33.919 | for chunk in self._iterencode_default(o, markers):
2013-11-22 15:08:33.920 | File ""/usr/lib64/python2.6/json/encoder.py"", line 323, in _iterencode_default
2013-11-22 15:08:33.920 | newobj = self.default(o)
2013-11-22 15:08:33.920 | File ""/usr/lib64/python2.6/json/encoder.py"", line 344, in default
2013-11-22 15:08:33.920 | raise TypeError(repr(o) + "" is not JSON serializable"")
2013-11-22 15:08:33.920 | TypeError: <MagicMock name='cfg.CONF.host' id='989177040'> is not JSON serializable
2013-11-22 15:08:33.921 | WARNING:neutron.openstack.common.loopingcall:task run outlasted interval by <MagicMock name='cfg.CONF.AGENT.report_interval.__sub__().__neg__()' id='989263056'> sec
this can be observed in jenkins results for any patch on review.
Need to mock loopingcall in metadata agent tests in order to fix this"
0,"When trying to perform boot instance from volume using the VMwareESXDriver, the operation errors out.
Command:
$ nova boot --flavor 1 --block-device-mapping vda=222e8ece-8723-4930-803c-8ae5cf233a87:::0 vm1
Log messages
d3-59fce43903e8] Root volume attach. Driver type: vmdk attach_root_volume /opt/stack/nova/nova/virt/vmwareapi/volumeops.py:458
2013-10-26 14:49:13.393 30706 WARNING nova.virt.vmwareapi.driver [-] Task [RelocateVM_Task] (returnval){
   value = ""haTask-162-vim.VirtualMachine.relocate-327302855""
   _type = ""Task""
 } status: error The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 ERROR nova.compute.manager [req-e95b7262-a70c-436b-a9d5-0b8045cbf3f5 4471d6567a6b4dd29affbc849f3814d9 256df8ea370d4de2b40edfe9b0ea4063] [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Instance failed to spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Traceback (most recent call last):
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 178, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] admin_password, network_info, block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 538, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] data_store_ref)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 467, in attach_root_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] self._relocate_vmdk_volume(volume_ref, res_pool, datastore)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 295, in _relocate_vmdk_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] self._session._wait_for_task(task.value, task)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 901, in _wait_for_task
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] ret_val = done.wait()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] return hubs.get_hub().switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] return self.greenlet.switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] NovaException: The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]"
0,"In BaseTestCase a clean up is added to stop all patches :
self.addCleanup(mock.patch.stopall)
the tests that inherits from BaseTestCase don't need to stop their patches"
0,"unused code 'as e' in exception blocks with flowing files:
neutron/api/v2/resource.py
neutron/plugins/vmware/nsxlib/router.py"
1,"Earlier this month a fix was released to allow non-root CIDRs to be entered. The fix was for bug 1188845. It was approved and merged. Now, however, the functionality has been reversed by commit 53a66b299f18a7184972502b43441a5ad7b050fd (bug 1195974) which checks the input earlier in the path and rejects anything but the root CIDR.
I am unable to find documentation that says you cannot use a non-root (I.E. X.X.X.254) IP to specify a subnet. With an IP and subnet mask it is possible to create a network and determine the root for the subnet."
1,"The nova-manage db archive_deleted_rows fails if max_rows is a large number (I tried 1 million but a smaller value may also cause issues) because it receives an exception from the sqlalchemy and db layer regarding the number of parameters on the sql statement.
Database has a limite of maximum total length of host and indicator variables in SQL statement. When I ran the archive, the table had 165822 rows in it and 18489 of those were not deleted. Therefore, 147333 rows had deleted=1. So get error from Database."
1,"The storage profile in the volume type is ignored while creating a volume from stream-optimized image. Even though the backing is placed in a compliant datastore, the profile is not associated with the backing."
0,"See
http://lists.openstack.org/pipermail/openstack-dev/2014-October/047978.html"
1,"When checking if an instance is in the same AZ as a volume nova uses the instances availability_zone attribute. This isn't the correct way to get an instances AZ, it should use the value gotten through querying the aggregate the instance is on"
1,"Boot instance with flavor as below:
os@os2:~$ nova flavor-show 100
+----------------------------+------------------------+
| Property | Value |
+----------------------------+------------------------+
| OS-FLV-DISABLED:disabled | False |
| OS-FLV-EXT-DATA:ephemeral | 0 |
| disk | 0 |
| extra_specs | {""hw:numa_nodes"": ""2""} |
| id | 100 |
| name | numa.nano |
| os-flavor-access:is_public | True |
| ram | 512 |
| rxtx_factor | 1.0 |
| swap | |
| vcpus | 8 |
+----------------------------+------------------------+

The result is

  <cputune>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>

The cputune should be:

  <cputune>
    <vcpupin vcpu='0' cpuset='0-7,16-23'/>
    <vcpupin vcpu='1' cpuset='0-7,16-23'/>
    <vcpupin vcpu='2' cpuset='0-7,16-23'/>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='4' cpuset='8-15,24-31'/>
    <vcpupin vcpu='5' cpuset='8-15,24-31'/>
    <vcpupin vcpu='6' cpuset='8-15,24-31'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>"
1,"Startinstance response elements shown as below:
""<StartInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-5970ccd7-c763-456c-89f0-5b55ea18880b</requestId>
  <return>true</return>
</StartInstancesResponse>
""
But as per the AWS API reference doc, the response elements shown be as below:
==
<StartInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>0</code>
          <name>pending</name>
      </currentState>
      <previousState>
          <code>80</code>
          <name>stopped</name>
      </previousState>
    </item>
  </instancesSet>
</StartInstancesResponse>
===
here, <instanceSet> information missing in the response elements."
1,The resize ephemeral disk blueprint has regressed the ability to spawn instances with ephemeral disks.
0,"while restoring backup to a volume only data should get updated however target volume name getting changed with name of backuped volume source.

ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol | 1 | None | true | |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | silver_vol | 1 | silver | false | |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
ssatya@devstack:~/devstack$ cinder backup-restore a8b2f82c-a6a9-4fd1-b087-18798970ff4d --volume a0f016b7-6e6a-4d6f-a17b-1f18b0408158
ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol | 1 | None | true | |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | image_vol | 1 | silver | true | |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+"
0,"LBaaS extension is using the new resource_helper module to register its resources to resource manager.
This change https://review.openstack.org/#/c/69803/3 added new quota resources.
In order those new quota resources to be registered in quotas engine, register_quota=True argument should be passed to the resource_helper.build_resource_info() function.

neutron/extensions/loadbalancer.py line 351

This bug causes modified tempest quotas test from https://review.openstack.org/#/c/60008 to fail
because new lbaas quota resources are not registered"
0,"When user updates an image with location but introduced a typo, it's expected to run into location not found exception. However, because we don't distinguish the image not found and location not found, so Glance will say the image is not found based on current design.
Failed to find image 41a997ac-e647-4b5f-b158-8e6e13875f88 to update
See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L124 and https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L156 Though I think the issue is also existed in other backends."
1,"When a volume is attached to an instance and we backup the instance or try to create an image from it, the volume's disk is being backed up and not the instance's primary disk.
More info: https://communities.vmware.com/community/vmtn/openstack/blog/2013/08/28/introducing-vova-an-easy-way-to-try-out-openstack-on-vsphere#comment-29775"
1,"cinder-scheduler send a rpc message to cinder-volume which was killed just now.
Then when cinder-volume restart, it may receive the rpc message and begin to handle it before driver initialized.
Most functions in manager need to judge wether the driver has been initialized, so the rpc request would be reject. But those requests shoud be handle in fact.
The reason is, in service.py, we create consumer before manager call ""init_host()"", and we call ""self.driver.set_initialized()"" in init_host."
0,"This patch optimizes validate_networks so that it only queries neutron
when needed. Previously, this method would perform an additional net_list,
list_ports, and show_quota regardless if a request contains only
port_ids. If a request only contains port ids we do not need to check neutron
for quota as these ports are already allocated."
1,"the NSX plugin is unable to associate a floating IP to a different internal address on the same port where it's currently associated.
How to reproduce:
- create a port with two IPs on the same subnet (just because the fip needs to go through the same router)
- associate a floating IP with IP #1
- associate the same floating IP with IP#2
- FAIL!
This happens with this new test being introduced in tempest: https://review.openstack.org/#/c/71251"
0,"In the current implementation, CRD configuration is existing within the code of ML2 mechanism driver.
When any other plugin/driver need to use this configuration needs to duplicate the complete configuration.
So the CRD configuration options are to be moved to a separate file to be used with other plugin/drivers."
0,"When a driver fails to initialize at startup, the get_volume_stats() will always fail. The problem is that the log file doesn't include the driver name in the update.
""WARNING cinder.volume.manager [-] Unable to update stats, driver is uninitialized"""
0,"The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2011-01-21T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.compute+xml;version=2""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=2""
         }
      ],
      ""id"":""v2.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/os-compute-devguide-2.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/wadl/os-compute-2.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}
The links in the GET /v3 response do not work either:
{
   ""version"":{
      ""status"":""EXPERIMENTAL"",
      ""updated"":""2013-07-23T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=3""
         }
      ],
      ""id"":""v3.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v3/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/os-compute-devguide-3.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/wadl/os-compute-3.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
0,"In the following migration scripts, not all BadStoreUri error messages are logged with image_id. Hence it might become difficult to track the record which has got BadStoreUri. Including the image_id with the error log messages would help in tracking the faulty records easily when BadStoreUri exception is raised.
015_quote_swift_credentials.py
017_quote_encrypted_swift_credentials.py"
1,"The Hyper-V agent in Windows Server 2012 R2 is currently applying the security group rules wrong due to two reasons:
First, when assigning a weight for a rule, the agent takes into account the weight of the default reject rules, which causes certain reject rules to have a higher priority than allow rules.
Secondly, if a rule with no port_range_min and no port_range_max defined is applied first, it is replaced by the rules with the port range defined. This can cause faulty connectivity for the virtual machine, including for the default security group."
1,"method extend_volume in cinder/volume/drivers/rbd.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required.
method extend_volume in cinder/volume/drivers/sheepdog.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required."
1,"In order to fix bug (https://bugs.launchpad.net/neutron/+bug/1288358) following commit was made https://review.openstack.org/#/c/81334.

That has introduce a problem where neutron-db-manage does not work with some of the options.
For example:
root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> current
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 233, in current
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine

root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> stamp icehouse
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 91, in do_stamp
    sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 258, in stamp
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine"
0,"Exception in during volume creation in Nexenta NFS volume driver

2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp **args)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch"
1,"When the InstanceNotRunning exception is raised during the initial live-migrate, it should really just revert to task_state=None in the same way as if there was NoValidHost, or the compute service was down."
0,"The method copy_volume_to_image from windows.py specifies the wrong volume format, namely 'vpc'. In this case, the upload_volume method from image_utils will attempt to convert the volume to vhd which will result in an error from qemu as it does not recognize the format. Qemu-img still uses the legacy name 'vpc' for vhd files which causes this confusion.

The solution is to explicitly using vhd as a format when uploading volumes.

"
1,"If we initiate a connection to a floating ip right after creating/associating it with a VM's port, the connection goes to advanced service router for a short period if time, instead to the VM the floating ip is supposed to be associated with.
The root cause is when creating/associating a floating ip using advanced service router, in addition to create a DNAT rule, we also need to configure the floating ip on advanced service router's vNic so the advanced service router can reply ARP request for that IP. We configure the IP on the vnic before the DNAT rule is configured, therefore, there is a small window, after IP is configured but before DNAT is configured, that the traffic to floating IP will reach advanced service router.
The fix is to configure the DNAT rule before applying the IP on advanced service router's vNic."
0,"The BigSwitch plugin currently determines if it needs to reconnect by checking for an httplib ImproperConnectionState exception. However, this exception is too narrow and does not cover the other httplib exceptions that indicate a reconnection is necessary (e.g. NotConnected).
It should just catch httplib.HTTPExceptions."
1,"This change:

https://review.openstack.org/#q,I82073352641d3eb2ab3d6e9a6b64afc99a30dcc7,n,z

Causes an upgrade failure if users are backed by swift, as it is not in the list of defaults anymore.

This change should be reverted and depending on the list should be logged as a deprecation warning for 1 cycle."
0,"A few tests use ""called_once_with_args"" instead of mock's ""assert_called_once_with_args""
without checking the result.
That means that we're not asserting for that to happen.
Those tests need to be fixed.
[majopela@f20-devstack neutron]$ grep "".called_once_with"" * -R | grep -v assert
neutron/tests/unit/test_dhcp_agent.py: disable.called_once_with_args(network.id)
neutron/tests/unit/test_dhcp_agent.py: uuid5.called_once_with(uuid.NAMESPACE_DNS, 'localhost')
neutron/tests/unit/test_post_mortem_debug.py: mock_print_exception.called_once_with(*exc_info)
neutron/tests/unit/test_db_migration.py: mock_open.write.called_once_with('a')
neutron/tests/unit/test_agent_netns_cleanup.py: ovs_br_cls.called_once_with('br-int', conf.AGENT.root_helper)
neutron/tests/unit/test_metadata_agent.py: self.eventlet.wsgi.server.called_once_with("
0,"The hyperv unit appear to not properly mock all cases of report_state calls so an occasional exception will be thrown on an unrelated patch.[1]
1. http://logs.openstack.org/01/96201/6/gate/gate-neutron-python27/2b0de5e/console.html"
1,"I got the following errors in a tempest test at http://logs.openstack.org/97/62397/1/check/check-tempest-dsvm-full/3f7b8c3:
2013-12-16 16:07:52.189 27621 DEBUG nova.openstack.common.processutils [-] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-12-16 16:07:52.189 27621 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager._run_image_cache_manager_pass: Unexpected error while running command.
Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
Exit code: 1
Stdout: ''
Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task task(self, context)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5210, in _run_image_cache_manager_pass
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task self.driver.manage_image_cache(context, filtered_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 4650, in manage_image_cache
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task self.image_cache_manager.verify_base_images(context, all_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 603, in verify_base_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task inuse_backing_images = self._list_backing_images()
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 345, in _list_backing_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task backing_file = virtutils.get_disk_backing_file(disk_path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 442, in get_disk_backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task backing_file = images.qemu_img_info(path).backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/virt/images.py"", line 56, in qemu_img_info
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task 'qemu-img', 'info', path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/utils.py"", line 175, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task return processutils.execute(*cmd, **kwargs)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task cmd=' '.join(cmd))
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task ProcessExecutionError: Unexpected error while running command.
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Exit code: 1
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stdout: ''
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task"
0,"""Represents a block storage device that can be attached to a VM."" as the description of model Snapshot is incorrect."
1,NovaImageBuilder runs Anaconda and other installers inside a nova instance. After the install the instance is shut down and a snapshot is taken. After the snapshot finishes being saved the instance is terminated. In the last week I have noticed that the snapshot gets deleted also.
1,"Due to a small nit, the initiator name and the target name are inverted when passing the arguments to the method which associates the iSCSI target to an initiator. For this reason, this operation will fail.
The connection to the iSCSI target cannot be initialized properly as the method which gets portal information is missing the return value.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/windows/windows.py#L71-72
Trace: http://paste.openstack.org/show/74581/"
1,"Alex Xu reports on a related code review https://review.openstack.org/#/c/41647/
""""""
Xavier, When create vm as below, i got some error from nova-compute side:
'{""server"": {""name"": ""vm3"", ""image_ref"": ""b8cd5faa-a65f-4e47-bfc8-68061574b428"", ""flavor_ref"": ""1"", ""max_count"": 1, ""min_count"": 1, ""os-block-device-mapping:block_device_mapping"": [{""device_name"": ""/dev/vdc"", ""source_type"": ""blank"", ""destination_type"": ""local"", ""boot_index"": 0}], ""networks"": [{""uuid"": ""b6ba34f1-5504-4aca-825b-04511c104802""}]}}'
2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] Traceback (most recent call last): 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1018, in _build_instance 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] set_access_ip=set_access_ip) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1392, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] LOG.exception(_('Instance failed to spawn'), instance=instance) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1388, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] block_device_info) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1689, in spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] admin_pass=admin_password) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2002, in _create_image 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] size = eph['size'] * 1024 * 1024 * 1024 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
That because I miss 'volume_size' in the request.
""""""
I was also able to reproduce on devstack with latest master."
1,"By default, the token length for clients is 24 hours. When that token expires (or is invalidated for any reason), nova should obtain a new token.
Currently, when the token expires, it leads to the following fault:
    File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 136, in _get_available_networks
      nets = neutron.list_networks(**search_opts).get('networks', [])
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
      ret = self.function(instance, *args, **kwargs)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 325, in list_networks
      **_params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1197, in list
      for r in self._pagination(collection, path, **params):
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1210, in _pagination
      res = self.get(path, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1183, in get
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1168, in retry_request
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1103, in do_request
      resp, replybody = self.httpclient.do_request(action, method, body=body)
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 188, in do_request
      self.authenticate()
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 224, in authenticate
      token_url = self.auth_url + ""/tokens""
    TRACE nova.openstack.common.rpc.amqp TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
This error is occurring because nova/network/neutronv2/__init__.py obtains a token for communication with neutron. Nova is then authenticating the token (nova/network/neutronv2/__init__.py - _get_auth_token). Upon authentication, it passes in the token into the neutron client (via the _get_client method). It should be noted that the token is the main element passed into the neutron client (auth_url, username, password, etc... are not passed in as part of the request)
Since nova is passing the token directly into the neutron client, nova does not validate whether or not the token is authenticated.
After the 24 hour period of time, the token naturally expires. Therefore, when the neutron client goes to make a request, it catches an exceptions.Unauthorized block. Upon catching this exception, the neutron client attempts to re-authenticate and then make the request again.
The issue arises in the re-authentication of the token. The neutron client's authenticate method requires that the following parameters are sent in from its users:
 - username
 - password
 - tenant_id or tenant_name
 - auth_url
 - auth_strategy
Since the nova client is not passing these parameters in, the neutron client is failing with the exception above.
Not all methods from the nova client are exposed to this. Invocations to nova/network/neutronv2/__init__.py - get_client with an 'admin' value set to True will always get a new token. However, the clients that invoke the get_client method without specifying the admin flag, or by explicitly setting it to False will be affected by this. Note that the admin flag IS NOT determined based off the context's admin attribute.
Methods from nova/network/neutronv2/api.py that are currently affected appear to be:
 - _get_available_networks
 - allocate_for_instance
 - deallocate_for_instance
 - deallocate_port_for_instance
 - list_ports
 - show_port
 - add_fixed_ip_to_instance
 - remove_fixed_ip_from_instance
 - validate_networks
 - _get_instance_uuids_by_ip
 - associate_floating_ip
 - get_all
 - get
 - get_floating_ip
 - get_floating_ip_pools
 - get_floating_ip_by_address
 - get_floating_ips_by_project
 - get_instance_id_by_floating_address
 - allocate_floating_ip
 - release_floating_ip
 - disassociate_floating_ip
 - _get_subnets_from_port"
0,The volume's vmdk_type extra_spec property is ignored while copying image with vmware_disktype=thin/preallocated to a volume. The image should be downloaded and converted to appropriate type based on vmdk_type.
0," to be called by the imagebackend.py This will make the core logic in imagebackend.py easier to follow and make the rbd helpers easier to test."""
1,"Extending volume diff is in GB yet 3PAR is expecting MB
Extending Volume osv-iGH8frZFSDuht.TTE90Erw from 3 to 7, by 4 GB. from (pid=60905) extend_volume /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:246
Cinder is correct
cinder extend 7f688411-9618-460e-a0f6-eb5dc333bc1a 7
Results in incorrect virtual volume size for thin provisioned and extended volume"
1,"Mellanox Neutron Agent communicates with eswitchd to execute vports discovery and configuration.
In case eswitchd is not reachable, request timeout is received. The current handling of the timeout was to mark out of sync flag, flush the agent cache and try again. This behavior leads to miss the removed vports and their configuration is not removed. The request timeout was received in case group of VMs was removed at once. The solution is to add number of retries with increasing waiting time in case request timeout is received. If request timeout persists, exit the agent process."
1,"When a VLUN is created to attach a 3PAR volume to a host/instance and an NSP isn't used, the NSP isn't returned in the _create_vlun call. We were always assuming the nsp would be returned from the REST call."
0,"StorwizeSVCDriverTestCase.test_storwize_vdisk_copy_ops test case requires that there will be no context switches between green threads while it's being run.
This is achieved by monkey patching greenthread.sleep() function in setUp():
     self.sleeppatch = mock.patch('eventlet.greenthread.sleep')
     self.sleeppatch.start()
This seems to be a workaround in order to make the test pass, while it's hiding the actual reason of the problem. Any code causing the thread context switch in some other way will break the test (e.g. syncing the latest oslo.db code from oslo.incubator breaks this test, because it uses time.sleep() instead of eventlet.greenthread.sleep(), and the former is not monkey patched in the test).
Monkey patching of greenthread.sleep() in order to prevent thread context switches doesn't seem to be a good solution anyway as it is an assumption which is not true when Cinder is run in production."
1,"The connect_volume and disconnect_volume code in LibvirtISCSIVolumeDriver assumes that the targets for different portals are the same for the same multipath device. This is true for some arrays but not for others. When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during attach/detach volume operations."
0,"These showed up 9 times out of 156 runs and are of several varieties. Here is a sample of each:

https://review.openstack.org/#/c/51966/7
http://logs.openstack.org/66/51966/7/check/check-tempest-devstack-vm-postgres-full/a884365
2013-10-22 21:06:08.354 | Log File: c-vol
2013-10-22 21:06:08.355 | 2013-10-22 20:57:49.993 24174 ERROR cinder.brick.iscsi.iscsi [req-077902c7-a5e6-414a-b241-2f429766a569 c6dbf14b4d794430bb3b20395dad67d7 44b8cb996d984ea0aba77a30fa9531d0] Failed to create iscsi target for volume id:volume-52749038-fe54-4f93-9be1-50c480df4b6a: Unexpected error while running command.

https://review.openstack.org/#/c/47712/4
http://logs.openstack.org/12/47712/4/check/check-tempest-devstack-vm-neutron/843f8e8
2013-10-22 23:35:40.881 | Log File: c-vol
2013-10-22 23:35:40.882 | 2013-10-22 23:29:57.004 6308 ERROR cinder.brick.local_dev.lvm [req-4e6c087e-c345-4072-9c18-0f39861680d9 75f6a750c42745ca9440629f79dbaa18 21a7f4942ba7490d8198acf0510fe4fe] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-472b303e-7d40-4956-859e-e91506947641, RESPONSE: /dev/dm-1: stat failed: No such file or directory

https://review.openstack.org/#/c/53255/1
http://logs.openstack.org/55/53255/1/check/check-tempest-devstack-vm-neutron-pg/2319aee
2013-10-23 00:38:37.188 | Log File: c-vol
2013-10-23 00:38:37.188 | 2013-10-23 00:32:36.192 4619 ERROR cinder.brick.local_dev.lvm [req-025aa642-42ac-4a14-8b5c-0165c6c2c268 6105fe9f840049fba93305e06d8cab2c c085cabe4d8b46a19b35708f4ec1c21a] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a, RESPONSE: Can't remove open logical volume ""volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a"""
1,"HTTP 500 error occurs when one tries to get metadata by path constructed from folder name with appended value.
Steps to repro:
1. Launch VM and access its terminal
2. curl http://169.254.169.254/latest/meta-data/instance-id -- this returns some string, i.e. i-00000001
3. curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001 -- this returns HTTP 500
It's expected that the last call returns meaningful message and not produce trace backs in logs.
Errors:
----------
In VM terminal:
$ curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001
<html>
 <head>
  <title>500 Internal Server Error</title>
 </head>
 <body>
  <h1>500 Internal Server Error</h1>
  Remote metadata server experienced an internal server error.<br /><br />
 </body>
</html>$
In Neutron metadata agent:
2014-09-18 14:44:37.563 ERROR neutron.agent.metadata.agent [-] Unexpected error.
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Traceback (most recent call last):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 130, in __call__
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent return webob.exc.HTTPNotFound()
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 248, in _proxy_request
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent def _sign_instance_id(self, instance_id):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Exception: Unexpected response code: 400
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent
2014-09-18 14:44:37.566 INFO eventlet.wsgi.server [-] 10.0.0.2,<local> - - [18/Sep/2014 14:44:37] ""GET /latest/meta-data/instance-id/i-00000001 HTTP/1.1"" 500 229 0.348877
In Nova API service:
2014-09-18 14:31:19.030 ERROR nova.api.ec2 [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] FaultWrapper: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2 Traceback (most recent call last):
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 87, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2 return req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2 application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2 app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2 resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2 return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 99, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2 rv = req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2 application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2 app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2 resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2 return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/metadata/handler.py"", line 128, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2 data = meta_data.lookup(req.path_info)
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/metadata/base.py"", line 418, in lookup
2014-09-18 14:31:19.030 TRACE nova.api.ec2 data = self.get_ec2_item(path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/metadata/base.py"", line 300, in get_ec2_item
2014-09-18 14:31:19.030 TRACE nova.api.ec2 return find_path_in_tree(data, path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2 File ""/opt/stack/nova/nova/api/metadata/base.py"", line 565, in find_path_in_tree
2014-09-18 14:31:19.030 TRACE nova.api.ec2 data = data[path_tokens[i]]
2014-09-18 14:31:19.030 TRACE nova.api.ec2 TypeError: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2
2014-09-18 14:31:19.032 INFO nova.metadata.wsgi.server [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] 10.0.0.2,172.18.76.77 ""GET /latest/meta-data/placement/availability-zone/nova HTTP/1.1"" status: 400 len: 246 time: 0.5495760"
1,"In nova/api/openstack/compute/contrib/services.py there are codes like:
190 if id == ""disable-log-reason"":
191 reason = body['disabled_reason']
192 if not self._is_valid_as_reason(reason):
193 msg = _('The string containing the reason for disabling '
194 'the service contains invalid characters or is '
195 'too long.')
196 raise webob.exc.HTTPUnprocessableEntity(detail=msg)
But HTTPUnprocessableEntity should use 'explanation' parameter to return the error message.
The source can be referred here:
https://github.com/Pylons/webob/blob/master/webob/exc.py#L885"
1,"I tried the following on VMware using the VMwareVCDriver with nova-network:
1. Create an instance
2. Create a floating IP: $ nova floating-ip-create
3. Associate a floating IP with the instance: $ nova floating-ip-associate test1 10.131.254.249
4. Attempt a list of the floating IPs:
$ nova floating-ip-list
ERROR (ClientException): The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-dcb17077-c670-4e2a-8a34-715a8afc5f33)
It failed and printed out the following messages in n-api logs:
2014-08-12 13:54:29.578 ERROR nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Caught error: Cannot load 'instance' in the base class
2014-08-12 13:54:29.578 TRACE nova.api.openstack Traceback (most recent call last):
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 124, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return req.get_response(self.application)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-08-12 13:54:29.578 TRACE nova.api.openstack application, catch_exc_info=False)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-08-12 13:54:29.578 TRACE nova.api.openstack app_iter = application(self.environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/keystonemiddleware/auth_token.py"", line 565, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return self._app(env, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack response = self.app(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack resp = self.call_func(req, *args, **self.kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-08-12 13:54:29.578 TRACE nova.api.openstack return self.func(req, *args, **kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 908, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack content_type, body, accept)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 974, in _process_stack
2014-08-12 13:54:29.578 TRACE nova.api.openstack action_result = self.dispatch(meth, request, action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1058, in dispatch
2014-08-12 13:54:29.578 TRACE nova.api.openstack return method(req=request, **action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 146, in index
2014-08-12 13:54:29.578 TRACE nova.api.openstack self._normalize_ip(floating_ip)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 117, in _normalize_ip
2014-08-12 13:54:29.578 TRACE nova.api.openstack floating_ip['instance'] = fixed_ip['instance']
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/objects/base.py"", line 447, in __getitem__
2014-08-12 13:54:29.578 TRACE nova.api.openstack return getattr(self, name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/objects/base.py"", line 67, in getter
2014-08-12 13:54:29.578 TRACE nova.api.openstack self.obj_load_attr(name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack File ""/opt/stack/nova/nova/objects/base.py"", line 375, in obj_load_attr
2014-08-12 13:54:29.578 TRACE nova.api.openstack _(""Cannot load '%s' in the base class"") % attrname)
2014-08-12 13:54:29.578 TRACE nova.api.openstack NotImplementedError: Cannot load 'instance' in the base class
2014-08-12 13:54:29.579 INFO nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] http://10.131.179.211:8774/v2/875c3f62ef75400487e4a68679f8e239/os-floating-ips returned with HTTP 500
2014-08-12 13:54:29.580 DEBUG nova.api.openstack.wsgi [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Returning 500 to user: The server has either erred or is incapable of performing the requested operation. from (pid=12246) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1200"
0,neutron-check-nsx-config test the configuration for Neutron and VMware NSX. Extend this tool to check the backend even further.
1,"I feel like this is the remote end of the error? Or maybe it's an oslo bug, idk."
0,"The following test fails due to mismatching in the path separator.
FAIL: nova.tests.virt.hyperv.test_pathutils.PathUtilsTestCase.test_lookup_config_drive_path
----------------------------------------------------------------------
_StringException: Empty attachments:
  pythonlogging:''
Traceback (most recent call last):
  File ""C:\OpenStack\nova\nova\tests\virt\hyperv\test_pathutils.py"", line 48, i
 test_lookup_configdrive_path
    format_ext)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = 'C:/fake_instance_dir\\configdrive.vhd'
actual = 'C:/fake_instance_dir/configdrive.vhd'"
1,"log_handler:PublishErrorsHandler just emit the `msg` attribute of record. But many times we log with extra arguments, like ""LOG.debug('start %s', blabla)"", which will result in only show ""start %s"" in notification payload."
1,"We are getting deadlocks for concurrent quota reservations that we did not see in grizzly:
see https://bugs.launchpad.net/nova/+bug/1283987
The deadlock handling needs to be fixed as per above, but we shouldn't be deadlocking, here. It seems this is due to bad indexes in the database:
mysql> show index from quota_usages;
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| quota_usages | 0 | PRIMARY | 1 | id | A | 8 | NULL | NULL | | BTREE | | |
| quota_usages | 1 | ix_quota_usages_project_id | 1 | project_id | A | 8 | NULL | NULL | YES | BTREE | | |
| quota_usages | 1 | ix_quota_usages_user_id_deleted | 1 | user_id | A | 8 | NULL | NULL | YES | BTREE | | |
| quota_usages | 1 | ix_quota_usages_user_id_deleted | 2 | deleted | A | 8 | NULL | NULL | YES | BTREE | | |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
4 rows in set (0.01 sec)
mysql> explain select * from quota_usages where project_id='foo' and user_id='bar' and deleted=0;
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
| 1 | SIMPLE | quota_usages | ref | ix_quota_usages_project_id,ix_quota_usages_user_id_deleted | ix_quota_usages_project_id | 768 | const | 1 | Using index condition; Using where |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
1 row in set (0.00 sec)
We should have an index on project_id/deleted and project_id/user_id/deleted instead of the current values."
1,"cloning a volume using the --source-volid argument is failing with the following in the volume.log file:
""""""
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf dd if=/dev/zero of=/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb count=1024 bs=1M conv=fdatasync
Exit code: 1
Stdout: ''
Stderr: ""/bin/dd: fdatasync failed for `/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb': Input/output error\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 19.7612 s, 54.3 MB/s\n""
""""""
NOTE: the actual dd from the origin snapshot into the destination volume is working; it is the cleanup of the origin snapshot which fails"
1,"target codes: (https://github.com/openstack/neutron/blob/master/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py)
  def provision_network(self, port_id, port_mac,
                          network_id, network_type,
                          physical_network, segmentation_id):
        LOG.info(_(""Provisioning network %s""), network_id)
        if network_type == constants.TYPE_VLAN:
            LOG.debug(_(""creating VLAN Network""))
        elif network_type == constants.TYPE_IB:
            LOG.debug(_(""creating IB Network""))
        else:
            LOG.error(_(""Unknown network type %(network_type) "" <======== miss a conversion type here like 's'
                        ""for network %(network_id)""), <======== miss a conversion type here like 's'
                      {'network_type': network_type,
                       'network_id': network_id})
            return
        data = {
            'physical_network': physical_network,
            'network_type': network_type,
            'ports': [],
            'vlan_id': segmentation_id}
        self.network_map[network_id] = data"
0,"When setting connection_trace=True, the stack trace does not get printed for DB2 (ibm_db).
I have a patch that we've been using internally for this fix that I plan to upstream soon, and with that we can get output like this:
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] SELECT services.created_at AS services_created_at, services.updated_at AS services_updated_at, services.deleted_at AS services_deleted_at, services.deleted AS services_deleted, services.id AS services_id, services.host AS services_host, services.""binary"" AS services_binary, services.topic AS services_topic, services.report_count AS services_report_count, services.disabled AS services_disabled, services.disabled_reason AS services_disabled_reason
FROM services WHERE services.deleted = ? AND services.id = ? FETCH FIRST 1 ROWS ONLY
2013-09-11 13:07:51,985 INFO sqlalchemy.engine.base.Engine (0, 3)
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] (0, 3)
        File /usr/lib/python2.6/site-packages/nova/servicegroup/drivers/db.py:92 _report_state() service.service_ref, state_catalog)
        File /usr/lib/python2.6/site-packages/nova/conductor/api.py:270 service_update() return self._manager.service_update(context, service, values)
        File /usr/lib/python2.6/site-packages/nova/openstack/common/rpc/common.py:420 catch_client_exception() return func(*args, **kwargs)
        File /usr/lib/python2.6/site-packages/nova/conductor/manager.py:461 service_update() svc = self.db.service_update(context, service['id'], values)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:505 service_update() with_compute_node=False, session=session)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:388 _service_get() result = query.first()"
1,"The bulk of the stack traces in n-cpu is because emit_event is getting triggered on a VM delete, however by the time we get to emit_event the instance is deleted (we see this exception 183 times in this log - which means it's happening on *every* compute terminate) so when we try to look up the instance we hit the exception found here:
    @base.remotable_classmethod
    def get_by_instance_uuid(cls, context, instance_uuid):
        db_obj = db.instance_info_cache_get(context, instance_uuid)
        if not db_obj:
            raise exception.InstanceInfoCacheNotFound(
                    instance_uuid=instance_uuid)
        return InstanceInfoCache._from_db_object(context, cls(), db_obj)
A log trace of this interaction looks like this:
2014-04-08 11:14:25.475 DEBUG nova.openstack.common.lockutils [req-fe9db989-416e-4da0-986c-e68336e3c602 TenantUsagesTestJSON-153098759 TenantUsagesTestJSON-953946497] Semaphore / lock released ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""75da98d7-bbd5-42a2-ad6f-7a66e38977fa"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""<function _lock_name at 0x41635f0>"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Semaphore / lock released ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.928 AUDIT nova.compute.manager [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Terminating instance
2014-04-08 11:14:25.989 DEBUG nova.objects.instance [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Lazy-loading `system_metadata' on Instance uuid 75da98d7-bbd5-42a2-ad6f-7a66e38977fa obj_load_attr /opt/stack/new/nova/nova/objects/instance.py:519
2014-04-08 11:14:26.209 DEBUG nova.network.api [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Updating cache with info: [VIF({'ovs_interfaceid': None, 'network': Network({'bridge': u'br100', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'10.1.0.2'})], 'version': 4, 'meta': {u'dhcp_server': u'10.1.0.1'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.4.4'})], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'10.1.0.1'})}), Subnet({'ips': [], 'version': None, 'meta': {u'dhcp_server': None}, 'dns': [], 'routes': [], 'cidr': None, 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'tenant_id': None, u'should_create_bridge': True, u'bridge_interface': u'eth0'}, 'id': u'9751787e-f41c-4299-be13-941c901f6d18', 'label': u'private'}), 'devname': None, 'qbh_params': None, 'meta': {}, 'details': {}, 'address': u'fa:16:3e:d8:87:38', 'active': False, 'type': u'bridge', 'id': u'db1ac48d-805a-45d3-9bb9-786bb5855673', 'qbg_params': None})] update_instance_cache_with_nw_info /opt/stack/new/nova/nova/network/api.py:74
2014-04-08 11:14:27.661 2894 DEBUG nova.virt.driver [-] Emitting event <nova.virt.event.LifecycleEvent object at 0x4932e50> emit_event /opt/stack/new/nova/nova/virt/driver.py:1207
2014-04-08 11:14:27.661 2894 INFO nova.compute.manager [-] Lifecycle event 1 on VM 75da98d7-bbd5-42a2-ad6f-7a66e38977fa
2014-04-08 11:14:27.773 2894 ERROR nova.virt.driver [-] Exception dispatching event <nova.virt.event.LifecycleEvent object at 0x4932e50>: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 597, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 500, in refresh
    self.info_cache.refresh()
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 103, in refresh
    self.instance_uuid)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 112, in wrapper
    result = fn(cls, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 70, in get_by_instance_uuid
    instance_uuid=instance_uuid)
InstanceInfoCacheNotFound: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
2014-04-08 11:14:27.840 2894 INFO nova.virt.libvirt.driver [-] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Instance destroyed successfully.
Raw logs for a failure: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=TRACE
Specific failure point: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-04-08_11_14_25_928"
1,"In the method '_sync_instance_power_state', the log info is wrong.
if self.host != db_instance.host:
            # on the sending end of nova-compute _sync_power_state
            # may have yielded to the greenthread performing a live
            # migration; this in turn has changed the resident-host
            # for the VM; However, the instance is still active, it
            # is just in the process of migrating to another host.
            # This implies that the compute source must relinquish
            # control to the compute destination.
            LOG.info(_(""During the sync_power process the ""
                       ""instance has moved from ""
                       ""host %(src)s to host %(dst)s"") %
                       {'src': self.host,
                        'dst': db_instance.host},
                     instance=db_instance)
            return
The 'src' value should be 'db_instance.host'and the 'dst' value should be the 'self.host'. The method '_post_live_migration' should be invoked after the live migration completes and it is used to update the database.
In the situation, the instance has been migrated to another host successfully and the database has not been updated. The '_sync_instance_power_state' method is executed. Nova can list it in the dst host with the driver and the data in the database should be the source host."
1,"I am running with the ZooKeeper servicegroup driver on CentOS 6.4 (Python 2.6) with the RDO distro of Grizzly.

All nova services are successfully connecting to ZooKeeper, which I've verified using zkCli.

However, when I run `nova service-list` I get an HTTP 500 error from nova-api. The nova-api log (/var/log/nova/api.log) shows:

2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack File ""/usr/lib/python2.6/site-packages/nova/servicegroup/api.py""\
, line 93, in service_is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack return self._driver.is_up(member)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 116, in is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack all_members = self.get_all(group_id)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 141, in get_all
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack raise exception.ServiceGroupUnavailable(driver=""ZooKeeperDrive\
r"")
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack ServiceGroupUnavailable: The service from servicegroup driver ZooK\
eeperDriver is temporarily unavailable.

The problem seems to be around evzookeeper (using version 0.4.0).

To isolate the problem, I added some evzookeeper.ZKSession synchronous get() calls to test the roundtrip communication to ZooKeeper. When I do a `self._session.get(CONF.zookeeper.sg_prefix)` in the zk.py ZooKeeperDriver __init__() method it works fine. The logs show that this is immediately before the wsgi server starts up.

When I do the get() operation from within the ZooKeeperDriver get_all() method, the web request hangs indefinitely. However, if I recreate the evzookeeper.ZKSession within the get_all() method (after the wsgi server has started) the nova-api request is successful.

diff --git a/nova/servicegroup/drivers/zk.py b/nova/servicegroup/drivers/zk.py
index 2a3edae..7de2488 100644
--- a/nova/servicegroup/drivers/zk.py
+++ b/nova/servicegroup/drivers/zk.py
@@ -122,7 +122,14 @@ class ZooKeeperDriver(api.ServiceGroupDriver):
         monitor = self._monitors.get(group_id, None)
         if monitor is None:
             path = ""%s/%s"" % (CONF.zookeeper.sg_prefix, group_id)
- monitor = membership.MembershipMonitor(self._session, path)
+
+ null = open(os.devnull, ""w"")
+ local_session = evzookeeper.ZKSession(CONF.zookeeper.address,
+ recv_timeout=
+ CONF.zookeeper.recv_timeout,
+ zklog_fd=null)
+
+ monitor = membership.MembershipMonitor(local_session, path)
             self._monitors[group_id] = monitor
             # Note(maoy): When initialized for the first time, it takes a
             # while to retrieve all members from zookeeper. To prevent"
0,"If the remote location fails to return the image and raises an error, the http store doesn't show it in the logs, which makes debugging very difficult:
https://github.com/openstack/glance/blob/master/glance/store/http.py
        # Check for bad status codes
        if resp.status >= 400:
            reason = _(""HTTP URL returned a %s status code."") % resp.status
            raise exception.BadStoreUri(loc.path, reason)"
1,"missing in following files:
    nova/virt/libvirt/config.py
    nova/virt/libvirt/driver.py
    nova/virt/libvirt/volume.py"
1,"The following patch https://github.com/openstack/nova/commit/4c2f36bfe006cb0ef89ca7a706223f30488a182e#diff-5c6ee11140977e63b54542e2ff5763d3R22 caused a regression by changing the eventlet.subprocess.Popen with the builtin subprocess.Popen (by using the nova.utils execute method) without changing the way the args were parsed.

In this module, the execution args were parsed separated by whitespaces, which is not allowed by the builtin subprocess.Popen, causing a ""not found"" error. This error is returned for example when attaching a volume, at the point where iscsicli tool is used to login the iSCSI target or portal.

Trace:
http://paste.openstack.org/show/79418/"
1,If import of interface_driver in lbaas namespace_driver fails then message for logger raises exception because of taking interface_driver from haproxy section while actual interface_driver is in default section.
1,"Attach a volume to instance fails with following exception :

014-10-10 19:29:54.112 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mThe instance: (Property){
   value = ""vm-2360""
   _type = ""VirtualMachine""
 } for which initialize connection is called, exists.[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:656[00m
2014-10-10 19:29:55.114 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mBacking exists[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:666[00m
2014-10-10 19:29:57.068 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mDatastore: (ManagedObjectReference){
   value = ""datastore-34""
   _type = ""Datastore""
 }, profile: None[00m [00;33mfrom (pid=10484) _relocate_backing /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:1934[00m
2014-10-10 19:29:59.015 [01;31mERROR cinder.volume.manager [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mUnable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
2014-10-10 19:29:59.017 [01;31mERROR oslo.messaging.rpc.dispatcher [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mException during message handling: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mTraceback (most recent call last):
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m incoming.message))
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m return self._do_dispatch(endpoint, method, ctxt, args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m result = getattr(endpoint, method)(ctxt, **new_args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m return f(*args, **kwargs)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m raise exception.VolumeBackendAPIException(data=err_msg)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mVolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m
2014-10-10 19:29:59.021 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mReturning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value' to caller[00m
2014-10-10 19:29:59.022 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31m['Traceback (most recent call last):\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n incoming.message))\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n return self._do_dispatch(endpoint, method, ctxt, args)\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n result = getattr(endpoint, method)(ctxt, **new_args)\n', ' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', ' File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection\n raise exception.VolumeBackendAPIException(data=err_msg)\n', ""VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'\n""][00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._publish_service_capabilities[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.manager [[00;36m-[00;32m] [01;35m[00;32mNotifying Schedulers of capabilities ...[00m [00;33mfrom (pid=10484) _publish_service_capabilities /opt/stack/cinder/cinder/manager.py:128[00m
2014-10-10 19:30:02.552 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._report_driver_status[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.553 [00;36mINFO cinder.volume.manager [[00;36m-[00;36m] [01;35m[00;36mUpdating volume status[00m"
1,"currently some plugins always notify the agent of a port_update even if the allowed address pairs attribute is specified: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L600
However, if there's no change in allowed address pairs there is no need for triggering a notification
Affected plugins:
ml2
openvswitch
nec"
1,"When i use `neutron-ns-metadata-proxy --help`, some option doesn't have help text. It is because in neutron/agent/metadata/namespace_proxy.py:
opts = [
        cfg.StrOpt('network_id'),
        cfg.StrOpt('router_id'),
        cfg.StrOpt('pid_file'),
        cfg.BoolOpt('daemonize', default=True),
        cfg.IntOpt('metadata_port',
                   default=9697,
                   help=_(""TCP Port to listen for metadata server ""
                          ""requests."")),
    ]"
1,"When I update a service with not existing hostname or binary , I got the 500 error in nova api log.
2014-01-28 03:15:29.829 ERROR nova.api.openstack.extensions [req-5b1f3fc5-349a-4415-a4f5-63eab1c259a0 admin demo] Unexpected exception in API method
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 470, in wrapped
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/services.py"", line 172, in update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions self.host_api.service_update(context, host, binary, status_detail)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 3122, in service_update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/objects/base.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions result = fn(cls, context, *args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/objects/service.py"", line 105, in get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions db_service = db.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/db/api.py"", line 131, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions return IMPL.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 469, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions raise exception.HostBinaryNotFound(host=host, binary=binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions HostBinaryNotFound: Could not find binary nova-cert on host xu-de.
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions"
0,"In current default glance-api.conf, messaging configurations as below, but actually 'rabbit_notification_exchange = glance' and 'qpid_notification_exchange = glance' do not impact topic consumer_queue creation. because Oslo .messaging uses 'control_exchange' as queue name, default value is 'openstack'. other component such as cinder has written ''control_exchange=cinder'' into cinder conf. glance should do same change as well.

# Messaging driver used for 'messaging' notifications driver
# rpc_backend = 'rabbit'

# Configuration options if sending notifications via rabbitmq (these are
# the defaults)
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False

# Configuration options if sending notifications via Qpid (these are
# the defaults)
qpid_notification_exchange = glance
qpid_notification_topic = notifications
qpid_hostname = localhost
qpid_port = 5672
qpid_username =
qpid_password =
qpid_sasl_mechanisms =
qpid_reconnect_timeout = 0
qpid_reconnect_limit = 0
qpid_reconnect_interval_min = 0
qpid_reconnect_interval_max = 0
qpid_reconnect_interval = 0"
0,"In order for the Fibre Channel Zone Manager to automatically zone up the endpoints (initiator and target) for the Fibre Channel fabric, the 3PAR driver needs to return the initiator_target_map in initialize_connection and terminate_connection"
0,"The method failUnlessAlmostEqual has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using failUnlessAlmostEqual therefore we should use assertAlmostEqual instead."
0,"I created a volume from an image and booted an instance from it
when instance boots I get this: 'selected cylinder exceeds maximum supported by bios'
If I boot an instance from the same image I can boot with no issues so its just booting from the volume."
0,"When updating network quota using the following command:

neutron quota-update --network 10000000000

the client ouputs:

""Request Failed: internal server error while processing your request.""

This request fails since the parameter exceeds the integer range. An error message like ""Request Failed: quota limit exceeds integer range"" would be more friendly to users than just raising a single internal server error."
0,"The Midonet DhcpNoOpDriver currently references device_delegate, which is now named device_manager. This bug is to correct the naming."
1,"When OVS agent starts, it will send server a message with ""start_flag"" to tell server it's up, which is done by the _report_state method. Currently _report_state uses cast method to send rpc message, so there's no guarantee that server receives the ""start"" message. In our test, sometimes this message did miss. As a result, server didn't update the start time of the agent so fdb entries didn't send by the l2pop driver."
0,"Testcase test_import_record_with_verify uses 'FakeBackupService' but there is no
verfiy function implemented in that class."
1,"I'm seeing an error in the l2pop code where it's failing to add a flow for the ARP entry responder.
This is sometimes leading to DHCP failures for VMs, although a soft reboot typically fixes that problem.
Here is the trace:
2014-09-10 15:10:36.954 9351 ERROR neutron.agent.linux.ovs_lib [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 None] OVS flows could not be applied on bridge br-tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib Traceback (most recent call last):
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib return method(*args, **kwargs)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib for mac, ip in after:
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib
2014-09-10 15:10:36.955 9351 ERROR oslo.messaging.rpc.dispatcher [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Exception during message handling: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 55, in update_fdb_entries
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher self.fdb_update(context, fdb_entries)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 212, in fdb_update
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher getattr(self, method)(context, values)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher for mac, ip in after:
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher
2014-09-10 15:10:36.957 9351 ERROR oslo.messaging._drivers.common [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Returning exception 'NoneType' object is not iterable to caller
I don't know this code well enough to suggest a fix - whether it's checking the return from agent_ports.items() better, or that there is a bug elsewhere, so any help would be appreciated."
0,Radware LBaaS driver should transfer the tenant-id to the back system when creating an ADC service
0,/opt/stack/swift/swift/common/utils.py:docstring of swift.common.utils.parse_content_type:7: ERROR: Unexpected indentation.
1,"In certain scenarios explained below, floating ip association
and deletion does not work correctly
1) Create a neutron port and assign a floating ip to it. Now boot
     a vm using the neutron port. The vm boots correctly but
     the floating ip is not assigned to the vm.
2) Boot a vm and assign a floating ip to it. Now delete the
     floating ip. The floating ip is disassociated from the vm
     and deleted from neutron but is not deleted from Nuage
     VSD."
