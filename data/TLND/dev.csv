BIC,description
1,The nova notification patch introduces a hard dependency on novaclient when it is a runtime-configurable dependency. The import from novaclient should be conditional on the appropriate nova notification options being enabled in the config.
0,"Cinder volume service traces with the following error when lvchange is not in $PATH. As lvchange is in /sbin on SLE11, and /sbin is not part of the path for non-root, the test aborts with this trace:
2014-01-15 13:55:39.292 9325 ERROR cinder.volume.flows.create_volume [req-04565125-c12a-4081-b7dd-93fe0afc7e17 a0f6c8d7d68241f3bd6d1768775dc239 9d341e8aeb794086894e49e132766cd0] Unexpected build error:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume Traceback (most recent call last):
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume result = runner(context, *args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume self.result = self.task(*args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1499, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume **volume_spec)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1300, in _create_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume snapshot_ref)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/volume/drivers/lvm.py"", line 176, in create_volume_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume self.vg.activate_lv(snapshot['name'], is_snapshot=True)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 487, in activate_lv
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume if self.supports_lvchange_ignoreskipactivation:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 190, in supports_lvchange_ignoreskipactivation
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume (out, err) = self._execute(*cmd)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume return processutils.execute(*cmd, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 158, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume shell=shell)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/site-packages/eventlet/green/subprocess.py"", line 44, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume subprocess_orig.Popen.__init__(self, args, 0, *argss, **kwds)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/subprocess.py"", line 623, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume errread, errwrite)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume File ""/usr/lib64/python2.6/subprocess.py"", line 1141, in _execute_child
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume raise child_exception
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume OSError: [Errno 13] Permission denied
Solution/workaround is to run it via the root wrapper, which has /sbin in its trusted paths."
0,"https://review.openstack.org/#/c/58017 removes the need to explicitly call _recycle_ip. More specifically, it makes _recycle_ip a simple pass-through to _delete_ip_allocation. A follow-on needs to remove _recycle_ip and replace calls with _delete_ip_allocation."
1,"Version : Havana
OS : RHEL
Running Neutron services : neutron-server, neutron-openvswitch-agent, neutron-l3-agent
No routers being created, then the following exception will be raised.
2013-09-16 08:51:32.820 23737 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp **args)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/db/l3_rpc_base.py"", line 54, in sync_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp l3plugin.auto_schedule_routers(context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/db/l3_agentschedulers_db.py"", line 241, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp self, context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp context.session.add(binding)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449, in __exit__
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp self.commit()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp self._prepare_impl()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340, in _prepare_impl
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp self.session.flush()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp raise exception.DBError(e)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('28570d41-e7ab-4ca5-bed5-922bfbc154fc', '', '0214005e-8876-4bb1-a3b5-68f629b6e987')
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp
The root cause is that the router_ids transferred is defined as [''], which is not None, but have an invalid empty value, the DB can't take this value in."
1,"create_port can fail with a Lock wait timeout error.
The transaction performed in create_port/update_port makes a call to _process_port_bindings which then calls dvr_update_router_addvm. The notification made within dvr_update_router_addvm can hang. The transaction lock is held through the entire hang preventing other create_port threads from getting the lock and then timing out with a ""Lock wait timeout.""
2014-09-19 04:16:53.332 3391 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ipavailabilityranges.allocation_pool_id AS ipavailabilityranges_allocation_pool_id, ipavailabilityranges.first_ip AS ipavailabilityranges_first_ip, ipavailabilityranges.last_ip AS ipavailabilityranges_last_ip \nFROM ipavailabilityranges INNER JOIN ipallocationpools ON ipallocationpools.id = ipavailabilityranges.allocation_pool_id \nWHERE ipallocationpools.subnet_id = %s \n LIMIT %s FOR UPDATE' ('550ee3a3-6c4d-4a4a-b173-b7603e43356a', 1)"
1,"The snapshot_delete operation will fail if the backing file for the snapshot does not exist. This happens in legitimate cases such as when snapshot_create fails due to permissions problems.
The driver should allow the manager to delete the snapshot in this case where this is no action required for the driver to delete anything."
1,"When making an http request against the OpenStack metadata service, it returns ""Content-Type: text/html"", despite the returned content being in json format.
For example (using httpie):
$ http get http://169.254.169.254/openstack/latest/meta_data.json
HTTP/1.1 200 OK
Content-Length: 1283
Content-Type: text/html; charset=UTF-8
Date: Wed, 23 Oct 2013 18:55:17 GMT
{""random_seed"": ... }
The returned content type should really be something like:
Content-Type: application/json; charset=UTF-8
Version: grizzly"
0,"Currently volume attach, detach, and swap check on vm_state but not task_state. This means that, for example, volume attach is allowed during a reboot, rebuild, or migration.
As with other operations the check should be against a task state of ""None"""
1,"A GET on imagedata in glance v2 returns 404 instead of 204 when data does not exist.
See http://docs.openstack.org/api/openstack-image-service/2.0/content/get-image-file.html
curl -i -X GET -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" http://localhost:9292/v2/images/e3a8964d-b320-450d-a729-1735dac05ba9/file
HTTP/1.1 404 Not Found
Content-Length: 150
Content-Type: text/html; charset=UTF-8
X-Openstack-Request-Id: req-fcc598de-6012-4576-a74b-833bfd6e1299
Date: Wed, 13 Nov 2013 20:51:31 GMT
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  No image data could be found<br /><br />
 </body>
</html>"
1,"If the build process makes it to build_and_run_instance in the compute manager no instance faults are recorded for failures after that point. The instance will be set to an ERROR state appropriately, but no information is stored to return to the user."
1,"Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver raise NotImplementedError from the plug_vifs method which causes this failure on the compute node when it starts up:
http://paste.openstack.org/show/45241/
Looking at other virt drivers that don't support the plug_vifs method (hyperv and vmware), they simply pass, which is what the powervm driver use to do before that change.
The powervm driver should either revert to pass on plug_vifs or the nova compute manager should catch NotImplementedError from driver.plug_vifs and swallow it here:
https://github.com/openstack/nova/blob/2013.2.b2/nova/compute/manager.py#L638"
1,"Bugs reproduce process:
1. create a firewall rule and attache it to a firewall policy
2. create two firewalls with the firewall policy attached alternatively on two routers
3. remove the firewall rule from the firewall policy
it would occur the following error:
 Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
    result = method(request=request, **args)
  File ""/home/stack/neutron/neutron/api/v2/base.py"", line 201, in _handle_action
    return getattr(self._plugin, name)(*arg_list, **kwargs)
  File ""/home/stack/neutron/neutron/plugins/vmware/plugins/service.py"", line 1077, in remove_rule
    context, fwr['id'], edge_id)
  File ""/home/stack/neutron/neutron/plugins/vmware/vshield/edge_firewall_driver.py"", line 270, in delete_firewall_rule
    vcns_rule_id = rule_map.rule_vseid
AttributeError: 'NoneType' object has no attribute 'rule_vseid'
2014-05-22 16:21:22,244 INFO [neutron.plugins.vmware.vshield.tasks.tasks] TaskManager terminated
}}}
Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/tests/unit/vmware/vshield/test_fwaas_plugin.py"", line 650, in test_remove_rule_with_firewalls
    expected_body=attrs)
  File ""/home/stack/neutron/neutron/tests/unit/db/firewall/test_db_firewall.py"", line 295, in _rule_action
    self.assertEqual(res.status_int, expected_code)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 322, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 412, in assertThat
    raise MismatchError(matchee, matcher, mismatch, verbose)
MismatchError: 500 != 200
It is because when deleting the corresponding vcns_edge_firewallrule_binding entry, it query based on id instead of (edge_id, id) which leads to deleting the other rule_binding entry."
1,"$ ./run_tests.sh
No virtual environment found...create one? (Y/n) y
Traceback (most recent call last):
  File ""tools/install_venv.py"", line 29, in <module>
    from tools import install_venv_common as install_venv
ImportError: No module named tools
Change ""Enable H304 check"" (I099ed65db9b42223eaa4b66a3a5c6113d1cc56fe) causes it."
0,"When an instance is not found by libvirtd, _lookup_by_name in libvirt
driver raises InstanceNotFound.

It's better to catch this exception instead of NotFound who is the father
of InstanceNotFound."
1,"http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/logs/screen-q-svc.txt.gz
Shows a constraint failure, which is almost always evidence of some kind of race/poor error handling:
2013-08-27 18:45:06.141 29478 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp **args)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp neutron_ctxt, version, method, namespace, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp plugin.auto_schedule_routers(context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp self, context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp context.session.add(binding)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp self.commit()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp self._prepare_impl()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp self.session.flush()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp raise exception.DBError(e)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] Returning exception (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a') to caller
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', ' File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data\n **args)\n', ' File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch\n neutron_ctxt, version, method, namespace, **kwargs)\n', ' File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n result = getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers\n plugin.auto_schedule_routers(context, host, router_ids)\n', ' File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers\n self, context, host, router_ids)\n', ' File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers\n context.session.add(binding)\n', ' File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__\n self.commit()\n', ' File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit\n self._prepare_impl()\n', ' File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl\n self.session.flush()\n', ' File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap\n raise exception.DBError(e)\n', ""DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')\n""]
Full gate logs are here:
http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/"
1,"I am running Havana from precise-proposed in the UCA (nova 1:2013.2~b3-0ubuntu1~cloud0).

To reproduce:

- start an instance
- reboot (sudo reboot) the compute node on which it is running
- after the compute node is done booting, the instance will be off:

root@xen10:~# nova list
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| 4824dce8-d876-4022-a446-3fc8d708ac62 | test | SHUTOFF | None | Shutdown | novanetwork=172.20.46.3 |
+--------------------------------------+------+---------+------------+-------------+-------------------------+

(note that although my hostname has ""xen"" in it, I'm using KVM. Haven't updated DNS yet...)

- attempt to reboot the instance (nova reboot 4824dce8-d876-4022-a446-3fc8d708ac62)

# nova show 4824dce8-d876-4022-a446-3fc8d708ac62
+--------------------------------------+----------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------+
| status | SHUTOFF |
| updated | 2013-10-08T15:28:47Z |
| OS-EXT-STS:task_state | rebooting |

The reboot fails. The compute node will log:

2013-10-08 11:28:55.579 1400 WARNING nova.compute.manager [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] trying to reboot a non-running instance: (state: 4 expected: 1)

- attempt to start the instance (nova start 4824dce8-d876-4022-a446-3fc8d708ac62):

produces console output:
ERROR: Instance 4824dce8-d876-4022-a446-3fc8d708ac62 in task_state rebooting. Cannot start while the instance is in this state. (HTTP 400) (Request-ID: req-732224e1-8c34-4754-84f7-7a8476673185)

- wait about 120 seconds, and the compute node will log:
2013-10-08 11:30:56.082 1400 WARNING nova.virt.libvirt.driver [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] Failed to soft reboot instance. Trying hard reboot.

Afterwards, the instance will be running.

It's confusing that the reboot logs a failure for a very obvious reason (an instance that is not running can't be *re*booted), yet the instance's state remains as ""rebooting"". I had expected that the reboot had failed, and openstack was in some consistant state. I was then again suprised when in fact it *was* still rebooting -- it just took two minutes to do so. Less confusing would be to catch the original error, and report the reboot as failed. The log messages are confusing, because the first sets the expectation that a non-running instance can't be rebooted, but it can (two minutes later)."
1,"cinder/db/sqlalchemy/migrate_repo/manage.py is unusable
It's not possible to use manage.py in its current state due to an exception thrown by oslo.config:
  oslo.config.cfg.ArgsAlreadyParsedError:
    arguments already parsed: cannot register CLI option
This exception is a side-effect of including cinder.openstack.common.logging in most of the migration scripts."
0,"Using all default setting for setting up devstack. Once devstack is done, g-reg and g-api console will show same error like this.

KeyError: 'user_identity'

18c2599c92e1db002 with project_id : a068a8a689554202999265989f44f448 and roles: admin _build_user_headers /opt/stack/py
thon-keystoneclient/keystoneclient/middleware/auth_token.py:951
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 684, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 648, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'"
0,"The VolumeOpsTestCase class in test_hypervapi.py inherits from HyperVAPITestCase, resulting in all the tests from the base class being executed twice.
The patch that introduced the VolumeOpsTestCase is here:
https://github.com/openstack/nova/commit/d143540ad1b69ec93c2b7bfadd1f654c4d8c7a34"
1,"The URL used in the ""full sync"" operation of the ML2 mechanism driver is incorrect due to a string concatenation bug."
1,"Incorrectly says 'image' instead of 'image member'
curl -i -X PUT -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" https://localhost/v2/images/ef4570bf-2e26-4921-810a-5f8499e9822f/members/5855250 -d '{""status"": ""accepted""}'
HTTP/1.1 403 Forbidden
Content-Type: text/html;charset=UTF-8
Via: 1.1 Repose (Repose/2.12)
Content-Length: 177
Date: Fri, 08 Nov 2013 19:13:37 GMT
x-openstack-request-id: req-11c02e0c-d5bd-4f17-ab03-b474b071b3f0
Server: Jetty(8.0.y.z-SNAPSHOT)
<html>
<head>
<title>403 Forbidden</title>
</head>
<body>
<h1>403 Forbidden</h1>
You are not permitted to modify 'status' on this image.<br /><br /> <------------------this should say 'image member'
The issue is in ImmutableMemberProxy in glance/api/authorization.py where it uses the _immutable_attr function."
0,"Several issues to address:
- need better error handling for the add,get,get_size,delete operations: need to catch exception when httplib call fails, also need to log when the response is not expected.
- need to handle cases where the store_image_dir contains non expected characters. It should support the following use cases:
/openstack_glance
openstack_glance
openstack_glance/
openstack glance -> this one should fail with logging
openstack+glance
etc.
- need to quote special characters"
0,"The agent for SDN-VE plugin does not show its state periodically and the plugin does not support the ""agent"" extension. For conformity wit Neutron agents, this needs to be added to the plugin and the agent."
1,"In /nova/compute/manager.py/def _allocate_network_async,line 1559.
attempts = retries > 1 and retries + 1 or 1
retry_time = 1
for attempt in range(1, attempts + 1):
Variable attempts wants to determine the retry times of allocate network,but it made a small mistake.
See the Simulation results below:
retries=0,attempts=1
retries=1,attempts=1
retries=2,attempts=3
When retries=1, attempts=1 ,It actually does not retry."
1,"Currently socket options, socket.SO_REUSEADDR and socket.SO_KEEPALIVE
are set only if SSL is enabled.
Ref: https://github.com/openstack/cinder/blob/master/cinder/wsgi.py#L209
The above socket options should be set no matter SSL is enabled or not.
This issue is introduced in patch https://review.openstack.org/#/c/106377/"
1,"When cloud-init is running metadata-agent sends a lot of same requests to neutron-server. These requests serves for obtaining instance_id and tenant_id of running instance. When higher amount of instances is started at once mentioned requests cause high load for neutron-server.
Requests could be cached with small ttl."
0,"Some unit tests override notification driver, but doesn't clean it up properly.
So tests can be run against unintended notification driver resulting in error."
1,"cinder.volume.API has typo in its method name.
'_valid_availabilty_zone' should be '_valid_availability_zone'."
1,"The size of the console log can get bigger than expected because of a small nit when checking the existing log file size as well
as a wrong size constant.
The method which gets the serial port pipe at the moment returns a list which contains at most one element being the actual pipe path. In order to avoid confusion this should return the pipe path or None instead of a list."
0,"Currently, test_copy_to_file test requires swift and / or s3 to test this functionality.
Swift / S3 tests belong to their stores and test_copy_to_file should use a HTTP instance or whatever, to test this functionality.
https://github.com/openstack/glance/blob/master/glance/tests/functional/v1/test_copy_to_file.py"
1,"When old flavor's root_gb is not equal 0 and new flavor's root_gb is 0, the resize() in nova.compute.api will raise CannotResizeDisk.
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2368
    def resize(self, context, instance, flavor_id=None,
        if not flavor_id:
            LOG.debug(""flavor_id is None. Assuming migration."",
                      instance=instance)
            new_instance_type = current_instance_type
        else:
            new_instance_type = flavors.get_flavor_by_flavor_id(
                    flavor_id, read_deleted=""no"")
            if (new_instance_type.get('root_gb') == 0 and
                current_instance_type.get('root_gb') != 0):
                reason = _('Resize to zero disk flavor is not allowed.')
                raise exception.CannotResizeDisk(reason=reason)"
0,"After upgrading my machine to fedora 20 I noticed that some of our tests were failing. Then with a bit more investigation I saw that it was related to the version of the sqlite.
The problem seems to be already fixed in oslo by the review https://review.openstack.org/#/c/61405 . We need to sync the new version of the db.sqlalchemy module from oslo to ironic in order to get it fixed in Ironic as well.
LOG:
(venv)[lucasagomes@lucasagomes ironic]$ rpm -q sqlite
sqlite-3.8.2-1.fc20.x86_64
Example of broken test:
FAIL: ironic.tests.conductor.test_manager.ManagerTestCase.test_start_registers_driver_names
tags: worker-5
----------------------------------------------------------------------
Empty attachments:
  pythonlogging:''
  stdout
stderr: {{{
ironic/openstack/common/db/sqlalchemy/session.py:486: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6
  m = _DUP_KEY_RE_DB[engine_name].match(integrity_error.message)
}}}
Traceback (most recent call last):
  File ""ironic/tests/conductor/test_manager.py"", line 73, in test_start_registers_driver_names
    self.service.start()
  File ""ironic/conductor/manager.py"", line 106, in start
    'drivers': self.drivers})
  File ""ironic/objects/__init__.py"", line 28, in wrapper
    result = fn(*args, **kwargs)
  File ""ironic/db/sqlalchemy/api.py"", line 521, in register_conductor
    conductor.save()
  File ""ironic/openstack/common/db/sqlalchemy/models.py"", line 53, in save
    session.flush()
  File ""ironic/openstack/common/db/sqlalchemy/session.py"", line 551, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) UNIQUE constraint failed: conductors.hostname u'INSERT INTO conductors (created_at, updated_at, hostname, drivers) VALUES (?, ?, ?, ?)' ('2014-01-24 19:43:43.362929', '2014-01-24 19:43:43.362563', 'test-host', '[""fake3"", ""fake4""]')"
1,"The call to get_instance_nw_info fails with an error if a network is deleted during the deallocation. Networks are not supposed to be able to be deleted if they have fixed ips in use, but there is a race where a network can be deleted while an allocation is still in process. This is make many times worse by the fact that get_instance_nw_info makes 3*networks + 1 (db + rpc) calls. This should be converted to a) get everything in a single db request, b) handle networks not existing gracefully.
The traceback from get_nw_info failing can look like:
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher address, instance=instance)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher instance_uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher None, None)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher return func(*args, **kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 602, in get_instance_nw_info
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher rxtx_factor, host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 625, in build_network_info_model
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher instance_host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 701, in _get_subnets_from_network
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher network['project_id'], network['uuid'], vif.uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/nova_ipam_lib.py"", line 46, in get_subnets_by_net_id
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher n = network_obj.Network.get_by_uuid(context.elevated(), net_id)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher args, kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher wait_for_reply=True, timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher raise result
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFoundForUUID_Remote: Network could not be found for uuid 3af0a6e6-59f8-4d7e-90ec-b5b866f578f8
or:
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher address, instance=instance)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher instance_uuid)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher None, None)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher return func(*args, **kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 598, in get_instance_nw_info
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher network = self._get_network_by_id(context, vif.network_id)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 1445, in _get_network_by_id
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher project_only='allow_none')
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher args, kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher wait_for_reply=True, timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher raise result
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFound_Remote: Network 23 could not be found.
depending on where in the call stack the race occurs."
1,"When unshelving an offloaded instance, a new host is scheduled for the instance. However, the call to unshelve the instance on the new host does not go through the resource tracker, which may lead to over-allocation of the hosts resources."
1,"If we are using openvswitch in a system with a newer kernel (3.13/trusty) it should have the features required for neutron and not require an openvswitch dkms package. Therefore we should be able to use the native module.
In neutron/agent/linux/ovs_lib.py:
def get_installed_ovs_klm_version():
    args = [""modinfo"", ""openvswitch""]
    try:
        cmd = utils.execute(args)
        for line in cmd.split('\n'):
            if 'version: ' in line and not 'srcversion' in line:
                ver = re.findall(""\d+\.\d+"", line)
                return ver[0]
    except Exception:
        LOG.exception(_(""Unable to retrieve OVS kernel module version.""))
So if we run modinfo on a system without a DKMS package we get:
$ modinfo openvswitch
filename: /lib/modules/3.13.0-16-generic/kernel/net/openvswitch/openvswitch.ko
license: GPL
description: Open vSwitch switching datapath
srcversion: 1CEE031973F0E4024ACC848
depends: libcrc32c,vxlan,gre
intree: Y
vermagic: 3.13.0-16-generic SMP mod_unload modversions
signer: Magrathea: Glacier signing key
sig_key: 1A:EE:D8:17:C4:D5:29:55:C4:FA:C3:3A:02:37:FE:0A:93:44:6D:69
sig_hashalgo: sha512
Because 'version' isn't provided we need an alternative way of checking if the openvswitch module has the required features."
1,"When an IPv6 subnet is created, with ipv6_ra_mode not set, and ipv6_address_mode set to ""slaac"" - ipv6 addresses are added to the host file that dnsmasq uses, and this causes dnsmasq to cease responding to dhcp clients on the v4 side."
1,"Listing volumes with a non-admin user has poor performance.
This is caused by the query built to retrieve glance metadata associated to a volume which is sub-optimal: all rows of volume_glance_metadata table are returned."
1,"The case on our system where create two hosts, one for fc and the other is iscsi
larry@larry-ubuntu:~$ ssh superuser@9.115.247.251 'lshost'
superuser@9.115.247.251's password:
id name port_count iogrp_count status
...
7 OpenstackUbun-28670410 1 4 offline
...
9 OpenstackUbun-20533817 2 4 degraded
larry@larry-ubuntu:~/Desktop$ less -R screen-c-vol.2014-04-16-162932.log
larry@larry-ubuntu:~/Desktop$ ssh superuser@9.115.247.251 'lsvdiskhostmap volume-d86758fb-190a-44b9-9442-845c8af93d16'
superuser@9.115.247.251's password:
id name SCSI_id host_id host_name vdisk_UID IO_group_id IO_group_name
162 volume-d86758fb-190a-44b9-9442-845c8af93d16 0 7 OpenstackUbun-28670410 60050760008F03000C00000000000104 0 io_grp0
2014-04-18 13:55:45.683 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-e31cb4b8-cd98-4415-a8cf-ca525e416524 cbf3ff0436ed4c13a9da2fb2eb4e9277 36e2e0918ba54edaa4ce561926870bca] CLI Exception output:
 command: ['svctask', 'rmvdiskhostmap', '-host', '""OpenstackUbun-20533817""', u'volume-d86758fb-190a-44b9-9442-845c8af93d16']
 stdout:
 stderr: CMMVC5842E The action failed because an object that was specified in the command does not exist.
when detach the volume, find the host logic error."
1,"/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user, I am unable to delete a volume if it was not attached before.
the volume status became: error_deleting
$ cinder create 1
$ cinder delete daf0a29a-542b-4571-9d4d-5b9b59a7baf3
$ cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| daf0a29a-542b-4571-9d4d-5b9b59a7baf3 | error_deleting | None | 1 | None | false | |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
../screen-logs/screen-c-vol.log:
2014-03-31 10:16:43.863 787 ERROR oslo.messaging.rpc.dispatcher [req-8f294457-08a3-4039-9626-ce63a2435bed 46a5bd04d46f49aabcf9edf8214df7a1 d587e4e2b14a41e4b38f62b07a130b3e - - -] Exception during message handling: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 144, in lvo_inner1
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher return lvo_inner2(inst, context, volume_id, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher retval = f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 143, in lvo_inner2
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher return f(*_args, **_kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 416, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher {'status': 'error_deleting'})
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 400, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher self.driver.remove_export(context, volume_ref)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher self.target_helper.remove_export(context, volume)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher volume['id'])
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 119, in wrapper
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher return f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1344, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher raise exception.ISCSITargetNotFoundForVolume(volume_id=volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher ISCSITargetNotFoundForVolume: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher
When the lioadm is the iscsi helper the target just exists when the volume attached.
I should be able to delete a volume what was never attached.
The volumes which was attached to a vm before the delete request are deletable."
1,"Change https://review.openstack.org/105411 introduced the following incorrect server_default value:
    multicast_ip_index = sa.Column(sa.Integer, default=0,
                                   server_default=sql.false())
in neutron/plugins/cisco/db/n1kv_models_v2.py for table cisco_network_profiles."
1,"Firewall policy update should validate rules as list of uuids, otherwise malformed request will result in ""500 Internal server error"" returned to the client."
1,"When an exception ProjectUserQuotaNotFound occurs, the message ""Quota
could not be found"" was obtained. I think that expecting ""Quota for
user XXX in project YYY could not be found."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 480, in test_get_by_project_and_user_with_wrong_resource
    'fake_user', 'wrong_resource')
  File ""/opt/stack/nova/nova/quota.py"", line 1066, in get_by_project_and_user
    user_id, resource)
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 247, in get_by_project_and_user
    user_id=user_id)
ProjectUserQuotaNotFound: Quota for user fake_user in project test_project could not be found.
As a similar problem, when an exception ConsoleTypeUnavailable occurs,
the message ""Unacceptable parameters."" was obtained. I think that
expecting ""Unavailable console type XXX."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 5020, in test_get_spice_console_unavailable
    conn.get_spice_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2155, in get_spice_console
    ports = get_spice_ports_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2153, in get_spice_ports_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='spice')
ConsoleTypeUnavailable: Unavailable console type spice.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 4979, in test_get_vnc_console_unavailable
    conn.get_vnc_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2135, in get_vnc_console
    port = get_vnc_port_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2133, in get_vnc_port_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='vnc')
ConsoleTypeUnavailable: Unavailable console type vnc."
0,Use dict_extend_functions mechanism to handle populating additional provider network attributes into Network model.
1,"Some uncaught libvirt errors may result in instances being set to ERROR state and is causing sporadic gate failures. This can happen for any of the code paths that use _destroy(). Here is a recent example of a failed resize:
[req-06dd4908-382e-455e-854e-e4d42a4bf62b TestServerAdvancedOps-724416891 TestServerAdvancedOps-711228572] [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Setting instance vm_state to ERROR
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Traceback (most recent call last):
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5902, in _error_out_instance_on_exception
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] yield
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/compute/manager.py"", line 3658, in resize_instance
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5468, in migrate_disk_and_power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] self.power_off(instance, timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2400, in power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] self._destroy(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 998, in _destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] timer.start(interval=0.5).wait()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 121, in wait
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] return hubs.get_hub().switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 293, in switch
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] return self.greenlet.switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/openstack/common/loopingcall.py"", line 81, in _inner
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] self.f(*self.args, **self.kw)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 971, in _wait_for_destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] dom_info = self.get_info(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3922, in get_info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] dom_info = virt_dom.info()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 183, in doit
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 141, in proxy_call
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] rv = execute(f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 122, in execute
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] six.reraise(c, e, tb)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 80, in tworker
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] rv = meth(*args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 1068, in info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] if ret is None: raise libvirtError ('virDomainGetInfo() failed', dom=self)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] libvirtError: Domain not found: no domain with matching uuid '525f4f95-f631-4fbb-a884-20c37711fb0d' (instance-00000097)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]"
0,"oslo.db may set additional options to engines that we may be interested in.
This will also ease the switch to mysql-connector if that gets approved."
0,"Passing a scheduler hint for the GroupAntiAffinityFilter no longer works:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
ERROR (NotFound): The resource could not be found. (HTTP 404) (Request-ID: req-21430f41-e6ca-46db-ab5c-890a1d1dbd01)
screen-n-api.log contains message:
Caught error: Instance group foo could not be found."
1,"1.Create metadata for a snapshot with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the snapshot, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the snapshot
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}"
1,"The patch that cleans up floating ips on router deletion [1] has triggered a race condition that causes spurious failures in the dvr job in the check queue. Reverting this patch [2] has shown to stabilize it.
[1] https://review.openstack.org/#/c/120885/
[2] https://review.openstack.org/#/c/121729/"
1,"In the compute manager's 'retry' logic (i.e., handling RescheduledException) in _build_and_run_instance, it has a catch-all case for all exceptions and sets reason=str(e). This works well in most cases, but for cases in which lower level code may generate locale-specific messages, it's possible to see the ""UnicodeEncodeError: 'ascii' codec can't encode character..."" error, which ultimately masks the message in the compute logs, etc.
It also means the instance won't be rescheduled as it fails with an error similar to...
WARNING nova.compute.manager [req-7fe662b1-c947-4303-b43f-114fbdafe875 None] [instance: 333474eb-dd07-4e05-b920-787da0fd5b32] Unexpected build failure, not rescheduling build.
(i.e., as a result of the UnicodeEncodeError previously mentioned)
The simple solution is to use six.text_type(e) instead of str(e)."
1,"Perform the following:
- Create a network
- Create two subnets on the network
- Create a port on the network using a fixed IP from one of the subnets
- Delete the other subnet
= = = > FAILURE: Subnet delete fails because SUPPOSEDLY there
are port(s) still associated with that subnet.
Looking at delete_subnet() in neutron/db/db_base_plugin_v2.py,
the check for port(s) still being associated with that _subnet_
is actually checking for port(s) still being associated with
the _network_ (not the subnet), i.e. it's doing a:
    filter_by(network_id=subnet.network_id)
rather than a:
    filter_by(subnet_id=subnet['id'])"
1,"When boot from volume and the volume is created from a image, the original image's min_ram, min_disk attributes are ignored, this is not good.
The reason of this failure is because the _check_requested_image() in compute/api.py ignore if the source if a volume."
0,"Attempting a HEAD request on the /images/detail resource results in a 500 response. This should ideally be an HTTP 405 response.
This issue does not occur when doing a HEAD on /images for some reason
curl -i -X HEAD -H ""X-Auth-Token: $AUTH_TOKEN"" -H 'Content-Type: application/json' -H 'User-Agent: python-glanceclient' http://localhost:9292/v1/images/detail
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Tue, 25 Mar 2014 15:18:34 GMT
Connection: close
The traceback for the error can be found here: http://paste.openstack.org/show/74261/"
0,The VMware store needs to be added to strategy module: https://github.com/openstack/glance/blob/9567c2b6a06aa1e8205f9f30beca63d77500dd1d/glance/common/location_strategy/store_type.py#L55
1,"$ nova boot --flavor 1 --image 76ae1239-0973-44cf-9051-0e1bc8f41cdd --nic net-id=a15cfbed-86d8-4660-9593-46447cb9464e vm1
$ nova list
+--------------------------------------+------+--------+------------+-------------+-------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+-------------------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1 | ACTIVE | - | Running | private=10.0.0.22 |
+--------------------------------------+------+--------+------------+-------------+-------------------+
$ brctl show
bridge name bridge id STP enabled interfaces
br-eth0 0000.fe989d8bd148 no
br-ex 0000.8a1d06d8854e no
br-ex2 0000.4a98bdebe544 no
br-int 0000.229ad5053a41 no
br-tun 0000.2e58a2f0e047 no
docker0 8000.000000000000 no
lxcbr0 8000.000000000000 no
qbr0ad6a86e-d9 8000.9e5491dd719a no qvb0ad6a86e-d9
       tap0ad6a86e-d9
$ neutron port-list
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| id | name | mac_address | fixed_ips |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d | | fa:16:3e:3a:3e:5a | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.22""} |
| 1e6bed8d-aece-4d3e-abcc-3ad7957d6d72 | | fa:16:3e:9e:dc:83 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.12""} |
| 5f522a9a-2856-4a95-8bd8-c354c00abf0f | | fa:16:3e:01:47:43 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.1""} |
| 6226f6d3-3814-469c-bf50-8c99dfec481e | | fa:16:3e:46:0e:35 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.2""} |
| a3f2ab1c-a634-446d-8885-d7d8e5978fa1 | | fa:16:3e:cf:02:d6 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.20""} |
| c10390a9-6f84-44f5-8a17-91cb330a9e12 | | fa:16:3e:41:7c:34 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.15""} |
| c814425c-be1a-4c06-a54b-1788c7c6fb31 | | fa:16:3e:f5:fc:d3 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.2""} |
| ebd874b7-43e6-4d18-b0ed-f86bb349d8b9 | | fa:16:3e:e6:b5:09 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.19""} |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
$ nova pause vm1
$ nova interface-detach vm1 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d
$ nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1 | PAUSED | - | Paused | |
+--------------------------------------+------+--------+------------+-------------+----------+
$ brctl show
bridge name bridge id STP enabled interfaces
br-eth0 0000.fe989d8bd148 no
br-ex 0000.8a1d06d8854e no
br-ex2 0000.4a98bdebe544 no
br-int 0000.229ad5053a41 no
br-tun 0000.2e58a2f0e047 no
docker0 8000.000000000000 no
lxcbr0 8000.000000000000 no
But tap still alive
$ ifconfig|grep tap0ad6a86e-d9
tap0ad6a86e-d9 Link encap:Ethernet HWaddr fe:16:3e:3a:3e:5a
And login into instance, exec 'ifconfig', it will found the interface still attach to the instance"
1,"I have multiple external networks, and therefore each l3-agent is running with a particular ext network ID.
When I create a lrouter, which is scheduled to be on one l3-agent randomly. If I set the ext-gw for that lrouter later on to be an external network different from the ext-net the l3-agent is on, the entire lrouter somehow goes down, both internal and external interfaces.
I would prefer to see:
1) the operation of putting the lrouter onto the ""wrong"" ext-net is disallowed;
2) or better, we can move the lrouter to the right l3-agent with the correct ext-net."
0,"nova patch https://review.openstack.org/#/c/104099/ caused the following unit tests to take 160 seconds:
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_killed_worker_recover
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_killed_worker_recover
This is because Server.wait() now waits for all workers to finish, but test_killed_worker_recover doesn't attempt to kill the workers like some of the other tests in MultiprocessWSGITest"
1,"Observed Error message in logs : "" 'The session is not authenticated.' to caller""
After above message I am un-able to create volumes , all volumes created are in error state.
Please see attached logs for reference"
1,"As part of the blueprint https://blueprints.launchpad.net/nova/+spec/serial-ports we introduced an API extension and a websocket proxy binary. The problem with the 2 is that a lot of the stuff was copied verbatim from the novnc-proxy API and service which relies heavily on the internal implementation details of NoVNC and python-websockify libraries.
We should not ship a service that will proxy websocket traffic if we do not acutally serve a web-based client for it (in the NoVNC case, it has it's own HTML5 VNC implementation that works over ws://). No similar thing was part of the proposed (and accepted) implementation. The websocket proxy based on websockify that we currently have actually assumes it will serve static content (which we don't do for serial console case) which will then when excuted in the browser initiate a websocket connection that sends the security token in the cookie: field of the request. All of this is specific to the NoVNC implementation (see: https://github.com/kanaka/noVNC/blob/e4e9a9b97fec107b25573b29d2e72a6abf8f0a46/vnc_auto.html#L18) and does not make any sense for serial console functionality.
The proxy service was introduced in https://review.openstack.org/#/c/113963/
In a similar manner - the API that was proposed and implemented (in https://review.openstack.org/#/c/113966/) that gives us back the URL with the security token makes no sense for the same reasons outlined above.
We should revert at least these 2 patches before the final Juno release as we do not want to ship a useless service and commit to a useles API method.
We could then look into providing similar functionality through possibly something like https://github.com/chjj/term.js which will require us to write a different proxy service."
1,"Minesweeper CI is seeing the following Tempest suites fail due to errors during performing rescue operations.
tempest.api.compute.servers.test_server_rescue.ServerRescueTestJSON
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueTestXML
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueV3Test
Error message seen in the nova cpu log is:
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
    incoming.reply(self.callback(incoming.ctxt, incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
    return self._dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 240, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 226, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2731, in rescue_instance
    reason=_(""Driver Error: %s"") % unicode(e))
InstanceNotRescuable: Instance 3f317fe3-1777-4f0d-a60a-f2370d9b2fb0 cannot be rescued: Driver Error: can't set attribute
Full logs for a run that saw this error is available here: http://208.91.1.172//logs/nova/72125/1/"
1,"The configuration hash db is updated on every response from the backend including errors that contain an empty hash. This is causing the hash to be wiped out if a standby controller is contacted first, which opens a narrow time window where the backend could become out of sync. It should only update the hash on successful REST calls."
1,"Idle client socket connections can persist forever, eg:
$ nc localhost 8776
[never returns]"
0,"Radware LBaaS driver should be able to work with a backend that was configured in HA mode.
The driver should try and call the other node in the HA pair and see if it is active."
1,"Missing allowed addr pairs support in NEC plugin.

It is needed to support VRRP. I hope it is a part of the release while I understand it is a bit late to the race.

It took some time to investigate the failure in XML security group RPC test (bug 1229954) and bug 1230083 that OVS plugin is not loaded properly."
1,"Testing on havana.
prep_resize() calls resource tracker's resize_claim() which creates a migration record. This record is cleared during the rt.drop_resize_claim() from confirm_resize() or revert_resize(), however if an exception is thrown before one of these is called or after, but before they clean up the migration record, then the migration record will hang around in the database indefinitely.
This results in an WARNING being logged every 60 seconds for every resize operation that ended with the instance in ERROR state as part of the update_available_resource period task, like the following:
2013-12-04 17:49:15.247 25592 WARNING nova.compute.resource_tracker [req-75e94365-1cca-4bca-92a7-19b2c62b9551 e4857f249aec4160bfa19c12eb805a96 a42cfb9766bf41869efab25703f5ce7b] [instance: 12d2551a-6403-4100-ba57-0995594c9c93] Instance not resizing, skipping migration.
This message is because the resource tracker's _update_usage_from_migrations() logs this warning if a migration record for an instance is found, but the instance's current state is not in a resize state.
These messages will be permanent in the logs even after the instance in question's state is reset, and even after a successful resize has occurred on that instance. There is no way to clean up the old migration record at this point.
It seems like there should be some handling when an exception occurs during resize, finish_resize, confirm_resize, revert_resize, etc. that will drop the resize claim, so the claim and migration record do not persist indefinitely."
1,"When booting instances passing in block-device and increasing the volume size , instances can go in to error state if the volume takes longer to create than the hard code value set in:
nova/compute/manager.py
  def _await_block_device_map_created(self, context, vol_id, max_tries=180,
                                        wait_between=1):
Here is the command used to repro:
nova boot --flavor ca8d889e-6a4e-48f8-81ce-0fa2d153db16 --image 438b3f1f-1b23-4b8d-84e1-786ffc73a298
--block-device source=image,id=438b3f1f-1b23-4b8d-84e1-786ffc73a298,dest=volume,size=128
--nic net-id=5f847661-edef-4dff-9f4b-904d1b3ac422 --security-groups d9ce9fe3-983f-42a8-899e-609c01977e32
Test_Image_Instance
max_retries should be made configurable.
Looking through the different releases, Grizzly was 30, Havana was 60 , IceHouse is 180.
Here is a traceback:
2014-06-19 06:54:24.303 17578 ERROR nova.compute.manager [req-050fc984-cfa2-4c34-9cde-c8aeea65e6ed
d0b8f2c3cf70445baae994004e602e11 1e83429a8157489fb7ce087bd037f5d9] [instance:
74f612ea-9722-4796-956f-32defd417000] Instance failed block device setup
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
Traceback (most recent call last):
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1394,
in _prep_block_device
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
self._await_block_device_map_created))
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 283,
in attach_block_devices
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
block_device_mapping)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 238,
in attach
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
wait_func(context, vol['id'])
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 909,
in _await_block_device_map_created
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
attempts=attempts)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
VolumeNotCreated: Volume 8489549e-d23e-45c2-ae6e-7fdb1a9c30d0 did not finish
being created even after we waited 65 seconds or 60 attempts.
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]"
1,"The doc (http://docs.openstack.org/admin-guide-cloud/content/customize-flavors.html , section ""Watchdog behavior"") suggests to use the flavor extra specs property called ""hw_watchdog_action"" to configure a watchdog device for libvirt guests. Unfortunately, this is broken due to ComputeCapabilitiesFilter trying to use this property to filter compute hosts, so that scheduling of a new instance always fails with NoValidHostFound error."
1,"When fixing bug#1208387, Image device info is returned in action setup_container.
But in that patch(https://review.openstack.org/#/c/41891/), I missed to reset the image device value in mount() and teardown().
I'm sorry to make such easy mistake, a new patch will be commited soon."
1,"when flavor pagenation, the conf of osapi_max_limit does not work.so we must modify."
1,"when a vxlan interface is created in linuxbridge agent, it is set with the argument ""proxy"" (ie ""ip link add vxlan1 type vxlan id 1 dev wlan0 proxy group 224.0.0.1""), which means that it uses an arp-proxy. This should only be done when L2-population is activated, otherwise it leads to unaccessibility between vm that are note hosted on the same host."
0,"Method setup_logging in glance/common/config.py wasn't used anywhere.
Similar function is in oslo, and some modules in Glance are using function log.setup from oslo,
like glance/cmd/manage.py#L123.
So we need remove this method ."
1,"An incorrect exception, which looks to contradict the corresponding log message logged
at this scenario seems to be raised in the evacuate instance API.
Evacuate instance needs to check that the compute service is actually down to perform evacuation of an instance,
but the exception raised when this check fails is incorrect:
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2907
    if self.servicegroup_api.service_is_up(service):
       ....
logs the message ""Instance compute service state on <compute_host> expected to be down, but it was up.""
if the check passes, but raises the exception exception.ComputeServiceUnavailable(Instance compute service state on <compute_host> expected to be down, but it was up.)
The exception raised and the message logged are contradictory."
1," which is represented by vm_mode.HVM"""
1,"This occurs twice in _validate_subnet:
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1049
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1063"
1,"Always see this error in the gate:
http://logs.openstack.org/73/122873/1/gate/gate-tempest-dsvm-neutron-full/e5a2bf6/logs/screen-n-cpu.txt.gz?level=ERROR#_2014-09-21_05_18_23_709
014-09-21 05:18:23.709 ERROR oslo.messaging.rpc.dispatcher [req-52e7fee5-65ee-4c4d-abcc-099b29352846 InstanceRunTest-2053569555 InstanceRunTest-179702724] Exception during message handling: Unexpected task state: expecting [u'powering-off'] but the actual state is deleting
Checking the EC2 API test in tempest,
    def test_run_stop_terminate_instance(self):
        # EC2 run, stop and terminate instance
        image_ami = self.ec2_client.get_image(self.images[""ami""]
                                              [""image_id""])
        reservation = image_ami.run(kernel_id=self.images[""aki""][""image_id""],
                                    ramdisk_id=self.images[""ari""][""image_id""],
                                    instance_type=self.instance_type)
        rcuk = self.addResourceCleanUp(self.destroy_reservation, reservation)
        for instance in reservation.instances:
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""running"":
                self.assertInstanceStateWait(instance, ""running"")
        for instance in reservation.instances:
            instance.stop()
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""stopped"":
                self.assertInstanceStateWait(instance, ""stopped"")
        self._terminate_reservation(reservation, rcuk)
The test is wait for instance become to stopped. But check the ec2 api code
https://github.com/openstack/nova/blob/master/nova/api/ec2/cloud.py#L1075
it always return stopped status immediately. Actually start/stop action is async call."
1,"Concern about this was raised by Duncan Thomas.
The GlusterFS Cinder driver uses ""qemu-img info"" to guess at whether a volume file is a raw image or a qcow2 image.
This is unsafe because if a user writes a qcow2 header into a volume, Cinder will interpret it as a qcow2-formatted image. It is believed this can lead to data being extracted from files on the Cinder volume host by writing a qcow2 header with a backing file pointer referencing a path to a file, and then cloning the volume. (Other similar paths may exist.)
To fix this, Cinder needs to track the file format of any file being processed this way and use ""qemu-img convert -f <source_format>"" when performing operations like volume clone, which disables qemu-img's auto format detection.
This seems to affect the GlusterFS driver, but it is possible that other attack vectors exist. The convert_image() method in cinder/image/image_utils.py is used in assorted places and does not specify a source format, so other uses of it will need to be examined for safety.
Fixing this in the GlusterFS driver is not simple: since volume/snapshot qcow2 chains are manipulated by Nova as well as Cinder, we will need to have Nova pass information back to Cinder when an operation such as volume_snapshot_delete is performed, indicating the resulting format of any files modified.
Since the above is a large effort, it may be possible to mitigate this in the short-term by having Cinder enforce some rules about whether a backing file pointer is valid before performing an operation on the file. For the GlusterFS driver that would be: must start with 'volume-<x>' and not contain '/', since our valid usage of this only points to another file named volume-<id>.<id> and does not use paths.
This attack hasn't yet been demonstrated to work, but this is a commonly known problem when processing qcow2 files."
1,"ubuntu@devstack-master:/opt/stack/nova$ grep -r exception.ProcessExecutionError *
nova/virt/libvirt/volume.py: except exception.ProcessExecutionError as exc:
commit 5e016846708ef62c92dcf607f03c67c36ce5c23f has been fixed all other wrong used places, but this one is added after this change.
[Impact]
Error doesn't exist, if error encountered then wrong error reported.
[Test Case]
grep -r exception.ProcessExecutionError /usr/lib/python2.7/dist-packages/nova/*
[Regression Potential]
Minimal. This change is in an error path already. When error path is encountered, the exception doesn't exist and causes a different error to occur but calling code treats both the same way. This ensures the right error is thrown."
1,"Booting a VM in a plain devstack setup with ceph enabled, I get an error like:

libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: -drive file=rbd:vmz/27dcd57f-948f-410c-830f-48d8fda0d968_disk.config:id=cindy:key=AQA00PxTiFa0MBAAQ9Uq9IVtBwl/pD8Fd9MWZw==:auth_supported=cephx\;none:mon_host=192.168.122.76\:6789,if=none,id=drive-ide0-1-1,readonly=on,format=raw,cache=writeback: error reading header from 27dcd57f-948f-410c-830f-48d8fda0d968_disk.config

even though config_drive is set to false.

This seems to be related to https://review.openstack.org/#/c/112014/, if I revert ecce888c469c62374a3cc43e3cede11d8aa1e799 everything works fine."
0,"https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_extension_security_group.py#L760
When the security group is created, the security group has already been deleted.
This means that the notfound error is raised because the security group has been removed, and not because the rule is being created with the wrong tenant id."
1,"this occurs while I creating an instance under my devstack env:
2013-10-11 02:56:29.374 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task task(self, context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/compute/manager.py"", line 4859, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task rt.update_available_resource(context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task return f(*args, **kwargs)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 313, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task self.pci_tracker.clean_usage(instances, migrations, orphans)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 285, in clean_usage
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task for dev in self.claims.pop(uuid):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task"
1,"DESCRIPTION:

Shared attribute is not shown when creating firewall.
I understand that, admin can only create shared firewall since it will affect other tenants also
In that case, creating shared firewall is prohibited correctly however I am able to update the firewall from tenant by shared = true
This should not be allowed

Steps to Reproduce:

root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared true
Invalid values_specs true
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared false
Invalid values_specs false
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared
{""NeutronError"": {""message"": ""Policy doesn't allow create_firewall to be performed."", ""type"": ""PolicyNotAuthorized"", ""detail"": """"}}
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2
Created a new firewall:
+--------------------+--------------------------------------+
| Field | Value |
+--------------------+--------------------------------------+
| admin_state_up | True |
| description | |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name | f2 |
| status | PENDING_CREATE |
| tenant_id | bf4fbb928d574829855ebfd9e5d0e58c |
+--------------------+--------------------------------------+
root@IGA-OSC:~# fwu f2 --shared true --------------------------------------------------------------------> able to update
Updated firewall: f2
root@IGA-OSC:~# fws f2
+--------------------+--------------------------------------+
| Field | Value |
+--------------------+--------------------------------------+
| admin_state_up | True |
| description | |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name | f2 |
| status | ACTIVE |
| tenant_id | bf4fbb928d574829855ebfd9e5d0e58c |
+--------------------+--------------------------------------+

Actual Results:
Able to update the shared attribute of tenant's firewall
Expected Results:
tenant's firewall should not be able to update the shared attribute"
1,"in nova.objects.base it imports conductor
from nova.conductor import api as conductor_api
self._conductor = conductor_api.API()
This bypasses the logic to detemin whether to use conductor RPC service or not.
Should do
from nova import conductor
self._conductor = conductor.API()"
0,"Currently each plugin calls db.configure() within the plugin's __init__ class or defines an initialize() method that's sole job is to call this method. Instead we should just call the super method of the db_base_plugin so that it calls this for us automatically.

Note: the only reason why I'm making this change is that I want to add something to the __init__() class of the db_base_plugin that's needed for the nova-event-callback blueprint and adding it in the base class of init looks to be the best place."
1,"I've found some situations where dhcp_release is not called when a port is deleted. When this happens, dnsmasq refused to give out the IP to a new port when the IP address gets recycled. The result is that the VM with the new port cannot get its IP address on boot.
There are a few conceivable scenarios that lead to this. I will attempt to describe some in the comments."
1,"Loopingcall will failed if user delete the volume before it finish copy, .
2014-03-31 16:01:04.086 ERROR cinder.openstack.common.loopingcall [-] in fixed duration looping call
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 76, in
_inner
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall self.f(*self.args, **self.kw)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/_init_.py"",
line 634, in _check_volume_copy_ops
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall volume = self.db.volume_get(ctxt, vol_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/api.py"", line 205, in volume_get
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return IMPL.volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1152, in volume_ge
t
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return _volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1145, in _volume_g
et
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall raise exception.VolumeNotFound(volume_id=volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall VolumeNotFound: Volume 8ff1a61e-21c2-48a7-890e-e7d958172721 could not be found.
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall"
1,"When the driver was not initialized, I uploaded a volume as image into glance, I got some error as follow:
[01;31mException during message handling: local variable 'volume' referenced before assignment^[[00m
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m incoming.message))
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/cinder/cinder/volume/manager.py"", line 721, in copy_volume_to_image
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m if (volume['instance_uuid'] is None and
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mUnboundLocalError: local variable 'volume' referenced before assignment
As we can see the code from cinder/volume/manager.py, the method require_driver_initialized occurs exception so that the method volume_get will be not executed, and the variable 'volume' is not defined.
 def copy_volume_to_image(self, context, volume_id, image_meta):
        payload = {'volume_id': volume_id, 'image_id': image_meta['id']}
        try:
            # NOTE(flaper87): Verify the driver is enabled
            # before going forward. The exception will be caught
            # and the volume status updated.
            utils.require_driver_initialized(self.driver)
            volume = self.db.volume_get(context, volume_id)
            image_service, image_id = \
                glance.get_remote_image_service(context, image_meta['id'])
            self.driver.copy_volume_to_image(context, volume, image_service,
                                             image_meta)
            LOG.debug(_(""Uploaded volume %(volume_id)s to ""
                        ""image (%(image_id)s) successfully""),
                      {'volume_id': volume_id, 'image_id': image_id})
        except Exception as error:
            with excutils.save_and_reraise_exception():
                payload['message'] = unicode(error)
        finally:
            if (volume['instance_uuid'] is None and
                    volume['attached_host'] is None):
                self.db.volume_update(context, volume_id,
                                      {'status': 'available'})
            else:
                self.db.volume_update(context, volume_id,
                                      {'status': 'in-use'})"
0,"Instances have been observed to remain stuck forever in ""BUILD"" state, with no errors surfaced to `nova show` after nova boot fails with the following guestfs error.
2013-03-20 18:49:08,590.590 ERROR nova.compute.manager [req-f85ccdcd-74f1-4f50-98eb-b68fb8dc8e1a dba071d520c9438ab9fb91077b6f3248 1ba6328ea66c4041bfab7cfcbc2305cf] [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Instance failed to spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Traceback (most recent call last):
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1055, in _spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] block_device_info)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1517, in spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] admin_pass=admin_password)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1913, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] instance=instance)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] self.gen.next()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1908, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] mandatory=('files',))
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/api.py"", line 304, in inject_data
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] fs.setup()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/vfs/guestfs.py"", line 114, in setup
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] {'imgfile': self.imgfile, 'e': e})
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] NovaException: Error mounting /var/lib/nova/instances/5f3fe8ba-a148-48e5-8e19-d2f65968b2db/disk with libguestfs (cannot find any suitable libguestfs supermin, fixed or old-style appliance on LIBGUESTFS_PATH (search path: /usr/lib/guestfs))"
1,"If some error occurs on tgt-admin call, update_iscsi_target fails with UnboundLocalError instead of expected ISCSITargetUpdateFailed because of debug output.
Traceback (most recent call last):
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
    **args)
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
    return getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 345, in create_volume
    _run_flow_locked()
  File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 340, in _run_flow_locked
    _run_flow()
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 336, in _run_flow
    flow_engine.run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in run
    self._run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 128, in _run
    self._revert(misc.Failure())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 81, in _revert
    misc.Failure.reraise_if_any(failures.values())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 487, in reraise_if_any
    failures[0].reraise()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 494, in reraise
    six.reraise(*self._exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 36, in _execute_task
    result = task.execute(**arguments)
  File ""/opt/stack/new/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1450, in execute
    model_update = self.driver.create_export(context, volume_ref)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 595, in create_export
    return self._create_export(context, volume)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 626, in _create_export
    volume_path, chap_auth)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 447, in _create_tgtadm_target
    old_name=old_name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 187, in create_iscsi_target
    self.update_iscsi_target(name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 253, in update_iscsi_target
    LOG.debug(""StdOut from tgt-admin --update: %s"" % out)
UnboundLocalError: local variable 'out' referenced before assignment"
1,"Enable flake8 checking for
E711 comparison to False should be 'if cond is False:' or 'if not cond:'
E712 comparison to True should be 'if cond is True:' or 'if cond:'"
0,"In the file neutron/agent/linux/dhcp.py, the DeviceManager setup method calls get_device which calls get_dhcp_port. This results in an RPC call. But, we already had the port in the setup method.
I discovered this as I was trying to optimize the number of these RPC calls."
0,"cinder/image/image_utils.py uses
  def qemu_img_info(path):
      """"""Return a object containing the parsed output from qemu-img info.""""""
      out, err = utils.execute('env', 'LC_ALL=C', 'LANG=C',
                               'qemu-img', 'info', path,
                               run_as_root=True)
      return QemuImgInfo(out)
This was added as part of I849b04b8aae76da068abcd2a20c1fcecca8a5caa
There is nothing wrong with that per se, however the rootwrap filters were updated with:
+ env: CommandFilter, /usr/bin/env, root
env is a wrapper that allows to run any command in the $PATH, so this is more or less equivalent to allowing bash in commandfilter. As a hardening precaution, env should not be allowed in CommandFilter.
The code in question can be easily reworked and EnvFilter can be used instead to harden the check."
1,"There is a possible scenario that is not correctly handled when validating quota limit. If the remaining quota is unlimited (-1) and new quota value is unlimited (-1) then both values are summed resulting in a -2 value that then causes the following error: ""Quota limit must be greater than 0""."
0,"FAIL: neutron.tests.unit.test_extension_ext_gw_mode.TestL3GwModeMixin.test_update_router_gw_with_gw_info_none
tags: worker-3
----------------------------------------------------------------------
Empty attachments:
pythonlogging:''
stderr
stdout
Traceback (most recent call last):
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 251, in test_update_router_gw_with_gw_info_none
self._test_update_router_gw(None, True)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 238, in _test_update_router_gw
self.context, self.router.id, gw_info)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_gwmode_db.py"", line 62, in _update_router_gw_info
context, router_id, info, router=router)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_db.py"", line 205, in _update_router_gw_info
   l3_port_check=False)
TypeError: delete_port() got an unexpected keyword argument 'l3_port_check'"
1,"this is because the db query is not including the deleted instance while
_delete_instance_files() in libvirt driver.
I can reproduce this bug both in master and stable havana.
reproduce steps:
1. create an instance
2. stop nova-compute
3. wait for nova-manage serivce list display xxx of nova-compute
4. modify the running_deleted_instance_poll_interval=60
running_deleted_instance_action = reap,
and start nova-compute and wait for this clean up peroidic task
5. a warnning will be given in the compute log:
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found.
the debug trace is:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1006)cleanup()
   1005 block_device_mapping = driver.block_device_info_get_mapping(
-> 1006 block_device_info)
   1007 for vol in block_device_mapping:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1007)cleanup()
   1006 block_device_info)
-> 1007 for vol in block_device_mapping:
   1008 connection_info = vol['connection_info']
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1041)cleanup()
   1040
-> 1041 if destroy_disks:
   1042 self._delete_instance_files(instance)
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> s
--Call--
> /opt/stack/nova/nova/virt/libvirt/driver.py(4950)_delete_instance_files()
   4949
-> 4950 def _delete_instance_files(self, instance):
   4951 # NOTE(mikal): a shim to handle this file not using instance objects
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4953)_delete_instance_files()
   4952 # everywhere. Remove this when that conversion happens.
-> 4953 context = nova_context.get_admin_context()
   4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1905)_shutdown_instance()
   1904 self.driver.destroy(context, instance, network_info,
-> 1905 block_device_info)
   1906 except exception.InstancePowerOffFailure:
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1906)_shutdown_instance()
   1905 block_device_info)
-> 1906 except exception.InstancePowerOffFailure:
   1907 # if the instance can't power off, don't release the ip
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1910)_shutdown_instance()
   1909 pass
-> 1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1911)_shutdown_instance()
   1910 except Exception:
-> 1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1914)_shutdown_instance()
   1913 # volume api calls, preserving current behavior
-> 1914 self._try_deallocate_network(context, instance,
   1915 requested_networks)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
2014-02-14 16:22:02.701 DEBUG nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Deallocating network for instance from (pid=19192) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1531
2014-02-14 16:22:02.704 DEBUG oslo.messaging._drivers.amqpdriver [-] MSG_ID is e529a4edb22b480cb0641a62718e9b04 from (pid=19192) _send /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py:358
2014-02-14 16:22:02.705 DEBUG oslo.messaging._drivers.amqp [-] UNIQUE_ID is aed682f94730441aaa14e43a37c86227. from (pid=19192) _add_unique_id /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py:333
2014-02-14 16:22:02.718 WARNING nova.openstack.common.loopingcall [-] task run outlasted interval by 11.632922 sec
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
--Return--
None
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> l
   1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
   1913 # volume api calls, preserving current behavior
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
   1917 self._try_deallocate_network(context, instance, requested_networks)
   1918
   1919 for bdm in vol_bdms:
   1920 try:
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(5225)_cleanup_running_deleted_instances()
   5224 self._shutdown_instance(context, instance, bdms,
-> 5225 notify=False)
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5227)_cleanup_running_deleted_instances()
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
-> 5227 except Exception as e:
   5228 LOG.warning(_(""Periodic cleanup failed to delete ""
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5228)_cleanup_running_deleted_instances()
   5227 except Exception as e:
-> 5228 LOG.warning(_(""Periodic cleanup failed to delete ""
   5229 ""instance: %s""),
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5230)_cleanup_running_deleted_instances()
   5229 ""instance: %s""),
-> 5230 unicode(e), instance=instance)
   5231 else:
ipdb> n
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found."
1,"The ""monkey_patch"" BoolOpt is described as ""Whether to log monkey patching"" but appears to act as ""Enable monkey patching"".
Option definition: http://git.openstack.org/cgit/openstack/cinder/tree/cinder/common/config.py?id=e34ab12#n189
Use: http://git.openstack.org/cgit/openstack/cinder/tree/cinder/utils.py?id=e34ab12#n593"
1,"Today if we modify the VSM credential in the cisco_plugins.ini, the
older VSM ip address remains in the db, and all requests are sent to
the older VSM. This patch deletes all VSM credentials on neutron
start up before adding the newer VSM credentials. Hence making sure
that there is only one VSM IP address and credential in the db."
1,"The consistency watch dog is calling the wrong method for a health check which is raising an exception. However, since it is in a greenthread, the exception is silently discarded so the watchdog dies without any indication."
1,"See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L225
Based on current design, the image status will be updated to 'queued' if all locations are removed from the target image, but for now, the image size won't be updated. It doesn't make sense and will confuse the end user."
0,"The docs recommend setting the 'workers' option equal to the number of CPUs on the host but defaults to 1. I proposed a change to devstack to set workers=`nproc` but it was decided to move this into glance itself:

https://review.openstack.org/#/c/99739/

Note that nova changed in Icehouse to default to number of CPUs available also, and Cinder will most likely be doing the same for it's osapi_volume_workers option.

This will have a DocImpact and probably UpgradeImpact is also necessary since if you weren't setting the workers value explicitly before the change you'll now have `nproc` glance API workers by default after restarting the service."
0,"The HyperV driver doesn't support CHAP authentication, which is always enabled by the Storwize/SVC driver."
1,"If a nova-compute machine restarts when openvswitch comes up it logs the following warning messages for all tap interfaces that do not exist:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the compute-node starts it recreates the interfaces and re-adds them to the
ovs-bridge. Unfortunately, ovs does not reinitialize the interfaces as they
are already in ovsdb and does not assign them a ofport number.
This situation corrects itself though the next time a port is added to the
ovs-bridge which is why no one has probably noticed this issue till now.
In order to correct this we should first remove interface that exist and
then readd them.
same bug on neutron-side: https://bugs.launchpad.net/neutron/+bug/1268762"
0,"In unit tests, resource contextmanagers such as network(), subnet() try to delete themselves after returning from yield even if an exception occurs. However when an exception occurs, there is a case where deletion fails. In this case original exception will be hidden and it makes difficult to debug test failures.
Before each time starts, resources like database entries will be recreated, so there is no need to try to delete resources even when an exception occurs.
For example, there is a test with programming error below:
    def test_create_dummy(self):
        with self.network() as network:
            port_res = self._create_port(self.fmt, network['network']['id'])
            port = self.deserialize(self.fmt, port_res)
            # --> Some programming error!!!!
            self.assertEqual(20, hoge)
            self._delete('ports', port['port']['id'])
When running this unit tests, we will get the following error. It is hard to understand.
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 816, in test_create_dummy
    self._delete('ports', port['port']['id'])
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""neutron/tests/unit/test_db_plugin.py"", line 534, in network
    self._delete('networks', network['network']['id'])
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 409 != 204
It is better we have the original exception:
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 809, in test_create_dummy
    self.assertEqual(20, hoge)
NameError: global name 'hoge' is not defined"
0,The plugin requires one more Alembic migration script; needs to be added to the list of plugins in the script for ext_gw_mode.
1,"The bigswitch plugin fails to use the admin context to update the floating IPs on the backend controller.
When one tenant assigns a floating IP, the update to the controller doesn't contain the floating IPs of the other tenants so the controller thinks they have been removed."
0,"Description
===========
After making the following steps:
1. Create a pool
2. Create members
3. Create a VIP
The attempt to create a health monitor and associate it with the created pool
leads to the situation when the health monitor is shown as active
but it is not being added to /opt/stack/data/neutron/lbaas/<pool_id>/conf:
# neutron lb-healthmonitor-show c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
+----------------+------------------------------------
| Field | Value
+----------------+------------------------------------
| admin_state_up | True
| delay | 3
| expected_codes | 200
| http_method | GET
| id | c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
| max_retries | 3
| pools | {""status"": ""ACTIVE"", ""status_description"": null, ""pool_id"": ""50497a9f-1439-431b-ac50-dd4cca0216fa""}
| tenant_id | dcf66bfd01aa4b82b10d8fb263ef375d
| timeout | 3
| type | HTTP
| url_path | /
+----------------+------------------------------------
# cat /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/conf
global
        daemon
        user nobody
        group nogroup
        log /dev/log local0
        log /dev/log local1 notice
        stats socket /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/sock mode 0666 level user
defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 50000
        timeout server 50000
frontend 5842ed27-02b9-499b-8512-bcf071a81ab2
        option tcplog
        bind 10.0.0.4:80
        mode http
        default_backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        option forwardfor
backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        mode http
        balance roundrobin
        option forwardfor
        server 78bbb52b-06e9-4eb6-9af9-95ecdf41c83d 10.0.0.3:80 weight 1"
1,"Volume-transfer creation provides to specify the 'name' param in cinder.
I tested it on json format, everything's ok.
But when I create it on xml format, the 'name' param is not transmitted to Cinder.
--------------
Here is input-body of my test:
    <transfer name=""transfer6"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
Here's the result, you can found the 'name' is 'None' in response.
<?xml version='1.0' encoding='UTF-8'?>
<transfer
    xmlns:os-volume-transfer=""http://docs.openstack.org/volume/ext/volume-transfer/api/v1.1"" auth_key=""9266c59563c84774"" created_at=""2013-10-23 07:59:07.231575"" id=""7e7d872b-791a-40a6-887f-9b718b4e04b0"" name=""None"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
--------------
The reason is due to the inconsistent processing between xml and json.
The CreateDeserializer of xml-format uses 'display_name' param, not 'name':
        --> attributes = ['volume_id', 'display_name']
But the processing of create() in volume-transfer, uses 'name' not 'display_name':
        --> name = transfer.get('name', None)
Therefore, no matter the 'name' is specified, its value in response will also be 'None'."
1,"Cinder's Equallogic driver fails to create any volumes because /usr/lib/python2.6/site-packages/cinder/utils.py check_ssh_injection treats spaces as bad things. Tested with latest RDO Havana release.
2014-02-14 14:16:35.386 12786 ERROR cinder.openstack.common.rpc.common [req-b7476613-4e55-4407-be61-738081c20040 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n **args)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n return getattr(proxyobj, method)(ctxt, **kwargs)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper\n return func(self, *args, **kwargs)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection\n conn_info = self.driver.initialize_connection(volume, connector)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection\n volume[\'name\'])\n', ' File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__\n self.gen.next()\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection\n self._eql_execute(*cmd)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute\n args, attempts=self.configuration.eqlx_cli_max_retries)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh\n utils.check_ssh_injection(cmd_list)\n', ' File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection\n raise exception.SSHInjectionThreat(command=str(cmd_list))\n', ""SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')\n""]
2014-02-14 14:16:38.308 12786 INFO cinder.volume.manager [req-2d51da59-6aba-4212-a9a2-61864fe18cf3 None None] Updating volume status
2014-02-14 14:16:39.470 12786 ERROR cinder.volume.drivers.eqlx [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Failed to initialize connection to volume volume-3045438e-096a-4838-a769-ec39692fa41f
2014-02-14 14:16:39.470 12786 ERROR cinder.openstack.common.rpc.amqp [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Exception during message handling
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp **args)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp conn_info = self.driver.initialize_connection(volume, connector)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp volume['name'])
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp self._eql_execute(*cmd)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp args, attempts=self.configuration.eqlx_cli_max_retries)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp utils.check_ssh_injection(cmd_list)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp raise exception.SSHInjectionThreat(command=str(cmd_list))
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')
Reason and fix for this is simple:
Line 395 @ /usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py:
                cmd.extend(['authmethod chap', 'username',
When it should be (to pass the current requirements at /usr/lib/python2.6/site-packages/cinder/utils.py's check_ssh_injection):
                cmd.extend(['authmethod', 'chap', 'username',"
1,"[root@controller ~]# nova resize --poll a9dd1fd6-27fb-4128-92e6-93bcab085a98 100
Instance resizing... 100% complete
Finished
but the instance is not finished yet and error logs in nova log"
1,IDE disks are not hot addable. They will cannot be added when a VM is running.
0,"In a number of cases now users are reporting bugs for features that don't work in Nova with versions of libvirt that are older than the minimum supported version. Most recently in Juno libvirt 0.9.11 is required but user reported bugs about a problem in 0.9.8

https://bugs.launchpad.net/nova/+bug/1376307

People clearly aren't seeing the log error message about their unsupported libvirt version, so we should turn this into a fatal exception that blocks nova-compute startup."
1,"This is my first openstack bug report. I believe this should go to Xing Yang.
The EMC VMAX driver parses an XML config file (default is /etc/cinder/cinder_emc_config.xml) to retrieve different pieces of config information. The parsing helper methods are not consistent in their expectation of the format of this file.
For example, I provide a file with newlines and white-space like the following:
<?xml version=""1.0"" encoding=""UTF-8""?>
<EMC>
  <EcomServerPort>5988
  </EcomServerPort>
  <Array>000198700498</Array>
  <EcomServerIp>5.55.55.5
  </EcomServerIp>
  <Pool>PowerVC_thin</Pool>
<EcomUserName>admin</EcomUserName><PortGroups><PortGroup>PORTGROUP1</PortGroup><PortGroup>PORTGROUP2</PortGroup></PortGroups></EMC>
This shows newlines and indenting for some properties, and no newlines or indenting for others.
Then, EMCVMAXUtils.get_ecom_server() will return:
    (u'5.55.55.5\n ', u'5988\n ')
Where the newlines and whitespace are included in the address and port strings. This causes the driver to fail.
If I take out the whitespace and newlines, then the IP address and port are parsed correctly. However, the portgroups are not parsed correctly if I remove whitespace and newlines as is shown in the last line of the file. In this case EMCVMAXUtils.parse_file_to_get_port_group_name() returns:
    u'PORTGROUP1PORTGROUP2'
Where it should return one of the port groups at random. Instead it munges the two together. Can the EMCVMAXUtils class be fixed to handle hand tolerate normal XML?"
1,"According to the note at the end of http://docs.openstack.org/trunk/config-reference/content/enable-hp-3par-fibre-channel.html
""You can configure one or more iSCSI addresses by using the hp3par_iscsi_ips option. When you configure multiple addresses, the driver selects the iSCSI port with the fewest active volumes at attach time. The IP address might include an IP port by using a colon (:) to separate the address from port. If you do not define an IP port, the default port 3260 is used. Separate IP addresses with a comma (,). The iscsi_ip_address/iscsi_port options might be used as an alternative to hp3par_iscsi_ips for single port iSCSI configuration.""
The 3PAR Driver should not consume ALL N:S:P iSCSI Paths (and FC!) from the 3PAR to the host, especially when it is configured with a single port iSCSI Configuration when attaching a volume to a host.
Instead, it should simply choose the least used (at the time of connection).
Steps to reproduce:
1. Configure a 3PAR to a blade with more than one path: e.g. 2 iscsi & 2 fc
2. Create an iscsi volume
3. Attach the iscsi volume to an instance
4. Observe that 4 VLUNs are created, one for each path between the 3par & the instance (2 iscsi & 2 fc, yes, fc too even if this is an iscsi volume)"
0,"In test_create_instance_with_neutronv2_fixed_ip_already_in_use of both
v2 and v3, these tests pass wrong parameters ""fixed-ip"" instead of
valid parameter ""fixed_ip"".
The purposes of these tests are to check the behaviors when passing
in-use addresses, but current tests contain two negative factors and
we cannot test the purposes now."
1,"create vxlan successfully with invalid vxlan id for ml2:
curl -i http://127.0.0.1:9696/v2.0/networks -H ""X-Auth-Token:$token_id"" -H ""Content-Type: application/json"" -X POST -d '{""network"": {""name"": ""vxlan_test"", ""admin_state_up"": true, ""tenant_id"": $tenant_id, ""provider:network_type"": ""vxlan"", ""router:external"": false, ""shared"": false, ""provider:segmentation_id"": -1}}'

response:
HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Content-Length: 332
X-Openstack-Request-Id: req-1e9af36d-b742-4c76-bfdf-0cea4df6399f
Date: Fri, 11 Apr 2014 09:25:51 GMT

{
    ""network"": {
        ""status"": ""ACTIVE"",
         ""subnets"": [

        ],
         ""name"": ""vxlan_test"",
         ""provider:physical_network"": null,
         ""admin_state_up"": true,
         ""tenant_id"": ""bfed9cd5990c49ad8a42ba36d505c003"",
         ""provider:network_type"": ""vxlan"",
         ""router:external"": false,
         ""shared"": false,
         ""id"": ""5bd9b587-02f8-4432-b0d0-18a126126072"",
         ""provider:segmentation_id"": -1
    }
}"
0,"Recently I have been doing some queries for extraConfig VM options and found that the most efficient way to retrieve a given property is to do:
session._call_method(vim_util, 'get_dynamic_property', vm_ref, 'VirtualMachine', 'config.extraConfig[""some_prop_here""]')
Right now we ask for all extraConfig options and then we iterate over the result set to find a particular one."
1,"(Using ML2 with ovs)
While playing with https://github.com/stackforge/cookbook-openstack-network/blob/master/files/default/neutron-ha-tool.py to migrate routers between L3 agents, I had the issue that the connectivity got lost.
After investigating, it turns out that interface for the port of the router that is connected to the external network (attached to br-public -- which is usually named br-ext) gets a tag. Manually removing the tag makes things work.
I'm attaching a bit of the log where the port_update message is received (it's received for the two interfaces of the router, so this needs some care when reading). We can see the following:
 Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=2', 'set', 'Port', 'qg-923f9b0e-aa', 'tag=2']
However, reading the code, it seems that this kind of actions should only be done for ports on br-int (because of ""vif_port = self.int_br.get_vif_port_by_id(port['id'])""). So this shouldn't be run for other ports."
1,"\nova\db\sqlalchemy\api.py quota_reserve()
when decided whether to refresh the user_usages[resource], one rule is that if the last refresh was too long time ago, we need refresh user_usages[resource].
 elif max_age and (user_usages[resource].updated_at -
                              timeutils.utcnow()).seconds >= max_age:
using last update time minus current time result in a overflow ,so that the refresh action always be executed,in consideration of the max_age won't be a max number."
0,"In db/api.py, qos_specs_update calls ""IMPL.qos_specs_update()"" instead of ""return IMPL.qos_specs_update()"".
This generates a pylint lintstack error:
[""Assigning to function call which doesn't return"", ""res = db.qos_specs_update(context, qos_specs_id, specs)""]
This doesn't look right -- I assume the result should be returned from db.qos_specs_update."
1,"I run multiple neutron-servers using haproxy. Here's the exception thrown by all the neutron-servers when services restart:
2014-04-14 11:42:18.315 6457 ERROR neutron.service [-] Unrecoverable error: please check log for details.
2014-04-14 11:42:18.315 6457 TRACE neutron.service Traceback (most recent call last):
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 103, in serve_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service service.start()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 72, in start
2014-04-14 11:42:18.315 6457 TRACE neutron.service self.wsgi_app = _run_wsgi(self.app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 117, in _run_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service app = config.load_paste_app(app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 145, in load_paste_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-04-14 11:42:18.315 6457 TRACE neutron.service return loadobj(APP, uri, name=name, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-04-14 11:42:18.315 6457 TRACE neutron.service return context.create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service app = loader.get_app(app_name, global_conf=global_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service app = loader.get_app(pipeline[-1])
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service return fix_call(context.object, context.global_conf, **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service return cls(**local_config)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service plugin = manager.NeutronManager.get_plugin()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service return cls.get_instance().plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service cls._create_instance()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-14 11:42:18.315 6457 TRACE neutron.service return f(*args, **kwargs)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service cls._instance = cls()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service plugin_provider)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service return plugin_class()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 106, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service self.type_manager.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/managers.py"", line 74, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service driver.obj.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 81, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service self._sync_vxlan_allocations()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 172, in _sync_vxlan_allocations
2014-04-14 11:42:18.315 6457 TRACE neutron.service session.add(alloc)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 402, in __exit__
2014-04-14 11:42:18.315 6457 TRACE neutron.service self.commit()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 314, in commit
2014-04-14 11:42:18.315 6457 TRACE neutron.service self._prepare_impl()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 298, in _prepare_impl
2014-04-14 11:42:18.315 6457 TRACE neutron.service self.session.flush()
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 615, in _wrap
2014-04-14 11:42:18.315 6457 TRACE neutron.service _raise_if_duplicate_entry_error(e, get_engine().name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 559, in _raise_if_duplicate_entry_error
2014-04-14 11:42:18.315 6457 TRACE neutron.service raise exception.DBDuplicateEntry(columns, integrity_error)
2014-04-14 11:42:18.315 6457 TRACE neutron.service DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0) ... displaying 10 of 100000 total bound parameter sets ... (99999, 0), (100000, 0))
2014-04-14 11:42:18.315 6457 TRACE neutron.service
2014-04-14 11:42:18.331 6457 CRITICAL neutron [-] (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0) ... displaying 10 of 100000 total bound parameter sets ... (99999, 0), (100000, 0))"
1,NUMATopologyFilter tries to get instance_properties from filter_properties. But in fact instance_properties are in another dictionary (request_spec) that is embedded in filter_properties.
0,"There is a loop inside cinder/volume/drivers/storwize_svc.py _delete_vdisk() function that will
wait on flashcopy to finish before the vdisk can be deleted. If trying to delete a cinder volume
that is created from snapshot or another volume before the flashcopy finishes, the volume
service process will loop and wait for the flashcopy to be done. Since the code is blocked
in the _delete_vdisk code, volume service is blocked and won't respond to REST API
or update status. The service will be marked offline.
I am waiting for the person who found this bug to test a change that puts the while loop into an
inline function that I then run with FixedIntervalLoopingCall.
I hope to have a patch to post here later today once we have been able to test the code I wrote."
1,"Based on the OpenStack documentation at http://docs.openstack.org/api/openstack-compute/2/content/ChangesSince.html...""To allow clients to keep track of changes, the changes-since filter displays items that have been recently deleted. Both images and servers contain a DELETED status that indicates that the resource has been removed."" This allows OpenStack consumers to determine when images and servers are deleted when using the changes-since support. OpenStack works as documented for images. However, changes-since support in OpenStack Havana may not return deleted servers with a DELETED status.
The recreate scenario is as follows:
1) Boot a new server and wait for the boot to complete.
2) Save the current time
3) Use changes-since to get all servers changed since the time saved in step 2. It should be none.
4) Delete the server created in step 1 and wait for the delete to complete.
5) Use changes-since to get all servers changed since the time saved in step 2. It should include the server deleted in step 4 and the server's status should be DELETED. The actual result is that the server is included but its status is ERROR.
For the above recreate scenario, the boot failed because no valid host was found. As a result, the server is in ERROR status after step 1. I have not tested with a successful boot."
0,db_base_plugin_v2 AUTO_DELETE_PORT_OWNERS should use the constant defined for network:dhcp.
0,"During rollback operations, the resource is cleaned up from the neutron database but leaves a few stale entries in the n1kv specific tables.
Vlan/VXLAN allocation tables are inconsistent during network rollbacks.
VM-Network table is left inconsistent during port rollbacks.
Explicitly clearing ProfileBinding table entry (during network profile rollbacks) is not required as delete_network_profile internally takes care of it."
1,"If an instance is booted using the block device mapping v2 API and source=snapshot is used, no image metadata will be copied into the instance system_metadata which can cause issues further in the boot process. Since properties like os_type are missed which may be used by a virt driver."
1,"With the new early majority code, doing a bulk delete where you delete all the objects in a container and then try to delete the container at the end will very frequently fail. This is because it is very likely that all 3 objects have not been deleted by the time the middleware got a successful response. A possible solution is to just sleep for a second and retry if you get a 409. idk

btw- this behavior would also happen for people emptying / deleting containers but I guess that's the client problem- also it already existed, its just now more frequent.

another possible solution is just to not do anything :)"
1,"Currently the failures during the creation of resources related to the creation of a HA router are handled my a try/except to avoid a potential lock wait timeout. This has been done in order to keep the RPC calls outside the transactions.
All the related resources are created in the _create_router_db but this method is called inside a transaction which is started is the create_router method. Moreover the try/except mechanism used to rollback the router creation will not work since we are in a already opened transaction."
0,"http://logs.openstack.org/64/41464/4/check/gate-tempest-devstack-vm-neutron/4288a6b/console.html
Seen testing https://review.openstack.org/#/c/41464/
2013-08-13 17:34:46.774 | Traceback (most recent call last):
2013-08-13 17:34:46.774 | File ""tempest/scenario/test_network_basic_ops.py"", line 176, in test_003_create_networks
2013-08-13 17:34:46.774 | router = self._get_router(self.tenant_id)
2013-08-13 17:34:46.775 | File ""tempest/scenario/test_network_basic_ops.py"", line 141, in _get_router
2013-08-13 17:34:46.775 | router.add_gateway(network_id)
2013-08-13 17:34:46.775 | File ""tempest/api/network/common.py"", line 78, in add_gateway
2013-08-13 17:34:46.776 | self.client.add_gateway_router(self.id, body=body)
2013-08-13 17:34:46.776 | File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-13 17:34:46.776 | ret = self.function(instance, *args, **kwargs)
2013-08-13 17:34:46.776 | File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 396, in add_gateway_router
2013-08-13 17:34:46.777 | body={'router': {'external_gateway_info': body}})
2013-08-13 17:34:46.777 | File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 987, in put
2013-08-13 17:34:46.777 | headers=headers, params=params)
2013-08-13 17:34:46.778 | File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 970, in retry_request
2013-08-13 17:34:46.778 | raise exceptions.ConnectionFailed(reason=_(""Maximum attempts reached""))
2013-08-13 17:34:46.778 | ConnectionFailed: Connection to neutron failed: Maximum attempts reached"
1,"function in compute/manager.py _poll_unconfirmed_resizes will translate the migrate status from finished to error
whenever it find a problem , consider following case
1) _poll_unconfirmed_resizes running, it found several migrations to be confirmed
2) user want to delete an instance ,its task state will be changed to DELETING by
3) _poll_unconfirmed_resized will found the task state is not None, it will make the migration error status
4) following code in _delete
if instance.vm_state == vm_states.RESIZED:
                self._confirm_resize_on_deleting(context, instance)
will fail because migration status already updated
so we should not set the migration status to 'error' , let it be and report warning message is enough"
1,"The NetApp NFS and iSCSI QOS extra spec for volume types is not implemented correctly. It currently requires a QOS policy to be applied at the flexVol level. The scheduler then assigns a new cinder volume to the flexVol which has the QOS policy applied to it. This results in a situation where multiple cinder volumes are all fighting for the same QOS limits, rather than each getting the implied limit. For example:
QOS policy of 100 MB/s is applied to a flexVol by the NetApp admin.
5 cinder volumes are created with the 100 MB/s QOS policy applied via volume-type. They are all placed into the flexVol created in the first step.
These 5 cinder volumes are now fighting each other for the 100 MB/s that the flexVol has allocated to it.
The expected behavior is that each cinder volume would independently have their own 100 MB/s limit, not a combined limit.
In order to do this, the QOS policy should be applied at the LUN level for the iSCSI driver. The NFS driver is another can of worms, as I'm not aware of a way to apply a QOS policy to a file."
1,"With DVR, connections to external network using floating ips flow through the router namespace into the fip namespace and then to the external network. These connections are tracked in router namespace for filter and nating purpose. Currently these connections are also being tracked in the fip namespace because tracking is turned on by default. To avoid unnecessary consumption of resources tracking of such connections should be turned off in fip namespace."
1,Split ML2 cisco nexus event handers for update and delete into precommit (called during DB transactions) and postcommit (called after DB transactions) methods.
0,"In cinder volume restore api, we do twice size check when restore volume with a given volume uuid. It's necessary to remove the redundant check."
0,The request context that comes into Neutron is not included in the request to the backend. This makes it difficult to correlate events in the debug logs on the backend such as what incoming Neutron request resulted in particular REST calls to the backend and if admin privileges were used.
1,"Under a stock DevStack setup on bare metal, I started stack.sh, which creates a public network (w/o DHCP) and a private network (with DHCP) and a router. On the host I created a br-ex and added eth3 to the bridge. The public network is connected via a physical switch, to another host (also running Devstack).
I am able to create VMs, and ping between them, and I can also (with the new VPNaaS feature under development) ping VMs over the public (provider?) network.
If I create a VM (seen this with cirros and others) and specify the private network, or create a VM with the public network, they launch fine. However, if I try to boot an instance with two NICs, the launch fails:
 localnet=`neutron net-list | grep private | cut -f 2 -d'|' | cut -f 2 -d' '`
 nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 mary --nic net-id=$localnet
pubnet=`neutron net-list | grep public | cut -f 2 -d'|' | cut -f 2 -d' '`
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 peter --nic net-id=$pubnet
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 paul --nic net-id=$localnet --nic net-id=$pubnet
nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| 0fc9c41c-fcd7-45a8-ae10-568b506a331f | mary | ACTIVE | None | Running | private=10.2.0.4 |
| 76925e14-a0b7-4285-9197-5ff0e92f5bb4 | paul | ERROR | None | NOSTATE | |
| c8cd45a4-10d9-4d8c-9a5a-f806e63683c0 | peter | ACTIVE | None | Running | public=172.24.4.235 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
Looking at the logs, I see that the screen-n-cpu.log reports an error and has a traceback:
2013-09-04 18:01:47.963 ERROR nova.compute.manager [req-187bbd89-f40d-4637-b549-0edfc9b66419 admin admin] [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Instance failed to spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Traceback (most recent call last):
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/opt/stack/nova/nova/compute/manager.py"", line 1293, in _spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1699, in spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2697, in _creat\
e_domain_and_network
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] domain = self._create_domain(xml, instance=instance, power_on=power_on\
)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2652, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] domain.XMLDesc(0))
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2647, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] domain.createWithFlags(launch_flags)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 17\
9, in doit
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] result = proxy_call(self._autowrap, f, *args, **kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 13\
9, in proxy_call
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] rv = execute(f,*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77\
, in tworker
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] rv = meth(*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 581, in createW\
ithFlags
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed',\
 dom=self)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] libvirtError: internal error Cannot instantiate filter due to unresolvable\
 variables: DHCPSERVER
There is no DHCP server on the public network, but there is one on the local network.
This is consistently reproducible and occurs with other image types.
The nova code is off the master branch (havana) with the SHA1 of 86c97ff from 6 days ago."
0,The plugin is skipping the unit tests for multiple subnets
1,"There is a case where multiple OFC delete-port operations run in parallel. It is usually observed in tempest api tests:
ofc-delete-port triggered by delete-network API request and ofc-delete-port request from dhcp-agent (release_dhcp_port) triggered by delete-subnet.
There are several manifests I see, however this kind of ""not found"" is a valid situation and should be ignored during deleting OFC port.
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/675/ (PortNotFound)
2014-01-28 08:12:37.449 32365 ERROR neutron.api.v2.resource [req-13a26437-c201-4fc6-8af4-dfbff6663f3b None] delete failed
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 356, in delete_network
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource port = self.deactivate_port(context, port)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 252, in deactivate_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource port_status)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 166, in _update_resource_status
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource obj_db = obj_getter(context, id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 266, in _get_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource raise q_exc.PortNotFound(port_id=id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource PortNotFound: Port b6d8480f-4c59-4095-bc72-bc7516fba1d7 could not be found
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/588/
2014-01-24 11:54:40.217 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [202:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [404:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 WARNING neutron.plugins.nec.common.ofc_client [-] Operation on OFC failed: status=404, detail=
2014-01-24 11:54:40.411 31910 ERROR neutron.plugins.nec.nec_plugin [-] delete_ofc_port() failed due to An OFC exception has occurred: Operation on OFC failed
2014-01-24 11:54:40.523 31910 ERROR neutron.api.v2.resource [-] delete failed
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 362, in delete_network
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource raise nexc.OFCException(reason=reason)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource OFCException: An OFC exception has occurred: Failed to delete port(s)=f77286eb-8493-406c-8240-6e62e027c59d from OFC."
1,"Time unit is missing in the description of option ""vmware_task_poll_interval"" in cinder.conf.sample. This description is used in the auto-generated config reference at http://docs.openstack.org/trunk/config-reference/content/vmware-vmdk-driver.html"
0,"Need to enable the integrated test_create_multiple_servers v3 version of the test
when the V3 multiple create extension merges"
1,"Missing token in Neutron context but Nova/Cinder context has. Store the toke into the context can make the third party plugin or driver integrate the authentication with KeyStone.

And Phillip Toohill [mailto:<email address hidden>] also ask for that.

Currently, Neutron did not pass the token to the context. But Nova/Cinder did that. It's easy to do that, just 'copy' from Nova/Cinder.

1. How Nova/Cinder did that
class NovaKeystoneContext(wsgi.Middleware)
///or CinderKeystoneContext for cinder

              auth_token = req.headers.get('X_AUTH_TOKEN',
                                     req.headers.get('X_STORAGE_TOKEN'))
              ctx = context.RequestContext(user_id,
                                     project_id,
                                     user_name=user_name,
                                     project_name=project_name,
                                     roles=roles,
                                     auth_token=auth_token,
                                     remote_address=remote_address,
                                     service_catalog=service_catalog)

2. Neutron not passed token. Also not good for the third part network infrastructure to integrate the authentication with KeyStone.
class NeutronKeystoneContext(wsgi.Middleware)
.................
##### token not get from the header and not passed to context. Just change here like what Nova/Cinder did.
        context.Context(user_id, tenant_id, roles=roles,
                              user_name=user_name, tenant_name=tenant_name,
                              request_id=req_id)
        req.environ['neutron.context'] = ctx"
0, which can be low now due to threads_per_disk giving us an effective multiplier on total number of object server processes. If workers * threads_per_disk * disks > 1024
1,"cisco plugin is missing from a few migration files causing missing DB tables. The following files in particular:
versions/folsom_initial.py
versions/176a85fc7d79_add_portbindings_db.py"
0,"IBM NAS cinder driver sets 'rw' permissions to all during volume create operation from a volume snapshot or from an existing volume (volume clone operation).
This is not required as 'rw' permissions to the user only should be sufficient.
This also helps resolve the security issue setting 'rw' permissions to all."
1,"If you run nova shelve api in nova-cell environment It throws following error:
Nova cell (n-cell-child) Logs:
2014-07-06 23:57:13.445 ERROR nova.cells.messaging [req-a689a1a1-4634-4634-974a-7343b5554f46 admin admin] Error processing message locally: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging Traceback (most recent call last):
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/cells/messaging.py"", line 200, in _process_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/cells/messaging.py"", line 1287, in _process_message_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/cells/messaging.py"", line 700, in run_compute_api_method
2014-07-06 23:57:13.445 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/compute/api.py"", line 192, in wrapped
2014-07-06 23:57:13.445 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/compute/api.py"", line 182, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/compute/api.py"", line 163, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging File ""/opt/stack/nova/nova/compute/api.py"", line 2458, in shelve
2014-07-06 23:57:13.445 TRACE nova.cells.messaging instance.save(expected_task_state=[None])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging TypeError: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging
Nova compute log:
2014-07-07 00:05:19.084 ERROR oslo.messaging.rpc.dispatcher [req-9539189d-239b-4e74-8aea-8076740
31c2f admin admin] Exception during message handling: 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _
dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _
dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _
do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dis
t-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher payload)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return f(self, context, *args, **kw)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher pass
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 330, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 308, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher kwargs['instance'], e, sys.exc_info())
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 3847, in shelve_instance
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher self.conductor_api.notify_usage_exists(
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/conductor/api.py"", line 206, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 320, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher extra_usage_info=extra_usage_info_p)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher retry=self.retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher timeout=timeout, retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher raise result
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher ignore_missing_network_data)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher macs = [vif['address'] for vif in nw_info]
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.093 ERROR oslo.messaging._drivers.common [req-9539189d-239b-4e74-8aea-807674031c2f admin admin] Returning exception 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
 to caller
Shelve api is failing in nova-cell environment because the compute_api shelve/unshelve
methods expect an Instance object, but cell is still passing the sqlalchemy form.
Also 'info_cache' and 'metadata' are not present in instance-object and shelve requires these
properties to be present in Instance object."
1,"When an exception occurs during exception handling, it lose the
information of the first exception.
In glance.store.rbd.Store.add(), in some cases, before re-transmission
of the exception, the exception is rewritten."
1,"IP lib can not distinguish between interfaces with an '@' in their name to a VLAN interfaces.
And an interface name can have more than one '@' in their name."
0,"In neutron/common/config.py, the argument to setup_logging is unused:

  def setup_logging(conf):
     product_name = ""neutron""
     logging.setup(product_name)
     LOG.info(_(""Logging enabled!""))

As a minor cleanup, we should remove the argument in the interests of simpler code and avoiding confusion."
0,"Commit e00bdd7aa8c1ac9f1ae5057eb2f774f34a631845 change get_floating_ip_pools in a way that it now return a list of names rather than a list whose elements are in the form {'name': 'pool_name'}.

The implementation of this method in nova.network.neutron_v2.api has not been adjusted thus causing tempest.api.compute.floating_ips.test_list_floating_ips.FloatingIPDetailsTestJSON to always fail with neutron

The fix is straightforward."
1,"The following stack trace observed in the gate:
ERROR oslo.messaging.rpc.dispatcher [req-110db567-3322-4922-95c8-a54d166c8ead ] Exception during message handling: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'
TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
TRACE oslo.messaging.rpc.dispatcher incoming.message))
TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 57, in sync_routers
TRACE oslo.messaging.rpc.dispatcher context, host, router_ids)
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_agentschedulers_db.py"", line 191, in list_active_sync_routers_on_active_l3_agent
TRACE oslo.messaging.rpc.dispatcher active=True)
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 306, in get_sync_data
TRACE oslo.messaging.rpc.dispatcher DEVICE_OWNER_DVR_INTERFACE])
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 1016, in _get_router_info_list
TRACE oslo.messaging.rpc.dispatcher active=active)
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 919, in _get_sync_routers
TRACE oslo.messaging.rpc.dispatcher return self._build_routers_list(context, router_dicts, gw_ports)
TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 241, in _build_routers_list
TRACE oslo.messaging.rpc.dispatcher rtr['gw_port'] = gw_ports[gw_port_id]
TRACE oslo.messaging.rpc.dispatcher KeyError: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'a
Link: http://logs.openstack.org/48/112948/3/check/check-tempest-dsvm-neutron-pg/503d619/logs/screen-q-svc.txt.gz?#_2014-08-11_07_19_33_893"
0,"Here the list of misspelled word found in glance source
(using misspellings)
http://paste.openstack.org/show/60786/"
1,"This problem occurs when ml2 plugin runs under Metaplugin.
error log of dhcp_agent is as follows:
---
2014-03-28 18:57:17.062 ERROR neutron.agent.dhcp_agent [req-9c53d7a6-d850-42de-896f-184827b33bfd None None] Failed reporting state!
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Traceback (most recent call last):
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/dhcp_agent.py"", line 564, in _report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent self.state_rpc.report_state(ctx, self.agent_state, self.use_call)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/agent/rpc.py"", line 72, in report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent return self.call(context, msg, topic=self.topic)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent File ""/opt/stack/neutron/neutron/openstack/common/rpc/proxy.py"", line 129, in call
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent exc.info, real_topic, msg.get('method'))
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Timeout: Timeout while waiting on RPC response - topic: ""q-plugin"", RPC method: ""report_state"" info: ""<unknown>""
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent
---
This problem is brought by the patch:
 https://review.openstack.org/#/c/72565/
because ml2 plguin does not become to open RPC connection at plugin initialization."
0,"The Swift backup driver for Cinder supports only Auth 1.0, for Swift clusters that work with Auth 2.0, the backup driver cannot authenticate with Swift."
0,"commit 9d13ea88 had an argument mismatch which causes the following:
TypeError: _get_ports() takes exactly 2 arguments (1 given)"
1,"The baremetal driver's throwing the following error:

ERROR nova.virt.baremetal.driver [req-aa418c3f-a45b-40a2-b0b3-dff16886a8ab 94242353fea44662973dbe7ce3dc980c 93e2f6898a394052a6c1b7b22aedcf1f] Exception no pxe bootfile-name path: 'PXE' object has no attribute 'get_pxe_config_file_path'

The error happens because of this line in the baremetal/driver.py file.

nova/virt/baremetal/driver.py:511: bootfile_path = self.driver.get_pxe_config_file_path(instance)

Looking at the baremetal/pxe.py you'll see that the `get_pxe_config_file` method is not part of the PXE class, but that method exist in the file."
1,"From nova-compute log when exceeding a quota.
/json; charset=UTF-8'} {""NeutronError"": ""Quota exceeded for resources: ['port']""}
 http_log_resp /opt/stack/python-neutronclient/neutronclient/common/utils.py:179
2013-08-02 21:30:38.115 30306 DEBUG neutronclient.v2_0.client [-] Error message: {""NeutronError"": ""Quota exceeded for resources: ['port']""} _handle_fault_response /opt/stack/python-neutronclient/neutronclient/v2_0/client.py:756
2013-08-02 21:30:38.115 30306 ERROR nova.compute.manager [-] Instance failed network setup after 1 attempt(s)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager Traceback (most recent call last):
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/nova/nova/compute/manager.py"", line 1280, in _allocate_network_async
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager security_groups=security_groups)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/nova/nova/network/api.py"", line 49, in wrapper
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager res = f(self, context, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 310, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager LOG.exception(msg, port_id)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 287, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager port_client.create_port(port_req_body)['port']['id'])
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager ret = self.function(instance, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 276, in create_port
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager return self.post(self.ports_path, body=body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 872, in post
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager headers=headers, params=params)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 795, in do_request
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager self._handle_fault_response(status_code, replybody)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 765, in _handle_fault_response
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager exception_handler_v20(status_code, des_error_body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 81, in exception_handler_v20
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager message=error_dict)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager NeutronClientException: Quota exceeded for resources: ['port']
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
Furthermore nova doesn't show its a quota issue but instead says:
| fault | {u'message': u'NoValidHost', u'code': 500, u'created': u'2013-08-02T21:31:04Z'} |"
1,"When I run neutron-db-manage revision --autogenerate, it detects lbaas table deletion
even when loadbalancer service plugins are declared in neutron.conf.
The reason is table definitions of service plugins are not loaded when ""revision --autogenerate"" is run."
1,"The new live_migrate task in the conductor does not pass extra_specs from the flavor through to the filters - thus giving an incorrect result.
This showed up when using the TrustedFilter which depends on extra_specs (set by nova.scheduler.utils.build_request_spec) - however nova.conductor.tasks.live_migrate.LiveMigrationTask._get_candidate_destination does not use this build_request_spec and builds it's own - which missed this extra_specs value.
Marked as a security vulnerability as it means that the use of live migration will bypass filters intended to provide a secure environment such as TrustedFilter."
1,"The current implementation always tries to get a transport from oslo.messaging assuming the transport_url option has been set. This is done to keep backwards compatibility. However, since the default `rpc_backend` is rabbit, it'll always try to load such driver. The problem raises when `kombu` is not installed and the `notifier_strategy` is set to qpid. This will make glance-api fail because it'll try to load the rabbit driver *before* loading the qpid one."
1,"As per markmc's comments on https://review.openstack.org/#/c/39086/24 - the function is used to get the swap out of the list context as the block_device_info data structure (used internally by the virt drivers) needs 'swap' field to be either a single dict or none. However if passed something that is not an obvious list of swap looking things - the function will happily reutrn the passed list.
More safe and correct behaviour would be to return None (or raise)."
0,"in v2/v3 API layer
def get_spice_console(self, req, id, body):
didn't catch NotImplement exception and handle it"
0,"Variable err in restore_backup method is not being used, so remove this 'err ' variable."
1,"Steps to reproduce as admin:
1. create an aggregate (ID 1)
$ nova aggregate-create foo
2. curl -i ""http://127.0.0.1:8774/v2/""`keystone token-get | awk '/ tenant_id /{print $4}'`""/os-aggregates/1/action"" -X POST -H ""Content-Type: application/json"" -H ""X-Auth-Token: ""`keystone token-get | awk '/ id /{print $4}'` -d '{""add_host"": {""host"": [""host-2"", ""host-1""]}}
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-f6ea35a8-029a-444a-9741-7c6abd27f294
Date: Wed, 21 May 2014 08:34:57 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}
Expected solution:
A: Response with 400(Bad Request) when the ""host"" value is not the expected type.
B: Add multiple hosts by a single request"
1,"The VMware Minesweeper CI is occasionally seeing an error when deleting snapshots. The error is:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 309, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2692, in backup_instance
    task_states.IMAGE_BACKUP)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2758, in _snapshot_instance
    update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 645, in snapshot
    _vmops.snapshot(context, instance, name, update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 873, in snapshot
    self._delete_vm_snapshot(instance, vm_ref, snapshot)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 780, in _delete_vm_snapshot
    self._session._wait_for_task(delete_snapshot_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 948, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A general system error occurred: concurrent access
Full logs for an affected run can be found here: http://10.148.255.241/logs/85961/2"
1,Resulting in AttributeError: 'dict' object has no attribute 'vcpus' if we try to start with a flavor that will result in Nova trying to decide on an automatic topology (for example providing only number of nodes with hw:numa_nodes extra_spec)
1,"On your saio if you `touch /srv/node1/asdf` and run the container-auditor you get an uncaught exception:
container-6011: UNCAUGHT EXCEPTION#012Traceback (most recent call last):#012 File ""/usr/local/bin/swift-container-auditor"", line 7, in <module>#012 execfile(__file__)#012 File ""/vagrant/swift/bin/swift-container-auditor"", line 23, in <module>#012 run_daemon(ContainerAuditor, conf_file, **options)#012 File ""/vagrant/swift/swift/common/daemon.py"", line 110, in run_daemon#012 klass(conf).run(once=once, **kwargs)#012 File ""/vagrant/swift/swift/common/daemon.py"", line 55, in run#012 self.run_once(**kwargs)#012 File ""/vagrant/swift/swift/container/auditor.py"", line 99, in run_once#012 self._one_audit_pass(reported)#012 File ""/vagrant/swift/swift/container/auditor.py"", line 54, in _one_audit_pass#012 for path, device, partition in all_locs:#012 File ""/vagrant/swift/swift/common/utils.py"", line 1756, in audit_location_generator#012 partitions = listdir(datadir_path)#012 File ""/vagrant/swift/swift/common/utils.py"", line 2195, in listdir#012 return os.listdir(path)#012OSError: [Errno 20] Not a directory: '/srv/node1/asdf/containers'
This may only happen if mount_check is false, otherwise it's probably skipped earlier. Still, we should probably just skip over files in the devices root."
0,"Evacuate' function aims to help administrator/operator to evacuate servers if this compute node fails.

So we need to add a protection here, the target host should not be the original host."
0,"Currently there exists no policy to control data uploads.

https://bugs.launchpad.net/glance/+bug/1250918 is for adding upload_image policy in glance v2 api, this bug is for adding it to the glance v1 api."
1,"The message cinder returns to the user when a create volume/snapshot request would exceed quota does not give the user the information they need to correct the problem. The error message returned is:

Requested volume or snapshot exceeds allowed Gigabytes quota.

The following goes to the api log:

Quota exceeded for 12345678, tried to create 2048G volume (2048G of 4048G already consumed)

However, in general, the user won't have access to the api.log, so those numbers should be passed to the user."
1,"Use dnsmasq as dhcp server, OpenStack deploy one instance, we found the deployed instance cannot get the targeted IPv6 address, but it can get the targeted IPv4 address.
At the earlier time, I found this issue when use Vmware vcenter driver, today I also found this issue is Linux env.
I use dnsmasq as dhcpv6 server, use 'tcpdump -i tapXXXX' to monitor the network data, and in deployed instance, and run ""dhclient -6"", the result of tcpdump as below:
  22:02:54.354287 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:54.354689 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:55.434954 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:55.435283 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:57.587164 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:57.587456 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:59.354082 IP6 fe80::184d:82ff:fec4:d268 > fe80::f816:3eff:fe34:a80e: ICMP6, neighbor solicitation, who has fe80::f816:3eff:fe34:a80e, length 32
  22:02:59.354922 IP6 fe80::f816:3eff:fe34:a80e > fe80::184d:82ff:fec4:d268: ICMP6, neighbor advertisement, tgt is fe80::f816:3eff:fe34:a80e, length 24
from dnsmasq log, I got ""no address available"" error.
The root cause is dnsmasq need to read host file, and distinguish MAC addresses from IPv6 addresses.
the current host file as below:
fa:16:3e:25:f4:31,host-2001-2011-0-f104--3.openstacklocal, 2001:2011:0:f104::3
We need to wrap the ipv6 address with '[]' to let dnsmasq can distinguish MAC addresses from IPv6 addresses."
0,"When ML2 plugin is used, L3 agent won't set SNAT rules between internal networks when used with ML2 plugin.

external gateway modes introduces a new ""enable_snat"" comlumn to Router DB, but as ML2 plugin was merged right after this extension, it isn't included in alembic migration script [1], so this column won't be present for this plugin. As a consequence, L3 agents won't set-up SNAT rules in neutron routers.

[1]https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/128e042a2b68_ext_gw_mode.py"
1,"In the current version (2.0) of the VMAX driver, we retrieve iSCSI IP addresses dynamically from SMI-S. However, we ran into situations where we can't get them reliably during testing. The fix is to get this information from cinder.conf, just like in version 1.0."
1,"For vmware vcenter driver, resize a VM, during resizing , at the same time, delete the vm, the VM-orig can not be deleted in ESXi host. So makes VM leaks."
0,"When deleting an instance if an instance has multiple ports and one
of the deletes fail nova should LOG an error and continue trying
to delete the other ports. Previously, nova would stop deleting ports
are the first non 404 error."
0,"The default values of the task_poll_interval is 5 seconds. This means that any operation against the VC will wait at least 5 seconds.
An example of this - a spawn operation would take on average take 25 seconds. When this parameter what changed to 0.2 - 1.0 seconds the operation would take on average 9 seconds."
0,"Downgrading the cinder schema fails when running 018_add_qos_specs.py under MySQL. The upgrade path of this schema patch adds the foreign key volume_types_ibfk_1 on table volume_types, and the downgrade does not correspondingly remove it before attempting to drop the qos_specs_id column."
0,"If specifying false string (""False"") as ""return_reservation_id"" parameter of create multiple servers API, Nova considers it as True.
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency."
0,"It appears that during live migration, for certain duration network is unavailable and there is a scope to reduce the network downtime.
Please refer http://paste.openstack.org/show/69718/ to check the packet loss during live migration."
1,"When using the volume initialisation with an image, not all of the image might be written back on the volume before it gets unmapped due to not flushing/avoiding disk caching on write. The problem seems to occur mostly with volumes that are provided via multipath/Fibre Channel, e.g. where the LUNs disappear without the cinder-volume guest node having full control over it (and the kernel there being able to trigger a flush in time)."
1,"I'm seeing Deadlocks when deleting large numbers of VMs in a multinode system.
The port delete fails, and then ports are left behind after the VMs are deleted.
VMs cannot be created as the IP are not released.
The ports have to be manually deleted.
2014-02-24 15:18:12.606 1819 ERROR neutron.api.v2.resource [-] delete failed
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/base.py"", line 432, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 634, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource super(OVSNeutronPluginV2, self).delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1403, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource self._delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1425, in _delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource a['ip_address'])
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 415, in _recycle_ip
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource ip_address)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 449, in _delete_ip_allocation
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource subnet_id=subnet_id).delete()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2581, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource delete_op.exec_()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 816, in exec_
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource self._do_exec()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 942, in _do_exec
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource params=self.query._params)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 531, in _wrap
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource _raise_if_deadlock_error(e, get_engine().name)"
0,"NoPermission object has a privilegeId that tells us which permission the user did not have. Presently the VMware nova driver does not log this data. This is very useful for debugging user permissions problems on vCenter or ESX.
http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.wssdk.apiref.doc/vim.fault.NoPermission.html"
1,"When I pass correct parameters to the glance-replicator tool I get the following output:

ERROR: Bad format of the given arguments.
rohit@precise-dev-102:~/devstack$ glance-replicator compare 10.2.3.1:9292 10.2.3.2:9292
replication_compare compare <fromserver:port> <toserver:port>

    Compare the contents of fromserver with those of toserver.

    fromserver:port: the location of the master glance instance.
    toserver:port: the location of the slave glance instance.

ERROR: Bad format of the given arguments.

This is due to an incorrect regular expression used int he code to match the input parameters.
The same regex is used by all the replicator functions to verify the parameters:
?https://github.com/openstack/glance/blob/master/glance/cmd/replicator.py#L58

SERVER_PORT_REGEX = '\w+:\w+'

The character class \w = [a-zA-Z0-9_]. So if we pass an IP Address or an FQDN having characters outside the set, this check will fail.

The regex should be corrected for glance-replicator to work correctly."
1,"There's a problem in the attach volume logic of the EMC SMI-S driver. The existing logic checks if a volume is already attached to a host, but it doesn check whether it is attached to the host inidicated by the connector info.
So we need to add a check to see if a volume is already attached to a specific host. If not, it will do the attach."
1,"When detaching a volume, Nova doesn't disconnect the iSCSI portal and just return for there are other volumes attaching to the host. But the mutipath mapping device descriptor is there and multipath tool sends IOs periodically to storage array. This leads to some cinder storage drivers, such as huawei, can not unmap that volume for it detect periodic IOs.
So we need to remove the multipath device descriptor in this case."
1,"Just saw that virt drivers now take an extra arg ""context"" on the destroy method. For some reason, it has not been added to the docker driver... The destroy method with the driver enabled currently fails with the following error:
2013-10-25 23:03:20.764 ERROR nova.openstack.common.rpc.amqp [req-75ff872a-fd4d-4b63-a587-93b9fd3ede4b demo demo] Exception during message handling
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 353, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp pass
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1792, in terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp do_terminate_instance(instance, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1784, in do_terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp reservations=reservations)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/hooks.py"", line 105, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp rv = f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1757, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp user_id=user_id)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1729, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp self._shutdown_instance(context, db_inst, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1662, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp requested_networks)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 1652, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp context=context)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp TypeError: destroy() got an unexpected keyword argument 'context'
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp"
0,"in nova-network, deallocate_fixed_ip function reserve quota first then do deallocate operations
if any operation failed, the quota reserve operation need to be rollback"
1,"1. create a cinder from an existed image
cinder create 2 --display-name hbvolume-newone --image-id 9769cbfe-2d1a-4f60-9806-16810c666d7f
2. set the created volume with error status
cinder reset-state --state error 76f5e521-d45f-4675-851e-48f8e3a3f039
3. boot a vm from the created volume
nova boot --flavor 2 --block-device-mapping vda=76f5e521-d45f-4675-851e-48f8e3a3f039:::0 device-mapping-test2 --nic net-id=231eb787-e5bf-4e65-a822-25d37a84eab8
# cinder list
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
| 21c50923-7341-49ba-af48-f4a7e2099bfd | available | None | 1 | None | false | |
| 76f5e521-d45f-4675-851e-48f8e3a3f039 | error | hbvolume-2 | 2 | None | true | |
| 92de3c7f-9c56-447a-b06a-a5c3bdfca683 | available | hbvolume-newone | 2 | None | true | |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
#RESULTS
it reports ""failed to get the volume""
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume 76f5e521-d45f-4675-851e-48f8e3a3f039. (HTTP 400)
#Expected Message:
report the status of the volume is not correct to boot a VM"
1,"How to reproduce:
On a setup with two L3 agents, create ten HA routers, the scheduler will place them on both agents, but the same agent will host the active instance of all ten routers. This defeats the idea of load sharing traffic across all L3 agents.
Solutions:
This can be solved in one of two ways:
1) Enable preemptive elections for HA routers. Keepalived enables a configuration value that causes VRRP pre-emptive elections. This way we can set a random VRRP priority for each router instance, and the elections process will guarantee a random distribution of active routers on the available agents. Preemptive elections have a major downside - If an agent that's hosting a master instance drops, the backup router will come in to play, but when the node is fixed the old master will re-assume its role. This second state transition is costly and redundant.
2) With non-preemptive elections the first router instance to come up will become the master. We can exploit this and send the notification from the server to the agents in a random order."
1,"When finish_revert_migration is called, the caller of
finish_revert_migration already includes context as a parameter,
but finish_revert_migration did not reuse this parameter and still
re-generate the context inside finish_revert_migration, we should
add context as a new parameter for finish_revert_migration so that
the functions inside of it can reuse context when needed.
For the function _create_domain_and_network in libvirt/driver.py,
it set context as an optional parameter, but context isn't really
an optional parameter for this method. We should always pass a
context down because it might be needed somewhere inside
_create_domain_and_network()."
0,"'createImage' server action does not work for V2.1 API.
This needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1,"running live migration with --block-migrate fails if the disk was resized before (aka detached from the cow image). This is because nova.virt.libvirt.driver.py uses disk_size, not virt_disk_size for re-creating the qcow2 file on the destination host. in the case of qcow2 files, qemu-img however needs to get the virt_disk size passed down, otherwise the block migration step will not be able to convert all blocks."
0,"cinder-api ran into hang loop in python2.6

#cinder-api
...
...
snip...
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
...
...
snip..."
1,"Problem description
===================
In the Open vSwitch neutron agent, ip_lib is currently used to list the bridges.
as userspace only vswiches do not create bridge local ports in the kernel ip_lib will not correctly detect the bridge configuration.
Proposed Change
==============
use of ip_lib blocks reuse of the Open vSwitch agent with userspace only open vSwitchs implementations.
to enable this use case ovs_lib.get_bridges should be used instead."
1,"_update_volume_status() method in cinder/volume/driver.py is not called by anyone because misspelling.
_update_volume_status() method in cinder/volume/drivers/eqlx.py is also the same.
I think it should be _update_volume_stats()."
1,"Getting the following error when trying to unlock an instance
2013-12-03 15:14:31.198 31688 DEBUG nova.compute.api [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] [instance: fb468a1d-2e64-4560-850d-31a5fd698305] Unlocking unlock /usr/lib/python2.7/dist-packages/nova/compute/api.py:2612
2013-12-03 15:14:31.200 31688 ERROR nova.cells.messaging [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] Error processing message locally: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging Traceback (most recent call last):
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 205, in _process_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 1476, in _process_message_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 708, in run_compute_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return self._run_api_method(message, method_info, fn)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 702, in _run_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 198, in wrapped
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 2615, in unlock
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging instance.save()
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 52, in save
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging session.add(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1399, in add
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_state(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1411, in _save_or_update_state
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1667, in _save_or_update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1661, in _update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._attach(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1749, in _attach
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging state.session_id, self.hash_key))
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging InvalidRequestError: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging"
1,"when unshelve a vm, if this vm has been offloaded, the process would involve re-scheduling.
in nova/conductor/manager.py def unshelve_instance(self, context, instance):
elif instance.vm_state == vm_states.SHELVED_OFFLOADED:
            try:
                with compute_utils.EventReporter(context, self.db,
                        'get_image_info', instance.uuid):
                    image = self._get_image(context,
                            sys_meta['shelved_image_id'])
            except exception.ImageNotFound:
                with excutils.save_and_reraise_exception():
                    LOG.error(_('Unshelve attempted but vm_state not SHELVED '
                                'or SHELVED_OFFLOADED'), instance=instance)
                    instance.vm_state = vm_states.ERROR
                    instance.save()
            filter_properties = {}
            hosts = self._schedule_instances(context, image,
                                             filter_properties,instance) <<<<<this re-scheduling would cause exception,when it occurs,the
         <<<<<<instance will be stuck in task_state: unshelving forever
            host = hosts.pop(0)['host']
            self.compute_rpcapi.unshelve_instance(context, instance, host,
                    image)"
0,"""Could not send notification to notifications"" errors started happening today.
probably due to oslo.messaging changes.

2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging version=self.version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 96, in _send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging self._driver.send_notification(target, ctxt, message, version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 394, in send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging envelope=(version == 2.0), notify=True)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 355, in _send
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging rpc_amqp.pack_context(msg, context)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqp.py"", line 212, in pack_context
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging context_d = six.iteritems(context.to_dict())
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 553, in iteritems
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging return iter(d.iteritems(**kw))
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'Context' object has no attribute 'iteritems'
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging"
1,"nova/compute/manager.py in do_build_and_run_instance
Under except exception.RescheduledException as e:
...
self._set_instance_error_state(context, instance.uuid)
This should be passing instance only not instance.uuid"
0,"cinder-manage loads up more stuff than it needs to, should probably look at reducing this since it's an admin command.

For one, it loads up the paramiko library since it's used in utils.py. This seems rather unnecessary.

(Noticed because the python crypto module loaded by paramiko was throwing warnings to my terminal about linking against a newer libgmp.)"
1,network_device_mtu should be IntOpt
1,"This subject came up on IRC here [1]. It relates to the blueprint about pluggable external network connections and so I jumped in.
There are two reasons that using multiple external networks to allow multiple floating ip subnets [2] is not optimal.
- Extra L2 infrastructure needed.
- A neutron router cannot have a gateway connection to more than one external network. So, floating IPs wouldn't be able to float as freely as we'd like them to.
I cracked open devstack and started playing with it. I tried this first just to add a second subnet full of floating IPs.
neutron subnet-create ext-net 10.224.24.0/24 --disable-dhcp
In devstack, I needed to add a ""gateway router"". I did this by adding an IP to the br-ex interface. In a real cloud, we'd need to configure the upstream router as a gateway on the second subnet.
sudo ip addr add 10.224.24.1/24 dev br-ex
At this point, I was able to get a router to host floating IPs on both subnets! Pretty cool! I was very surprised it worked so easily.
There is one bug which this bug report addresses! Traffic between floating IPs on the second subnet went up to the router and then back down. The upstream router sent ICMP redirect packets periodically back to the Neutron router sourcing the traffic. These did the router no good because what it really needed to know was that the IP was on link but the upstream router couldn't tell it that. Some upstream routers may not be configured to send redirects or route back through the port of origin.
The answer to this is to add an on-link route for each subnet on the external network to each router's gateway interface. This will require an L3 agent change but should not be very difficult.
[1] http://eavesdrop.openstack.org/irclogs/%23openstack-neutron/%23openstack-neutron.2014-04-08.log starting at 2014-04-08T23:23:51 (near the bottom)
[2] http://docs.openstack.org/admin-guide-cloud/content/adv_cfg_l3_agent_multi_extnet.html"
0,"The unicode() function is Python2-specific, we should use six.text_type() instead."
0," which is not necessary  we should downgrade them"""
1,"The network cache is not correctly updated when running ""nova interface-attach"": only the latest allocated IP is used. See this log:
http://paste.openstack.org/show/46643/
Nevermind the error reported when running ""nova interface-attach"": I believe it is an unrelated issue, and I'll write another bug report for it.
I noticed this issue a few months ago, but haven't had time to work on it. I'll try and submit a patch ASAP. See my analysis of the issue here: https://bugs.launchpad.net/nova/+bug/1197192/comments/3"
1,The UnexpectedTaskStateError that this currently results in should be caught and handled correctly to avoid unnecessary tracebacks in the compute log.
1,"In usual devstack setup but with the following config changes:
/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user I was able to attach the volume to vm, but as demo user it fails with require_admin_context.
 ../screen-logs/screen-c-vol.log:
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 797, in initialize_connection
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher self.driver.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher self.target_helper.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher volume['id'])
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 118, in wrapper
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher raise exception.AdminRequired()
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher AdminRequired: User does not have admin privileges
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher
 ../screen-logs/screen-n-cpu.log:
2014-03-31 10:04:25.586 ERROR nova.compute.manager [req-99dc297a-db00-4409-a634-8a17a8313a84 demo demo] [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Failed to attach a6fd9536-151d-48ac-b084-7e5419efad78 at /dev/vdc
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Traceback (most recent call last):
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4140, in _attach_volume
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] do_check_attach=False, do_driver_attach=True)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 44, in wrapped
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] ret_val = method(obj, context, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 225, in attach
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 173, in wrapper
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] res = method(self, ctx, volume_id, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 271, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 321, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] {'connector': connector})[1]['connection_info']
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 250, in _action
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] return self.api.client.post(url, body=body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 210, in post
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] return self._cs_request(url, 'POST', **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 174, in _cs_request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 157, in request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] raise exceptions.from_response(resp, body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Forbidden: User does not have admin privileges (HTTP 403) (Request-ID: req-7a872133-72f4-4765-b055-4791629d73f8)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]"
0,"There are incorrect body dict for stop actions. Such as:
def test_stop_not_ready(self):
        self.stubs.Set(db, 'instance_get_by_uuid', fake_instance_get)
        self.stubs.Set(compute_api.API, 'stop', fake_start_stop_not_ready)
        req = fakes.HTTPRequest.blank('/v2/fake/servers/test_inst/action')
        body = dict(start="""")
        self.assertRaises(webob.exc.HTTPConflict,
            self.controller._stop_server, req, 'test_inst', body)"
1,"Creating a snapshot of an instance and then trying to boot from it will result the following Hyper-V exception: ""HyperVException: WMI job failed with status 10"". Here is the trace: http://paste.openstack.org/show/47904/ .
The ideea is that Hyper-V fails to expand the image, as it gets the request to resize it to it's actual size, which leads to an error."
1,"File ""neutron/lib/python2.7/site-packages/neutron/wsgi.py"", line 98, in start
    session.get_engine(sqlite_fk=True).pool.dispose()
AttributeError: 'module' object has no attribute 'get_engine'

https://review.openstack.org/#/c/77205/

wsgi.py was not updated with these changes and execution results in an exception when get_engine from neutron.openstack.common.db.sqlalchemy.session is invoked."
1,"In recent testing, delete CG failed because it compares host@backend with host@backend#pool (in delete_consistencygroup in volume/manager.py). So extract_host needs to be called to fix this problem.
Another issue is about deleting a CG with no host. This will throw exception when extract_host(group['host']) is called in delete_consistencygroup in volume/rpcapi.py. The solution is to check the host field in consistencygroup/api.py and delete it from db there."
0,"assertEquals is deprecated in Python 2.7 , need drop it
http://docs.python.org/2/library/unittest.html#deprecated-aliases"
1,"Ex.:
             except nexenta.NexentaException as exc:
- LOG.warning(_('Cannot delete snapshot %(origin): %(exc)s'),
+ LOG.warning(_('Cannot delete snapshot %(origin)s: %(exc)s'),
                             {'origin': origin, 'exc': exc})"
1,"When creating an instance snapshot, if such instance is deleted while in the middle of the process, the snapshot may be left in the SAVING state because the instance disappears in the middle of the process or moves to the deleting task_state.
Steps to reproduce:
$ nova boot --image <image_id> --flavor <flavor> test
$ nova image-create test test-snap
$ nova delete test
The image 'test-snap' will be left in the SAVING state although it should be deleted when we detect the situation."
1,"[req-d4c97a98-2b0e-419e-83d6-0e88332a699a account1 account1] [instance: 036a26b1-7fe2-4d56-b7a2-4781e8cad696] Error: is_public
Traceback (most recent call last):
  File ""/opt/nova/nova/compute/manager.py"", line 1254, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/nova/nova/compute/manager.py"", line 394, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/nova/nova/compute/manager.py"", line 1655, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/nova/nova/compute/manager.py"", line 1652, in _spawn
    block_device_info)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2230, in spawn
    admin_pass=admin_password)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2538, in _create_image
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 184, in cache
    *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 310, in create_image
    prepare_template(target=base, max_size=size, *args, **kwargs)
  File ""/opt/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 174, in fetch_func_sync
    fetch_func(target=target, *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/utils.py"", line 654, in fetch_image
    max_size=max_size, imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 108, in fetch_to_raw
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 98, in fetch
    fetched_to_local = handler.is_local()
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/opt/nova/nova/openstack/common/fileutils.py"", line 98, in remove_path_on_error
    remove(path)
  File ""/opt/nova/nova/virt/images.py"", line 71, in _remove_image_on_exec
    image_path):
  File ""/opt/nova/nova/virt/imagehandler/__init__.py"", line 154, in handle_image
    img_locs = image_service.get_locations(context, image_id)
  File ""/opt/nova/nova/image/glance.py"", line 307, in get_locations
    if not self._is_image_available(context, image_meta):
  File ""/opt/nova/nova/image/glance.py"", line 446, in _is_image_available
    if image.is_public or context.is_admin:
  File ""/usr/local/lib/python2.7/dist-packages/warlock/model.py"", line 72, in __getattr__
    raise AttributeError(key)
AttributeError: is_public"
1,"When starting nova-compute service on node with lxc hypervisor, it failed with the following error:
 2013-12-11 03:04:41.439 ERROR nova.openstack.common.threadgroup [-] can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup x.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup return self.thread.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup return self._exit_event.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup return hubs.get_hub().switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup return self.greenlet.switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup result = function(*args, **kwargs)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup service.start()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self.manager.pre_start_hook()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 822, in pre_start_hook
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self.update_available_resource(nova.context.get_admin_context())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 4971, in update_available_resource
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup nodenames = set(self.driver.get_available_nodes())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/driver.py"", line 980, in get_available_nodes
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup stats = self.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4569, in get_host_stats
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup return self.host_state.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 429, in host_state
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self._host_state = HostState(self)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4960, in __init__
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self.update_status()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4999, in update_status
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self.driver.get_instance_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3702, in get_instance_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup caps = self.get_host_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2739, in get_host_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self._caps.host.cpu.parse_str(features)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/libvirt/config.py"", line 61, in parse_str
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup self.parse_dom(etree.fromstring(xmlstr))
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""lxml.etree.pyx"", line 2993, in lxml.etree.fromstring (src/lxml/lxml.etree.c:62980)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup File ""parser.pxi"", line 1614, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:92786)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup ValueError: can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup
This is because no cpu model info can be gotten from libvirt getCapabilities interface for lxc hypervisor."
1,"glance checks whether or not a specified store is valid, but if it is invalid the image has already been created.
I pulled the latest devstack code and then ran these commands after sourcing openrc:
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID Name Disk Format Container Format Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec ami ami 25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari ari 3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki aki 4955792
ubuntu@devstack-glance:/mnt/devstack$
ubuntu@devstack-glance:/mnt/devstack$ glance -d image-create --store s3e --disk-format raw --container-format bare --name complete_gibberish </etc/hosts
curl -i -X POST -H 'x-image-meta-container_format: bare' -H 'Transfer-Encoding: chunked' -H 'x-image-meta-store: s3e' -H 'User-Agent: python-glanceclient' -H 'x-image-meta-size: 221' -H 'x-image-meta-is_public: False' -H 'X-Auth-Token: <redacted_token>' -H 'Content-Type: application/octet-stream' -H 'x-image-meta-disk_format: raw' -H 'x-image-meta-name: complete_gibberish' -d '<open file '<stdin>', mode 'r' at 0x7f16181b6150>' http://10.4.36.1:9292/v1/images
HTTP/1.1 400 Bad Request
date: Thu, 12 Dec 2013 12:47:37 GMT
content-length: 52
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-c9bad6ee-d79c-41f3-bd96-d3929afd742c
400 Bad Request
Store for scheme s3e not found
Request returned failure status.
400 Bad Request
Store for scheme s3e not found
    (HTTP 400)
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID Name Disk Format Container Format Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
b26c03e4-7cdf-44fe-9187-7de315c9b38b complete_gibberish raw bare 221
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec ami ami 25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari ari 3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki aki 4955792
This problem occurs using the v1 API. If using the V2 API the '--store' option does not seem to be present."
1,"Cluster has 2 hosts, with each host having 24 cores.
Both hosts are active.
Stats update is correct

Snippet of nova-compute.log

-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free ram (MB): 57582
2014-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 99
2014-05-07 03:14:28.002 AUDIT nova.compute.resource_tracker [-] Free VCPUS: 48
2014-05-07 03:14:28.021 INFO nova.compute.resource_tracker [-] Compute_service record updated for sagar-devstack:domain-c162(Demo-Pulsar-Cluster-DRS)
2014-05-07 03:14:28.021 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=28072) inner /opt/stack/nova/nova/openstack/common/lockutils.py:252

Put one of the hosts in Maintenance mode, stats update does not ignore host in maintenance mode and the free vCPUS is still 48.
It should be 24"
1,"metadata agent had tried to cache auth info by the means of ""self.auth_info = qclient.get_auth_info()"" in _get_instance_and_tenant_id(), however this qclient is not the exact one which would be used in inner methods, In short, metadata agent does not implement auth info caching correctly but still retrieves new token from keystone every time."
0,"Library guestfs is not in requirements file, but is imported directly
in nova/virt/disk/vfs/guestfs.py. That conflicts with global variable
guestfs, leads out one more time import and shadow issue, so remove it
from import group."
1,"While investigating https://bugs.launchpad.net/neutron/+bug/1210664, salvatore-orlando discovered that a db exception was being raised during router syncing:
Traceback (most recent call last):
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
    **args)
  File ""/opt/stack/neutron/neutron/common/rpc.py"", line 44, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
    plugin.auto_schedule_routers(context, host, router_ids)
  File ""/opt/stack/neutron/neutron/db/agentschedulers_db.py"", line 303, in auto_schedule_routers
    self, context, host, router_ids)
  File ""/opt/stack/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
    context.session.add(binding)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
    self.commit()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
    self._prepare_impl()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
    self.session.flush()
  File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('2df68c3d-f3c9-43d2-bf45-e2e57e84b054', 'c4502f1f-a093-4c7c-b161-929b6342509b', '85d6b60f-f3ff-4437-8f5a-af165087f3ea')
This can be reproduced by running the quantum smoke test in tempest (nosetests tempest/scenario/test_network_basic_ops.py). The smoke test passes - the exception only occurs at test cleanup. It may be that the router syncing code is working with stale state after router deletion."
0,"The cisco_vlan_ids table is no longer used.
Remove __init__ and __repr__ methods from models since they are provided by model_base."
0, after adding metadata and performing an add-host you get the following stacktrace exception:
1,"When deleting a resizing instance that has not yet finished resizing, quotas should be adjusted for the old flavor type. Instead it incorrectly use values from the new flavor.
This was originally reported and fixed in bug 1099729 but has since resurfaced with the move to objects (commit dce64683291ba2cdb5e6617e01ccc2909254acb4). This was made possible by a prior change (commit a56f0b33069b919ebb24c4afdcc6b6c31592c98e) that accidentally removed the test put in place to guard against this error ever happening again."
0,"I noticed this while reviewing Ic2c87174. When I read through log
files, I don't want to see errors like this that come from validating
bad user input. Debug severity is more appropriate."
0,"hp_3par_common calls time.sleep()
It should be calling eventlet.greenthread.sleep() to make sure we don't block the volume manager."
1,"Trying to remove a security group from an instance which is not actually associated with the instance produces the following:
---
$nova remove-secgroup 71069945-5bea-4d53-b6ab-9026bfeebba4 phil
ERROR: [u'Security group %(security_group_name)s not assocaited with the instance %(instance)s', {u'instance': u'71069945-5bea-4d53-b6ab-9026bfeebba4', u'security_group_name': u'phil'}] (HTTP 404) (Request-ID: req-a334b53d-e7cc-482c-9f1f-7bc61b8367e0)
---
The variables are not being populated correctly, and there is a typo: "" assocaited"""
0,"Some hacking violations have come up after the FCZM landed and need to be addressed. John commented on several of them in the post merge review for this patch.
https://review.openstack.org/#/c/76011"
1,"The extension module for the L3_ext_gw_mode extension has:
- a RouterDNATdisabled exception which is not used anywhere
- a description which refers to disabling DNAT, which is not allowed by the extension
The above are 'vestigial' items from a previous release of the extension which allowed to disable DNAT (floating IPs)"
0,oslo.messaging provides a factory method to create a Server object. It should be used to create it. Additionaly Neutron uses custom RPCDispatcher to log incoming messages. This duplicates existing functionality in oslo.messaging.
1,"When the PowerVMFileTransferFailed occurs in
PowerVMLocalVolumeAdapter.create_volume_from_image, in some cases the
message is incorrect."
0,"1. I add special char like """"!@#$%^&*"" in volume ""metadata = {""key1"": ""value1"",
                    : ""!@#$%^&*""} .Then try to insert it into volume.
2. I test it successfully in volumes_client.create_volume_metadata Json function.
3. When I test it in XML function, it report error like below
Traceback (most recent call last):
  File ""/tempest/tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
    metadata)
  File ""/tempest/tempest/services/volume/xml/volumes_client.py"", line 381, in create_volume_metadata
    self.headers)
  File ""/tempest/tempest/common/rest_client.py"", line 302, in post
    return self.request('POST', url, headers, body)
  File ""/tempest/tempest/common/rest_client.py"", line 436, in request
    resp, resp_body)
  File ""/tempest/tempest/common/rest_client.py"", line 530, in _error_checker
    raise exceptions.ServerFault(message)
ServerFault: Got server fault
Details: The server has either erred or is incapable of performing the requested operation.
4. After our investigation, we find when we remove special char ""&"" , it will be ok. So I think in XML function, it can not translate these special char to correct format.
I also run it in tempest, it also run same error
FAIL: tempest.api.volume.test_volume_metadata.VolumeMetadataTestXML.test_create_volume_metadata_specialchart_blank[gate
2014-01-13 11:13:15.079 | Traceback (most recent call last):
2014-01-13 11:13:15.079 | File ""tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
2014-01-13 11:13:15.079 | metadata)
2014-01-13 11:13:15.079 | File ""tempest/services/volume/xml/volumes_client.py"", line 380, in create_volume_metadata
2014-01-13 11:13:15.079 | self.headers)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 302, in post
2014-01-13 11:13:15.080 | return self.request('POST', url, headers, body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 436, in request
2014-01-13 11:13:15.080 | resp, resp_body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 530, in _error_checker
2014-01-13 11:13:15.080 | raise exceptions.ServerFault(message)
2014-01-13 11:13:15.080 | ServerFault: Got server fault
2014-01-13 11:13:15.080 | Details: The server has either erred or is incapable of performing the requested operation"
1,"The VMware Minesweeper CI occasionally runs into this error when trying to boot an instance:
2013-12-11 04:50:15.048 20785 DEBUG nova.virt.vmwareapi.driver [-] Task [ReconfigVM_Task] (returnval){
   value = ""task-322""
   _type = ""Task""
 } status: success _poll_task /opt/stack/nova/nova/virt/vmwareapi/driver.py:926
Reconfigured VM instance to enable vnc on port - 5986 _set_vnc_config /opt/stack/nova/nova/virt/vmwareapi/vmops.py:1461
Instance failed to spawn
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1461, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 628, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 435, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1556, in _check_if_folder_file_exists
    ""browser"")
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 173, in get_dynamic_property
    property_dict = get_dynamic_properties(vim, mobj, type, [property_name])
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 179, in get_dynamic_properties
    obj_content = get_object_properties(vim, None, mobj, type, property_names)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 168, in get_object_properties
    options=options)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 187, in vim_request_handler
    fault_checker(response)
  File ""/opt/stack/nova/nova/virt/vmwareapi/error_util.py"", line 99, in retrievepropertiesex_fault_checker
    exc_msg_list))
VimFaultException: Error(s) NotAuthenticated occurred in the call to RetrievePropertiesEx
Full logs here for a CI build where this occurred are available here: http://162.209.83.206/logs/35303/31/"
0,"because libvirt lvm volumes are based on instance['name'], it means that the actual names used in lvm storage are based on an operator configuration variable: instance_name_template
the default is 'instance-%08x'
however this is site changable, and changable at any time. This creates 2 failure modes.
#1) operator changes this, the result is all volumes created before the change are no longer able to be cleaned up by nova
#2) operator has changed this to something that includes end user input, like %(display_name), which would allow one user to impact another (use A has display name ""bob"", user B has displayname ""bob_joe"") because of https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L1068
specifically:
            pattern = '%s_' % instance['name']
            def belongs_to_instance(disk):
                return disk.startswith(pattern)
#2 is a non default situation, and requires specific config by an adminstrator and specific naming by users, but it should be protected against.
A much better approach would be to use instance['uuid'] which has no operator or user impact on naming."
1,"There were some minor issues in initial commit of VNX Direct Driver
Juno Update (https://review.openstack.org/#/c/104413/), such as typo,
unclear config option help message and missing period.
This bug is to track these issues."
1,"The logical volume can not be removed when delete VM error. I look at the code, found that parameter is a list in the libvirt's lvm, but in imagebackend, parameters passed is a string.
in the Libvirt's LVM
def remove_volumes(paths): ## #the path is list
    """"""Remove one or more logical volume.""""""
    errors = []
    for path in paths:
        clear_volume(path)
        lvremove = ('lvremove', '-f', path)
        try:
            utils.execute(*lvremove, attempts=3, run_as_root=True)
        except processutils.ProcessExecutionError as exp:
            errors.append(str(exp))
    if errors:
        raise exception.VolumesNotRemoved(reason=(', ').join(errors))
in the imagebackend's LVM
 @contextlib.contextmanager
    def remove_volume_on_error(self, path):
        try:
            yield
        except Exception:
            with excutils.save_and_reraise_exception():
                lvm.remove_volumes(path) ### the path is string"
0,"Running tempest against juno code on RHEL 6.5 (python 2.6), I'm seeing this in the cinder-volume logs:
I'm seeing several warnings from the taskflow code in the cinder volume log:
2014-07-17 02:36:27.703 57746 WARNING taskflow.utils.misc [req-b459108b-be18-4922-94b9-1f0281764bfb 4deacb6f0bb7409cb1bfab8d7080e61f 574d6c38c2fd4ce697071225ecdf2125 - - -] Failure calling callback <bound method DynamicLogListener._task_receiver of <cinder.flow_utils.DynamicLogListener object at 0x9997d4e0>> to notify about event SUCCESS, details: {'task_uuid': '6ef1507b-7749-40ab-8d5e-55bbf9347229', 'result': None, 'task_name': 'cinder.volume.flows.manager.create_volume.CreateVolumeOnFinishTask;volume:create, create.end'}
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc Traceback (most recent call last):
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc File ""/usr/lib/python2.6/site-packages/taskflow/utils/misc.py"", line 596, in notify
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc callback(event_type, *args, **kwargs)
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc File ""/usr/lib/python2.6/site-packages/cinder/flow_utils.py"", line 104, in _task_receiver
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc if (self._logger.isEnabledFor(base_logging.DEBUG) or
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc AttributeError: ContextAdapter instance has no attribute 'isEnabledFor'
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc
This is basically the same issue as keystone bug 1213284.
It looks like a limitation with logging in python 2.6.
We could just add isEnabledFor to the oslo log ContextAdapter in py26 for now to delegate to logger.isEnabledFor or punt (return False) until this is all moving to the server projects with oslo.log:
https://blueprints.launchpad.net/oslo/+spec/remove-context-adapter"
1,"Performing the secure delete operation on volumes and snapshots writes zeros (or other patterns) across the entire surface of an LVM volume on delete calls. This is pointless with thin provisioning, and in fact results in defeating the purpose of thin provisioning as it requires actual allocation of all of the blocks.
Add a check before calling lvm.clear_volume and skip this step if thin provisioning is configured."
1,"Though parameters of this api are 'updated_since' , 'project_id' and 'deleted',
the error message is ""Only 'updated_since' and 'project_id' are understood.""
It shoud be ""Only 'updated_since', 'project_id' and 'deleted' are understood."""
1,"It appears that using the utils.is_neutron() function can create race conditions, if not used carefully. Review #56381 was attempting to check the presence of Neutron, in order to determine if hairpinning should be enabled on the bridge that Nova creates. Only after creating a new configuration flag in Nova.conf and checking the setting, were the unit tests passing and the race condition resolved.
My hunch is that the global variable that utils.is_neutron( ) creates is the root cause of the issue.
https://review.openstack.org/#/c/56381/"
0,"in the nova/virt/baremetal/driver.py the call to pxe.py pxe.get_pxe_config_file_path(instance) need to return the bootfile name not the path.
Line 84 reads only my_ip from nova conf. But, bootfile_path in line 512 (bootfile_path = self.driver.get_pxe_config_file_path(instance)) is getting set from pxe.py which is getting the value ?tftpboot/<instance-id>/config? This value should actually be xelinux.0?(or absolute location of pxelinux.0) . baremetal does not find the bootfile in ?tftpboot/<instance-id>/config? it finds it in /tftpboot/pxelinux.0 (where tftproot=/tftpboot set in nova.conf).
In pxe.py,
def get_pxe_config_file_path(instance):
    """"""Generate the path for an instances PXE config file.""""""
    return os.path.join(CONF.baremetal.tftp_root, instance['uuid'], 'config')"
0,"In nec plugin with l3-agent (icehouse) AttributeError: No such RPC function 'update_floatingip_statuses' occurs.

update_floatingip_statuses was implemented in Icehouse and RPC callback version related to L3RpcCallbackMixin was bumped to 1.1, but the version of L3RpcCallback in NEC plugin was not bumped to 1.1 yet.
update_floatingip_statues RPC call from l3-agent expects RPC version 1.1."
1,"2014-02-18 13:11:51.714 ERROR NeutronPlugin [-] Unable to update port id: 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7.
2014-02-18 13:11:51.714 TRACE NeutronPlugin Traceback (most recent call last):
2014-02-18 13:11:51.714 TRACE NeutronPlugin File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1316, in update_port
2014-02-18 13:11:51.714 TRACE NeutronPlugin nvp_port_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin File ""/opt/stack/neutron/neutron/plugins/nicira/nvplib.py"", line 1019, in get_port_status
2014-02-18 13:11:51.714 TRACE NeutronPlugin port_id=port_id, net_id=lswitch_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7 could not be found on network 0d192d2e-e64f-410e-8d3b-cb2eef5a2655
2014-02-18 13:11:51.714 TRACE NeutronPlugin"
1,"When a build fails in the driver spawn method attached volumes are not detached. If the instance goes to ERROR and is later deleted everything gets cleaned up appropriately. If the instance is rescheduled then the next compute will fail with:
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Traceback (most recent call last):
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 1786, in _prep_block_device
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] self.driver, self._await_block_device_map_created) +
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 368, in attach_block_devices
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] map(_log_and_attach, block_device_mapping)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 366, in _log_and_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] bdm.attach(*attach_args, **attach_kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 45, in wrapped
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] ret_val = method(obj, context, *args, **kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 218, in attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] volume_api.check_attach(context, volume, instance=instance)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 249, in check_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] raise exception.InvalidVolume(reason=msg)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] InvalidVolume: Invalid volume: status must be 'available'
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]
2014-06-18 20:09:02.002 11008 ERROR nova.compute.manager [req-e76e85f6-0520-4372-b47d-a80744c912a7 None] [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Failure prepping block device
which stops the build and properly stops a reschedule.
Cinder volumes need to be detached on a build failure."
1,"Nova needs to upload streamOptimized disks to Glance. These streamOptimized disks are converted/compressed on the fly by
vCenter. Consequently, it is not possible to know the size of the Glance image before upload. Without specifying the size
or size zero, vCenter will reject the request (Broken Pipe)."
1,"Sudhakar Gariganti has noticed that with a very large number of iptables rules that _modify_rules() was taking so long to complete (140 seconds) that VMs couldn't be reliably booted because the rules weren't getting put in place before the initial DHCP requests had timed out. With a small change the update can be done much quicker, and also allow each node to support a larger set of iptables rules.
I've included a snippet from the related bug for reference, https://bugs.launchpad.net/neutron/+bug/1253993
""We have done significant testing with this patch and want to share few results from our experiments.
We were basically trying to see how many VMs we can scale with the OVS agent in use. With default security groups(which has remote security group), beyond 250-300 VMs, VMs were not able to get DHCP IPs. We were having 16 CNs, with VMs uniformly distributed across them. The VM image had a wait period of 120 secs to receive the DHCP response.
By the time we have around 18-19 VMs on each CN(there were around 6k Iptable rules), each RPC loop was taking close to 140 seconds(if there is any update). And the reason VMs were not getting IPs was that the Iptable rules required for the VM to send out the DHCP request were not in place before the 120 secs wait period. Upon further investigations we discovered that the ""for loop searching iptable rules"" in _modify_rules method of iptables_manger.py is eating a big chunk of the overall time spent.
After this patch, we were able to see close to 680 VMs were able to get IPs. The number of Iptable rules at this point was close to 20K, with around 40 VMs per CN.
To summarize, we were able to increase the processing capability of compute node from 6K Iptable rules to 20K Iptable rules, which helped more VMs get DHCP IP within the 120 sec wait period. You can imagine the situation when the wait time is less than 120 secs."""
0,"For Cisco Neutron plugin:
Alembic migration from revision f44ab9871cd6 to 2eeaf963a447 fails because the floatingips table doesn't exist.
The fix is to add the plugin to the DB migration path for L3."
1,"the Nova has the logic to synchronize the state of virtual machine with the record in database, when the state is stopped but
the virtual machine is in running state, it will try to stop the virtual machine with compute api call. But the compute api
call has the check that only allow to execute the call when VM in ACTIVE,RESCUED, ERROR state. So the sync logic is broken here.
option 1:
   allow to run stop API when VM in stopped state
option 2:
   add another method in API such as force_stop , and sync_power_states will use this api to stop the VM.
option 3:
    sync_power_states to call the rpcapi to stop the VM"
1,"the change to the default value of state_path introduced by
I94502bcfac8b372271acd0dbc1710c0e3009b8e1 for the reasons set out
in my -1 review of the same that seems to have been skipped when the
change was accepted.
As implemented the change will break any existing systems that are using
the default value of state_path with no warning period, which goes beyond
the scope of change for UpgradeImpact"
1,"after recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665, non tapXXX device handling is broken.
(the corresponding gerrit review is https://review.openstack.org/#/c/100404/)"
0,"The following test cases in TestSecurityGroups of test_extension_security_group.py file
are making multiple calls to create a SG rule.
test_create_security_group_rule_min_port_greater_max()
test_create_security_group_rule_ports_but_no_protocol()
test_create_security_group_rule_port_range_min_only()
test_create_security_group_rule_port_range_max_only()
test_create_security_group_rule_icmp_type_too_big()
test_create_security_group_rule_icmp_code_too_big()
The redundant calls can be removed."
1,"There's a missing raise statement when checking the ConfigDrive format:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L283"
1,"iptables_manager permits to use a variable begining by a $ in order to reference a wrapped chain, the issue is that the remove rule method doesn't expand this variable, thus if a rule has been added with a variable, it is impossible to remove it since the iptables manager compares the expanded rule and the not expanded one, of course not equal."
0,"Currently the V2 API returns volume summary after volume create which requires the cinderclient to invoke additional GET request to get additional details.
This bug is to update the summary method to detail and remove the additional get from cinderclient."
0,"Cannot create new instances because of an internal error. It seems like that the docker client returns None instead of a empty array if not a single container is started.
Update:
The root cause for the issue is that the docker v1.8 rest API doesn't longer deliver the container list for v1.4 api calls. We must upgrade to >= 1.7 in order to make the docker driver working again.
Additional does docker not set the Content-Type correctly. The docker client expects that the Content-Type is application/json but it is plain/text. The parsing of the content will be skipped in this case.
Please see also: https://github.com/dotcloud/docker/pull/3974
Stacktrace:
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 821, in init_host
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup self._destroy_evacuated_instances(context)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 532, in _destroy_evacuated_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup local_instances = self._get_instances_on_driver(context, filters)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/compute/manager.py"", line 511, in _get_instances_on_driver
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup driver_instances = self.driver.list_instances()
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup File ""/opt/stack/nova/nova/virt/docker/driver.py"", line 96, in list_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup for container in self.docker.list_containers():
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup"
0,There are many violations of this rule. Originally it was intended to only fix H302 but flake8 will not check for H302 unless H301 is enabled as well. So must fix them both at the same time.
0,"instance is not really a user concept, the user concept, in the api, is servers.

Maybe in the v3 api we should rename instance actions to ""server actions""."
1,"Stacktrace here:
http://paste.openstack.org/show/60785/
Occurence here:
http://162.209.83.206/logs/58017/16/logs/screen-q-svc.txt.gz"
0,Added functionality to schedule routers and networks.
1,"In some situations it may be possible for the lun on an eseries controller to be mapped, but for cinder to not know about it. In this situation, if you then attempt certain operations in cinder (such as upload-to-image or create volume from image), cinder will attempt to attach the lun to the cinder node. This will fail because the eseries controller returns back an error that the lun is already mapped.
2014-04-18 12:30:04.424 DEBUG cinder.volume.drivers.netapp.eseries.client [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Invoking rest with method: POST, path: /storage-syste
ms/{system-id}/volume-mappings, data: {'mappableObjectId': u'0200000060080E500023BB340000C7C65350FAAF', 'targetId': u'8400000060080E500023C734003024D55350F8AA', 'lun': 1}, use_system: True, timeout: None, verify: False, kwargs: {}. from
 (pid=30195) _invoke /opt/stack/cinder/cinder/volume/drivers/netapp/eseries/client.py:123
2014-04-18 12:30:04.426 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Setting read timeout to None from (pid=30195) _make_request /usr/lib/pytho
n2.7/dist-packages/urllib3/connectionpool.py:375
2014-04-18 12:30:04.809 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] ""POST /devmgr/v2/storage-systems/f79b215b-b502-43b7-800c-9b6a08c7086b/volu
me-mappings HTTP/1.1"" 422 None from (pid=30195) _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-04-18 12:30:04.812 ERROR cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.812 DEBUG cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Cleaning up failed connect initialization. from (pid=30195) _attach_volume /
opt/stack/cinder/cinder/volume/driver.py:399
2014-04-18 12:30:04.862 ERROR oslo.messaging.rpc.dispatcher [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Exception during message handling: Bad or unexpected response from
the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/manager.py"", line 719, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher payload['message'] = unicode(error)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/manager.py"", line 713, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher image_meta)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/driver.py"", line 355, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher attach_info = self._attach_volume(context, volume, properties)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/driver.py"", line 406, in _attach_volume
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher raise exception.VolumeBackendAPIException(data=err_msg)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher ""retcode"" : ""105"",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher ""codeType"" : ""symbol""
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher }.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher
2014-04-18 12:30:04.865 ERROR oslo.messaging._drivers.common [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Returning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}. to caller"
0,"In the function neutron.db.l3_db.get_assoc_data(),
we get router_id first, then check whether this router is connecting to external network.

But the function we used to get router_id -- neutron.db.l3_db._get_router_for_floatingip(), has already checked this.

As it names says, this function is designed for get router_id, which can connect to floating network."
0,"An extra entry of invalid data is passes to the image cache aging:
USED: {'': (2, 0, ['instance-0000000a', 'instance-0000000a']), '7ee53435-b6d5-4c15-bce4-2f3dfac96ffd': (1, 0, ['instance-0000000a'])}
This is due to the change to objects."
1,"We use wrong lock name at here:
    def prepare_for_instance_event(self, instance, event_name):
        """"""Prepare to receive an event for an instance.
        This will register an event for the given instance that we will
        wait on later. This should be called before initiating whatever
        action will trigger the event. The resulting eventlet.event.Event
        object should be wait()'d on to ensure completion.
        :param instance: the instance for which the event will be generated
        :param event_name: the name of the event we're expecting
        :returns: an event object that should be wait()'d on
        """"""
        @utils.synchronized(self._lock_name)
        def _create_or_get_event():
            if instance.uuid not in self._events:
                self._events.setdefault(instance.uuid, {})
            return self._events[instance.uuid].setdefault(
                event_name, eventlet.event.Event())
        LOG.debug('Preparing to wait for external event %(event)s',
                  {'event': event_name}, instance=instance)
        return _create_or_get_event()
We should invoke self._lock_name, not pass it as name.
So will get log message as below:
2014-06-16 17:44:59.022 DEBUG nova.openstack.common.lockutils [req-97211458-bae1-473b-a3ad-47fd153ae30a admin admin] Got semaphore ""<function _lock_name at 0x7fe6a7edec08>"" from (pid=30672) lock /opt/stack/nova/nova/openstack/common/lockutils.py:168
Same problem for pop_instance_event and clear_events_for_instance"
1,"If a non-admin user tries to update an attribute, which should be updated only by admin, from a non-default value to default, the update is successfully performed and PolicyNotAuthorized exception is not raised.

The reason is that when a rule to match for a given action is built there is a verification that each attribute in a body of the resource is present and has a non-default value. Thus, if we try to change some attribute's value to default, it is not considered to be explicitly set and a corresponding rule is not enforced."
0," instead of this method must be used nms.folder.create_with_props"""
1,"In our production environment (2013.2.1), we're facing a random error thrown while starting nova-compute in Hyper-V nodes.
The following exception is thrown while calling '_destroy_evacuated_instances':
16:30:58.802 7248 ERROR nova.openstack.common.threadgroup [-] 'NoneType' object is not iterable
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
(...)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup File ""C:\Python27\lib\site-packages\nova\compute\manager.py"", line 532, in _get_instances_on_driver
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup name_map = dict((instance['name'], instance) for instance in instances)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
Full trace: http://paste.openstack.org/show/73243/
Our first guess is that this problem is related with number of instances in our deployment (~3000), they're all fetched in order to check evacuated instances (as Hyper-V is not implementing ""list_instance_uuids"").
In the case of KVM, this error is not happening as it's using a smarter method to get this list based on the UUID of the instances.
Although this is being reported using Hyper-V, it's a problem that could occur in other drivers not implementing ""list_instance_uuids"""
1,"I enable one cinder volume node with GPFS driver. Creating/deleting volume works correctly. However, deleting volume snapshot operation fails.
[root@zhaoqin-RHEL-GPFS1 install]# cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 88a10b05-78d9-495d-beb4-52863c016638 | available | zhaoqin-lvm | 1 | lvm | false | |
| d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | available | zhaoqin-gpfs | 1 | gpfs | false | |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-create --display_name=gpfs_snapshot d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b
+---------------------+--------------------------------------+
| Property | Value |
+---------------------+--------------------------------------+
| created_at | 2013-10-21T07:20:36.284189 |
| display_description | None |
| display_name | gpfs_snapshot |
| id | 6274e3a0-24f0-47dc-914d-235a13047b6a |
| metadata | {} |
| size | 1 |
| status | creating |
| volume_id | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| ID | Volume ID | Status | Display Name | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | available | gpfs_snapshot | 1 |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-delete 6274e3a0-24f0-47dc-914d-235a13047b6a
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| ID | Volume ID | Status | Display Name | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | error_deleting | gpfs_snapshot | 1 |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+"
1,"When launching an instance with block_device_mapping_v2, if source_type=""image"", destination_type=""volume"" and device_type=""cdrom"", the block device is attached as ""disk"" running on ""ide"" bus. Expected to be ""cdrom"" on ""ide"" bus.

example with python-novaclient: https://dpaste.de/uojBC/
libvirt.xml generated from above command: https://dpaste.de/bSw71/"
0,"High Precision Event Timer is x86 specific hardware design to replace older PIT and RTC.
Also, '-no-hpet' option makes qemu to fail on non x86 targets.
he libvirt's xml generated has the following:
<timer name=""hpet"" present=""no""/>
The error produced...
libvirtError: internal error: process exited while connecting to monitor: Option no-hpet not supported for this target
For non x86 arch, this bug is affecting test_server_basicops test in Tempest:
tempest.scenario.test_server_basic_ops.TestServerBasicOps.test_server_basicops
No valid host was found."
0,"2014-03-11 18:50:26.524 | FAIL: cinder.tests.api.contrib.test_admin_actions.AdminActionsTest.test_force_delete_snapshot
2014-03-11 18:50:26.524 | tags: worker-0
2014-03-11 18:50:26.524 | ----------------------------------------------------------------------
2014-03-11 18:50:26.524 | Empty attachments:
2014-03-11 18:50:26.524 | pythonlogging:''-1
2014-03-11 18:50:26.524 | stderr
2014-03-11 18:50:26.524 | stdout
2014-03-11 18:50:26.524 |
2014-03-11 18:50:26.525 | pythonlogging:'': {{{
2014-03-11 18:50:26.525 | Starting cinder-volume node (version 2014.1)
2014-03-11 18:50:26.525 | Starting volume driver FakeISCSIDriver (2.0.0)
2014-03-11 18:50:26.525 | volume cedd21ea-cd89-45ca-82d8-5a7cf1e0e6b5: skipping export
2014-03-11 18:50:26.525 | Updating volume status
2014-03-11 18:50:26.525 | Initializing extension manager.
2014-03-11 18:50:26.525 | Loaded extension: os-vol-tenant-attr
2014-03-11 18:50:26.525 | Loaded extension: os-types-extra-specs
2014-03-11 18:50:26.525 | Loaded extension: os-vol-host-attr
2014-03-11 18:50:26.525 | Loaded extension: os-volume-encryption-metadata
2014-03-11 18:50:26.525 | Loaded extension: OS-SCH-HNT
2014-03-11 18:50:26.525 | Loaded extension: os-availability-zone
2014-03-11 18:50:26.526 | Loaded extension: os-vol-image-meta
2014-03-11 18:50:26.526 | Loaded extension: os-snapshot-actions
2014-03-11 18:50:26.526 | Loaded extension: os-quota-sets
2014-03-11 18:50:26.526 | Loaded extension: os-volume-actions
2014-03-11 18:50:26.526 | Loaded extension: os-volume-manage
2014-03-11 18:50:26.526 | Loaded extension: os-image-create
2014-03-11 18:50:26.526 | Loaded extension: qos-specs
2014-03-11 18:50:26.526 | Loaded extension: backups
2014-03-11 18:50:26.526 | Loaded extension: encryption
2014-03-11 18:50:26.526 | Loaded extension: os-used-limits
2014-03-11 18:50:26.526 | Loaded extension: os-types-manage
2014-03-11 18:50:26.526 | Loaded extension: os-vol-mig-status-attr
2014-03-11 18:50:26.527 | Loaded extension: os-extended-services
2014-03-11 18:50:26.527 | Loaded extension: os-quota-class-sets
2014-03-11 18:50:26.527 | Loaded extension: os-volume-transfer
2014-03-11 18:50:26.527 | Loaded extension: os-volume-unmanage
2014-03-11 18:50:26.527 | Loaded extension: os-hosts
2014-03-11 18:50:26.527 | Loaded extension: os-extended-snapshot-attributes
2014-03-11 18:50:26.527 | Loaded extension: os-services
2014-03-11 18:50:26.527 | Loaded extension: os-admin-actions
2014-03-11 18:50:26.527 | POST http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action
2014-03-11 18:50:26.527 | http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action returned with HTTP 202
2014-03-11 18:50:26.527 | Arguments dropped when creating context: {'user': 'admin', 'tenant': 'fake'}
2014-03-11 18:50:26.527 | snapshot 5ccb04ea-6a62-4e57-9299-e8ae34ecc34b: deleting
2014-03-11 18:50:26.528 | Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Exception during message handling: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Traceback (most recent call last):
2014-03-11 18:50:26.528 | File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-11 18:50:26.528 | incoming.message))
2014-03-11 18:50:26.528 | File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-11 18:50:26.528 | return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-11 18:50:26.528 | File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-11 18:50:26.528 | result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-11 18:50:26.528 | File ""cinder/volume/manager.py"", line 166, in lso_inner1
2014-03-11 18:50:26.528 | return lso_inner2(inst, context, snapshot_id, **kwargs)
2014-03-11 18:50:26.529 | File ""cinder/openstack/common/lockutils.py"", line 247, in inner
2014-03-11 18:50:26.529 | retval = f(*args, **kwargs)
2014-03-11 18:50:26.529 | File ""cinder/volume/manager.py"", line 165, in lso_inner2
2014-03-11 18:50:26.529 | return f(*_args, **_kwargs)
2014-03-11 18:50:26.529 | File ""cinder/volume/manager.py"", line 542, in delete_snapshot
2014-03-11 18:50:26.529 | {'status': 'error_deleting'})
2014-03-11 18:50:26.529 | File ""cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-11 18:50:26.529 | six.reraise(self.type_, self.value, self.tb)
2014-03-11 18:50:26.529 | File ""cinder/volume/manager.py"", line 530, in delete_snapshot
2014-03-11 18:50:26.529 | self.driver.delete_snapshot(snapshot_ref)
2014-03-11 18:50:26.529 | File ""cinder/volume/drivers/lvm.py"", line 252, in delete_snapshot
2014-03-11 18:50:26.529 | self._delete_volume(snapshot, is_snapshot=True)
2014-03-11 18:50:26.530 | File ""cinder/volume/drivers/lvm.py"", line 128, in _delete_volume
2014-03-11 18:50:26.530 | self._clear_volume(volume, is_snapshot)
2014-03-11 18:50:26.530 | File ""cinder/volume/drivers/lvm.py"", line 155, in _clear_volume
2014-03-11 18:50:26.530 | raise exception.VolumeBackendAPIException(data=msg)
2014-03-11 18:50:26.530 | VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.530 | }}}
2014-03-11 18:50:26.530 |
2014-03-11 18:53:19.295 | Traceback (most recent call last):
2014-03-11 18:53:19.295 | File ""cinder/tests/api/contrib/test_admin_actions.py"", line 312, in test_force_delete_snapshot
2014-03-11 18:53:19.296 | snapshot['id'])
2014-03-11 18:53:19.296 | File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 393, in assertRaises
2014-03-11 18:53:19.296 | self.assertThat(our_callable, matcher)
2014-03-11 18:53:19.296 | File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-11 18:53:19.296 | raise mismatch_error
2014-03-11 18:53:19.296 | MismatchError: <function snapshot_get at 0x2348848> returned <cinder.db.sqlalchemy.models.Snapshot object at 0x6d02050>
http://logs.openstack.org/68/79568/1/gate/gate-cinder-python27/b823a23/console.html"
1,"Traceback (most recent call last):
File ""/opt/stack/nova/nova/compute/manager.py"", line 2727, in rescue_instance
rescue_image_meta, admin_password)
File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 688, in rescue
_vmops.rescue(context, instance, network_info, image_meta)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1026, in rescue
None, None, network_info)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 523, in spawn
cookies)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 566, in _create_config_drive
extra_md=extra_md)
File ""/opt/stack/nova/nova/api/metadata/base.py"", line 145, in __init__
obj_base.obj_to_primitive(instance))
File ""/opt/stack/nova/nova/conductor/api.py"", line 302, in get_ec2_ids
return self._manager.get_ec2_ids(context, instance)
File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 456, in get_ec2_ids
instance=instance_p)
File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
return self._invoke(self.proxy.call, ctxt, method, **kwargs)
File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
return cast_or_call(ctxt, msg, **self.kwargs)
File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
result = rpc.call(context, real_topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
return _get_impl().call(CONF, context, topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
rpc_amqp.get_connection_pool(conf, Connection))
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
rv = list(rv)
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
raise result
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'
Traceback (most recent call last):
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
**args)
File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
result = getattr(proxyobj, method)(ctxt, **kwargs)
File ""/opt/stack/nova/nova/conductor/manager.py"", line 521, in get_ec2_ids
ec2_ids['instance-id'] = ec2utils.id_to_ec2_inst_id(instance['uuid'])
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 193, in id_to_ec2_inst_id
return id_to_ec2_id(instance_id)
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 181, in id_to_ec2_id
return template % int(instance_id)
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'"
0,"Now that the DB is healed, neutron-db-manage revision --autogenerate needs to be updated.
The template should do unconditional upgrade/downgrade.
The env.py should include all models from head to compare against the schema."
0,"we need to to check the value of the configuration item glance_num_retries in the code in order to ensure the ""eqlx_cli_max_retries "" equal or bigger than 0"
1,"I just try to create an image by giving size =-1
and the image is creating succesfully.
 glance image-create --name cirros --is-public true --container-format bare --disk-format qcow2 --location https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk --size -1
+------------------+--------------------------------------+
| Property | Value |
+------------------+--------------------------------------+
| checksum | None |
| container_format | bare |
| created_at | 2013-12-13T13:48:07 |
| deleted | False |
| deleted_at | None |
| disk_format | qcow2 |
| id | 2da4e4f9-5f1a-4c8d-a67c-272588e2efbc |
| is_public | True |
| min_disk | 0 |
| min_ram | 0 |
| name | cirros |
| owner | 6a2db75adb964c5b84010fa22b464715 |
| protected | False |
| size | -1 |
| status | active |
| updated_at | 2013-12-13T13:48:38 |
+------------------+--------------------------------------+"
1,"This happens on master:

Follow these steps:

1) neutron net-create test --router:external=True
2) neutron subnet-create test 200.0.0.0/22 --name test
3) neutron floatingip-create test
4) neutron net-delete test

Watch command 4) hang (the server never comes back). Expected behavior would be for the command to succeed and delete the network successfully.

This looks like a regression caused by commit: b1677dcb80ce8b83aadb2180efad3527a96bd3bc (https://review.openstack.org/#/c/82945/)"
1,"The DHCP agent used to have a leg on the metadata network, this is a regression caused by:
https://review.openstack.org/#/c/69465/"
1,"The base driver file has a mechanism to attempt an iscsi target discovery in cases where the provider info is not present, this mechanism won't work in multi-backend mode because the sendtargets command is formed using volume['host'] to determine what ip to querie. In the case of multi-backend this host entry will look something like: ""cinder-host@backend-name"", the ""@backend-name"" is of course invalid for the iscsiadm call and will result in a failure."
1,"The reference implementation of the FWaaS iptables agent/driver supports only one firewall per tenant in Havana release. However, the FWaaS plugin will let you create more than one firewall. This should not be allowed since it creates an inconsistent state with only the most recent policy being applied."
0,'cls' may mislead developpers that this Decorator return a classmethod. Change parameter 'cls' to 'self' in wrapper.
1,"Network in container has wrong network mask.
In devstack you have a default network defined like following:
    nova network-list
    +--------------------------------------+---------+-------------+
    | ID | Label | Cidr |
    +--------------------------------------+---------+-------------+
    | f7c6e98d-d900-4df2-8523-0c8dd3a4ad7f | private | 10.0.0.0/24 |
    +--------------------------------------+---------+-------------+
If I start up a new container via nova and look at the created network device:
    sudo ip netns exec <container-id> ip a | grep pvnetr
    66: pvnetr70121: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
        inet 10.0.0.2/8 brd 10.255.255.255 scope global pvnetr70121
Then you can see that it has a 10.0.0.2/8 network.
I would expect to find a 10.0.0.2/24"
0,"""result = None"" in https://github.com/openstack/nova/blob/master/nova/api/ec2/__init__.py#L546 is unused code, remove it"
0,"When using the VC driver the following message is received:
2014-03-12 01:42:32.889 INFO nova.openstack.common.periodic_task [-] Skipping periodic task _periodic_update_dns because its interval is negative
2014-03-12 01:42:32.940 INFO nova.virt.driver [-] Loading compute driver 'vmwareapi.VMwareVCDriver'
2014-03-12 01:42:32.979 WARNING nova.virt.vmwareapi.driver [-] The VMware ESX driver is now deprecated and will be removed in the Juno release. The VC driver will remain and continue to be supported."
1,"I am working with gluster configured as cinder backend.
if I try to clone a volume from image when the new volume size is smaller than the image we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --image-id 0924d3a4-d163-4ae5-8f57-fb9f1912ee26 --display-name dafna_new
ERROR: Invalid input received: Image minDisk size 20 is larger than the volume size 10.
if I try to clone a volume from a volume when the new volume is smaller than the volume we clone from we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --source-volid c3b6cb41-d78d-420d-b64c-d5f3782f7772 --display-name new_clone
ERROR: Invalid input received: Clones currently disallowed when 10 < 20. They must be >= original volume size."
1,"Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make the obj_make_compatible calls consistent, but it actually changed them the wrong way.

Change https://review.openstack.org/#/c/121663/ addresses the bug but is sitting on top of a change that might be too risky at this point for juno-rc1, so this should be a separate fix."
0,"commit cb93eb60abf10af6fbb1ee438a6e8a51853e6788 changed the VMware store to use chunked encoding when the size is not known (or zero).
We still need to use Content-Length when we know the image size."
0,"Currently, there is no notifications when operating server groups, such as create/delete/update etc. This caused 3rd party cannot know the operation result on time.

We should add notification for server group operations."
1,"The issue was originally raised by a Red Hat performance engineer (Joe Talerico) here: https://bugzilla.redhat.com/show_bug.cgi?id=1136969 (see starting from comment 4).
Joe created a Fedora instance in his OS cloud based on RHEL7-OSP5 (Icehouse), where he installed Rally client to run benchmarks against that cloud itself. He assigned a floating IP to that instance to be able to access API endpoints from inside the Rally machine. Then he ran a scenario which basically started up 100+ new instances in parallel, tried to access each of them via ssh, and once it succeeded, clean up each created instance (with its ports). Once in a while, his Rally instance lost connection to outside world. This was because VXLAN tunnel to the compute node hosting the Rally machine was dropped on networker node where DHCP, L3, Metadata agents were running. Once we restarted OVS agent, the tunnel was recreated properly.
The scenario failed only if L2POP mechanism was enabled.
I've looked thru the OVS agent logs and found out that the tunnel was dropped due to a legitimate fdb entry removal request coming from neutron-server side. So the fault is probably on neutron-server side, in l2pop mechanism driver.
I've then looked thru the patches in Juno to see whether there is something related to the issue already merged, and found the patch that gets rid of _precommit step when cleaning up fdb entries. Once we've applied the patch on the neutron-server node, we stopped to experience those connectivity failures.
After discussion with Vivekanandan Narasimhan, we came up with the following race condition that may result in tunnels being dropped while legitimate resources are still using them:
(quoting Vivek below)
'''
- - port1 delete request comes in;
- - port1 delete request acquires lock
- - port2 create/update request comes in;
- - port2 create/update waits on due to unavailability of lock
- - precommit phase for port1 determines that the port is the last one, so we should drop the FLOODING_ENTRY;
- - port1 delete applied to db;
- - port1 transaction releases the lock
- - port2 create/update acquires the lock
- - precommit phase for port2 determines that the port is the first one, so request FLOODING_ENTRY + MAC-specific flow creation;
- - port2 create/update request applied to db;
- - port2 transaction releases the lock
Now at this point postcommit of either of them could happen, because code-pieces operate outside the
locked zone.
If it happens, this way, tunnel would retain:
- - postcommit phase for port1 requests FLOODING_ENTRY deletion due to port1 deletion
- - postcommit phase requests FLOODING_ENTRY + MAC-specific flow creation for port2;
If it happens the below way, tunnel would break:
- - postcommit phase for create por2 requests FLOODING_ENTRY + MAC-specific flow
- - postcommit phase for delete port1 requests FLOODING_ENTRY deletion
'''
We considered the patch to get rid of precommit for backport to Icehouse [1] that seems to eliminate the race, but we're concerned that we reverted that to previous behaviour in Juno as part of DVR work [2], though we haven't done any testing to check whether the issue is present in Juno (though brief analysis of the code shows that it should fail there too).
Ideally, the fix for Juno should be easily backportable because the issue is currently present in Icehouse, and we would like to have the same fix for both branches (Icehouse and Juno) instead of backporting patch [1] to Icehouse and implementing another patch for Juno.
[1]: https://review.openstack.org/#/c/95165/
[2]: https://review.openstack.org/#/c/102398/"
1,"The connect_volume and disconnect_volume code in brick assumes that the targets for different portals are the same for the same multipath device. This is true for some arrays but not for others. When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during copy image to volume and copy volume to image operations."
1,"when port nova-v3-test: test_server_actions.ServerActionsV3TestXML.test_lock_unlock_server. We found the exception.InstanceIsLocked is not caught in start and stop server API.
the following is the nova log:
2013-09-30 15:03:29.306 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mAction: 'action', body: <?xml version=""1.0"" encoding=""UTF-8""?>
<stop xmlns=""http://docs.openstack.org/compute/api/v1.1""/>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:935^[[00m
2013-09-30 15:03:29.307 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mCalling method <bound method ServersController._stop_server of <nova.api.openstack.compute.plugins.v3.servers.ServersController object at 0x577c250>>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:936^[[00m
2013-09-30 15:03:29.339 ^[[00;32mDEBUG nova.api.openstack.compute.plugins.v3.servers [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m[instance: cd4fec81-d2e8-43cd-ab5d-47da72dd90fa] ^[[00;32mstop instance^[[00m ^[[00;33mfrom (pid=23798) _stop_server /opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py:1372^[[00m
2013-09-30 15:03:29.340 ^[[01;31mERROR nova.api.openstack.extensions [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[01;31m] ^[[01;35m^[[01;31mUnexpected exception in API method^[[00m
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m return f(*args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 1374, in _stop_server
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m self.compute_api.stop(context, instance)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m File ""/opt/stack/nova/nova/compute/api.py"", line 198, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m return func(self, context, target, *args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m File ""/opt/stack/nova/nova/compute/api.py"", line 187, in inner
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m raise exception.InstanceIsLocked(instance_uuid=instance['uuid'])
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mInstanceIsLocked: Instance cd4fec81-d2e8-43cd-ab5d-47da72dd90fa is locked
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m
2013-09-30 15:03:29.341 ^[[00;36mINFO nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;36m] ^[[01;35m^[[00;36mHTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InstanceIsLocked'>^[[00m"
1,"When querying for the ""absolute limits"" of a specific tenant
the maxTotal* values reported aren't correct.
How to reproduce:
for example using devstack...
OS_TENANT_NAME=demo (11b2b129994844798c98f437d9809a9c)
OS_USERNAME=demo
$nova absolute-limits
+-------------------------+-------+
| Name | Value |
+-------------------------+-------+
| maxServerMeta | 128 |
| maxPersonality | 5 |
| maxImageMeta | 128 |
| maxPersonalitySize | 10240 |
| maxTotalRAMSize | 1000 |
| maxSecurityGroupRules | 20 |
| maxTotalKeypairs | 100 |
| totalRAMUsed | 128 |
| maxSecurityGroups | 10 |
| totalFloatingIpsUsed | 0 |
| totalInstancesUsed | 2 |
| totalSecurityGroupsUsed | 1 |
| maxTotalFloatingIps | 10 |
| maxTotalInstances | 10 | <-----------------------
| totalCoresUsed | 2 |
| maxTotalCores | 10 | <-----------------------
+-------------------------+-------+
OS_TENANT_NAME=admin (b0f08277004b43aab516ae7dbf36ff51)
OS_USERNAME=admin
$nova absolute-limits
+-------------------------+--------+
| Name | Value |
+-------------------------+--------+
| maxServerMeta | 128 |
| maxPersonality | 5 |
| maxImageMeta | 128 |
| maxPersonalitySize | 10240 |
| maxTotalRAMSize | 151200 |
| maxSecurityGroupRules | 20 |
| maxTotalKeypairs | 100 |
| totalRAMUsed | 1152 |
| maxSecurityGroups | 10 |
| totalFloatingIpsUsed | 0 |
| totalInstancesUsed | 18 |
| totalSecurityGroupsUsed | 1 |
| maxTotalFloatingIps | 10 |
| maxTotalInstances | 30 |
| totalCoresUsed | 18 |
| maxTotalCores | 30 |
+-------------------------+--------+
$nova absolute-limits --tenant 11b2b129994844798c98f437d9809a9c
+-------------------------+--------+
| Name | Value |
+-------------------------+--------+
| maxServerMeta | 128 |
| maxPersonality | 5 |
| maxImageMeta | 128 |
| maxPersonalitySize | 10240 |
| maxTotalRAMSize | 151200 |
| maxSecurityGroupRules | 20 |
| maxTotalKeypairs | 100 |
| totalRAMUsed | 128 |
| maxSecurityGroups | 10 |
| totalFloatingIpsUsed | 0 |
| totalInstancesUsed | 2 |
| totalSecurityGroupsUsed | 1 |
| maxTotalFloatingIps | 10 |
| maxTotalInstances | 30 | <-------------------
| totalCoresUsed | 2 |
| maxTotalCores | 30 | <-------------------
+-------------------------+--------+
note: arrows show the wrong values.
Seems that maxTotal* shows the values for the current tenant and not what is specified by ""--tenant""
as expected.
tested in havana and icehouse-1"
1,"Description of problem:
An attachment of a read only volume to an instance failed. The openstack was installed as AIO, Cinder was configured with Netapp back end. The following error from the nova compute log:
014-04-13 11:28:17.838 25176 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f7
0834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 360, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher payload)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return f(self, context, *args, **kw)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 244, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher pass
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 230, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 272, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher e, sys.exc_info())
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 259, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3876, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher bdm.destroy(context)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3873, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return self._attach_volume(context, instance, driver_bdm)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3894, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher self.volume_api.unreserve_volume(context, bdm.volume_id)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3886, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher do_check_attach=False, do_driver_attach=True)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher ret_val = method(obj, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 251, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher instance['uuid'], self['mount_device'])
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 174, in wrapper
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher res = method(self, ctx, volume_id, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 263, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher mountpoint)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 266, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher 'mode': mode})
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return self.api.client.post(url, body=body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 210, in post
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher return self._cs_request(url, 'POST', **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 174, in _cs_request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 157, in request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher raise exceptions.from_response(resp, body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher InvalidInput: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher
2014-04-13 11:28:17.858 25176 ERROR oslo.messaging._drivers.common [-] Returning exception Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31. to caller
Version-Release number of selected component (if applicable):
openstack-nova-conductor-2014.1-0.13.b3.el7.noarch
openstack-swift-object-1.12.0-1.el7.noarch
openstack-glance-2014.1-0.4.b3.el7.noarch
openstack-packstack-puppet-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-cert-2014.1-0.13.b3.el7.noarch
python-django-openstack-auth-1.1.4-1.el7.noarch
openstack-swift-1.12.0-1.el7.noarch
openstack-keystone-2014.1-0.4.b3.el7.noarch
openstack-utils-2013.2-3.p1.el7.noarch
openstack-nova-api-2014.1-0.13.b3.el7.noarch
openstack-nova-compute-2014.1-0.13.b3.el7.noarch
openstack-nova-novncproxy-2014.1-0.13.b3.el7.noarch
openstack-dashboard-2014.1-0.5.b3.el7.noarch
openstack-swift-account-1.12.0-1.el7.noarch
openstack-swift-proxy-1.12.0-1.el7.noarch
openstack-puppet-modules-2014.1-5.3.el7.noarch
openstack-cinder-2014.1-0.6.b3.el7.noarch
openstack-nova-common-2014.1-0.13.b3.el7.noarch
openstack-nova-console-2014.1-0.13.b3.el7.noarch
openstack-nova-network-2014.1-0.13.b3.el7.noarch
openstack-swift-container-1.12.0-1.el7.noarch
openstack-packstack-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-scheduler-2014.1-0.13.b3.el7.noarch
openstack-swift-plugin-swift3-1.7-3.el7.noarch
How reproducible:
100%
Steps to Reproduce:
1. Create a volume from a qcow2 image.
2. Attach the volume to an instance and make changes.
3. Detach the volume from the instance.
4. Set the volume as read only.
5. Attach the volume to an instance.
Actual results:
the attachment fails.
Expected results:
the attachment should succeed, but the volume cannot be changed."
1,"The ingress SG rule for RA has a wrong value for protocol
field."
0,"This patch points out the need for this in the v2 API and the need for a test in the v3 API to cover InstanceUserDataMalformed for the server create call.

https://review.openstack.org/#/c/54202/6/nova/api/openstack/compute/servers.py

It was out of scope to add/cover that in that patch, so reporting it as a bug to keep track of it."
0,"delete_volume implementation would like to log a warning ""no glance metadata found for volume %s"" in case of lack of metadata by catching a GlanceMetadataNotFound exception.
Nevertheless this exception is never throw by the SQL backend implementation.
So this log cannot be display correctly."
0,The auth_token middleware in keystoneclient is deprecated and will only get security updates. Projects should use the auth_token middleware in keystonemiddleware.
0,"When nova-compute gets forcably restarted, or fails, we get left over snapshots.

We have some clean up code for after nova-compute comes back up, but it would be good to clean up older snapshots, and generally try to minimize the size of the snapshot that goes to glance."
1,"If we use config-drive (whether set --config-drive=true in boot command or set force_config_drive=always in nova.conf), there is bug for config-drive when resize or migrate instances on hyperv.
You can see from current nova codes:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/migrationops.py#L269
when finished migration, there is no code to attach configdrive.iso or configdrive.vhd to the resized instance. compared to boot instance (https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L226). Although this commit https://review.openstack.org/#/c/55975/ handled coping configdrive to resized or migrated instance, there is no code to attach it after resized or migrated."
1,"when using rbd as disk backend. images_type=rbd in nova.conf
disk IO tunning doesn't work as described https://wiki.openstack.org/wiki/InstanceResourceQuota"
1,"In the Nova servers API (v2 and v3), this line could fail with an AtributeError if there is a FlavorNotFound exception above it:
https://github.com/openstack/nova/blob/2014.1.b3/nova/api/openstack/compute/servers.py#L611
That code should either check if instance_list is empty first or set instance_list to an empty InstanceList object if FlavorNotFound is hit."
1,"While investigating spurious failures in our TripleO continous deployment, I had this problem:
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| 3a9c6aca-e91f-49c9-850a-67db219fdf58 | L3 agent | overcloud-notcompute-wjo2jbvvd2sm | :-) | True |
| 3fb9f6cf-b545-4a34-a490-dda834973d1e | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx | True |
| 855349b2-b0fc-4270-bb96-385b61aa5a6c | DHCP agent | overcloud-notcompute-wjo2jbvvd2sm | :-) | True |
| 8b8a4128-9716-42ee-b886-f053db166ce3 | Metadata agent | overcloud-notcompute-wjo2jbvvd2sm | :-) | True |
| c8297e0d-8575-47f0-ae65-499c1e0319b3 | Open vSwitch agent | overcloud-notcompute-wjo2jbvvd2sm | :-) | True |
| f746fc1d-9083-46f4-a922-739c5d332d7c | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx | True |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
Note that overcloud-novacompute0-ubrjpv4jz64a has _two_ Open vSwitch agents.
This caused many 'vif_type=binding_failed' errors when booting nova instances.
Deleting f746fc1d-9083-46f4-a922-739c5d332d7c resulted in the problem going away.
Seems like there might be a race if the agent restarts quickly, thus not seeing its own agent record and sending a second RPC to create one. I think, I am not entirely sure how this works, that is just a hypothesis."
1,"When an aggregate with 'host' attribute not empty is deleted, InvalidAggregateAction exception will be raised, but this exception
is not handled.
$ nova --os-compute-api-version 3 aggregate-delete agg5
ERROR (ClientException): Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InvalidAggregateAction'> (HTTP 500) (Request-ID: req-9a8500ae-379a-4121-b217-7e7ea6188ad0)
2014-04-07 23:56:16.347 ERROR nova.api.openstack.extensions [req-f7c09203-a681-496c-a84e-18fb3d2e3659 admin demo] Unexpected exception in API method
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/aggregates.py"", line 155, in delete
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions self.api.delete_aggregate(context, id)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions payload)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions six.reraise(self.type_, self.value, self.tb)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions return f(self, context, *args, **kw)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 3363, in delete_aggregate
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions reason='not empty')
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions InvalidAggregateAction: Cannot perform action 'delete' on aggregate 3. Reason: not empty."
0,"The blueprint https://blueprints.launchpad.net/cinder/+spec/vmdk-storage-policy-volume-type introduced ""pbm_default_policy"" to allow an admin to specify what storage policy to use when creating a volume with either:
 - no volume_type or
 - with a volume_type with no vmware:storage_policy extra spec.

This could instead use the ""default_volume_type"" config that Cinder already supports. This way the choice of using a volume_type to capture the required policy is not broken."
1,"On a Devstack-deployed, single-node install, the ovs l2 agent is using an order of magnitude more cpu than any other service. On a nested-virt VM running on a 2.5GHz host, with no VM's provisioned:
 - WIth L2 agent running, 10-100% cpu usage recorded, averaging ~40%
 - With L2 agent stopped, 0-10% cpu usage recorded
Casual inspection with something like top or htop will show the excessive cpu usage, but won't give a good indication of the culprit. Enabling the CUTIME and CSTIME stats is necessary to highlight the problem, as all the time taken by the agent is in subprocess-invoked polling whose execution time is not directly included in the parent's cpu usage."
1,"When SIGHUP signal is send to cinder-api service, it stops all the cinder-api processes and while restarting the cinder-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
Steps to reproduce:
1. Run cinder-api service as daemon.
2. Send SIGHUP signal to cinder-api service
   kill -1 <parent_process_id_of_cinder_api>"
0,"The reboot operation is part of the core nova API but the powervm driver doesn't support it:

https://github.com/openstack/nova/blob/master/nova/virt/powervm/driver.py#L126

https://wiki.openstack.org/wiki/HypervisorSupportMatrix

This is a pretty basic operation that should be supported if the driver is to be considered valid.

Looking here:

http://pic.dhe.ibm.com/infocenter/powersys/v3r1m5/topic/iphcg/chsysstate.htm

It should be a pretty straight-forward operation:

""To perform an immediate restart of a partition (operator panel function 3):

chsysstate -r lpar -o shutdown --immed --restart { -n Name | --id PartitionID } [ -m ManagedSystem ]"""
0,"test_ovs_lib, test_ovs_neutron_agent and test_ofa_neutron_agent have duplicated same unit tests for check_ovs_vxlan_version. The only difference is SystemError (from ovs_lib) and SystemExit (from agents).
The tested logic is 99% same, and unit tests in ovs/ofa agent looks unnecessary."
0,"Change I1e8a87d7dc1a1cb9309aeefd41619e20f49f95a6 has introduced another of those SELECT FOR UPDATE NOT SUPPORTED errors in postgres. Even though this error occurs, that does not seem to upset the gate. This is one of the logs where it occurs:
http://logs.openstack.org/52/54752/1/check/check-tempest-devstack-vm-neutron-pg-isolated/8c288a4/logs/screen-q-svc.txt.gz
And one of the runs where it passes:
https://review.openstack.org/#/c/54752/1"
1,"The driver is broken, reporting incorrectly the backends free capacities, as an infinite resource.
This causes two main problems:
- the scheduler always chooses the same netapp backend, and isnt making any balance between the different backends.
- If the first storage backend has no real free capacity, the driver chooses it anyway, therefore after 3 schedule attemps, the volume creation stops with an error.
Is it possible to fix the driver, in order to report the real backends free capacity, so the scheduler balances the volume creation?
We can attach some log if is necessary.
Thanks in advance.
Agustin."
1,"Hello Stackers!
I'm trying SPICE Consoles and it is working smoothly! But, I detected a problem, it is ignoring the entry ""spicehtml5proxy_host"" in nova.conf for Dual-Stacked environment (IPv4 / IPv6).
So, it isn't listening on ""::"".
The following setup doesn't work....
---
[spice]
enabled = True
spicehtml5proxy_host = ::
html5proxy_base_url = http://controller.yourdomain.com:6082/spice_auto.html
keymap = en-us
---
Unless I patch the following file:
/usr/lib/python2.7/dist-packages/nova/cmd/spicehtml5proxy.py into this:
---
opts = [
    cfg.StrOpt('spicehtml5proxy_host',
               default='::',
               help='Host on which to listen for incoming requests'),
---
As you guys can see, I replaced the default ""0.0.0.0"" to ""::"" and now SPICE Proxy listens on both IPv4 and IPv6! But, this is a hack...
I don't know why it is ignoring the ""spicehtml5proxy_host = ::"" entry at nova.conf.
BTW, the ""novncproxy_host = ::"" works as expected for NoVNC but, I'm disabling VNC from my cloud in favor of SPICE.
Cheers!
Thiago"
1,"CREATE TABLE subnets (tenant_id VARCHAR(255),id VARCHAR(36) NOT NULL, name VARCHAR(255),network_id VARCHAR(36), ip_version INT NOT NULL, cidr VARCHAR(64) NOT NULL,gateway_ip VARCHAR(64),enable_dhcp SMALLINT,shared SMALLINT,ipv6_ra_mode VARCHAR(16), ipv6_address_mode VARCHAR(16),PRIMARY KEY (id), FOREIGN KEY(network_id) REFERENCES networks (id),CHECK (enable_dhcp IN (0, 1)), CHECK (shared IN (0, 1)), CONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')),CONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')))
for db2, this fails because the name ipv6_modes is used twice for a contraint name. In db2, A constraint-name must not identify a constraint that was already specified within the same CREATE TABLE statement. (SQLSTATE 42710).
=============
Checked neutron server.log and found
2014-03-18 18:37:45.799 19954 TRACE neutron Traceback (most recent call last):
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/bin/neutron-server"", line 10, in <module>
2014-03-18 18:37:45.799 19954 TRACE neutron sys.exit(main())
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/server/__init__.py"", line 54, in main
2014-03-18 18:37:45.799 19954 TRACE neutron neutron_api = service.serve_wsgi(service.NeutronApiService)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 113, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron LOG.exception(_('Unrecoverable error: please check log '
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/excutils.py"", line 68, in __exit_
_
2014-03-18 18:37:45.799 19954 TRACE neutron six.reraise(self.type_, self.value, self.tb)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 106, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron service.start()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 75, in start
2014-03-18 18:37:45.799 19954 TRACE neutron self.wsgi_app = _run_wsgi(self.app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 175, in _run_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron app = config.load_paste_app(app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 170, in load_paste_app
2014-03-18 18:37:45.799 19954 TRACE neutron app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-03-18 18:37:45.799 19954 TRACE neutron return loadobj(APP, uri, name=name, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-03-18 18:37:45.799 19954 TRACE neutron return context.create()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-03-18 18:37:45.799 19954 TRACE neutron app = loader.get_app(app_name, global_conf=global_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-03-18 18:37:45.799 19954 TRACE neutron app = loader.get_app(pipeline[-1])
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron return fix_call(context.object, context.global_conf, **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-03-18 18:37:45.799 19954 TRACE neutron return cls(**local_config)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron plugin = manager.NeutronManager.get_plugin()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-03-18 18:37:45.799 19954 TRACE neutron return cls.get_instance().plugin
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-03-18 18:37:45.799 19954 TRACE neutron cls._create_instance()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-03-18 18:37:45.799 19954 TRACE neutron return f(*args, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-03-18 18:37:45.799 19954 TRACE neutron cls._instance = cls()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron plugin_provider)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-03-18 18:37:45.799 19954 TRACE neutron return plugin_class()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 105, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron super(Ml2Plugin, self).__init__()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/db/db_base_plugin_v2.py"", line 225, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron db.configure_db()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 34, in configure_db
2014-03-18 18:37:45.799 19954 TRACE neutron register_models()
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 53, in register_models
2014-03-18 18:37:45.799 19954 TRACE neutron base.metadata.create_all(engine)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/schema.py"", line 2571, in create_all
2014-03-18 18:37:45.799 19954 TRACE neutron tables=tables)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 2302, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron conn._run_visitor(visitorcallable, element, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1972, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron **kwargs).traverse_single(element)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 67, in visit_metadata
2014-03-18 18:37:45.799 19954 TRACE neutron self.traverse_single(table, create_ok=True)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 86, in visit_table
2014-03-18 18:37:45.799 19954 TRACE neutron self.connection.execute(schema.CreateTable(table))
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron params)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
2014-03-18 18:37:45.799 19954 TRACE neutron compiled
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron context)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron context)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib/python2.6/site-packages/ibm_db_sa/ibm_db.py"", line 104, in do_execute
2014-03-18 18:37:45.799 19954 TRACE neutron cursor.execute(statement, parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1335, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron self._execute_helper(parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1247, in _execute_helper
2014-03-18 18:37:45.799 19954 TRACE neutron raise self.messages[len(self.messages) - 1]
2014-03-18 18:37:45.799 19954 TRACE neutron ProgrammingError: (ProgrammingError) ibm_db_dbi::ProgrammingError: Statement Execute Failed: [IBM][
CLI Driver][DB2/LINUXX8664] SQL0601N The name of the object to be created is identical to the existing name ""IPV6_MODES"" of type ""CHECK CONST""
. SQLSTATE=42710 SQLCODE=-601 ""\nCREATE TABLE subnets (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tnet
work_id VARCHAR(36), \n\tip_version INT NOT NULL, \n\tcidr VARCHAR(64) NOT NULL, \n\tgateway_ip VARCHAR(64), \n\tenable_dhcp SMALLINT, \n\tshar
ed SMALLINT, \n\tipv6_ra_mode VARCHAR(16), \n\tipv6_address_mode VARCHAR(16), \n\tPRIMARY KEY (id), \n\tFOREIGN KEY(network_id) REFERENCES netw
orks (id), \n\tCHECK (enable_dhcp IN (0, 1)), \n\tCHECK (shared IN (0, 1)), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-
stateful', 'dhcpv6-stateless')), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless'))\n)\n\n
"" ()
2014-03-18 18:37:45.799 19954 TRACE neutron"
1,"The errors_out_migration decorator in nova/compute/manager.py is attempting to use positional arguments to get the migration parameter. However, at run time, the decorated methods are called via RPC calls which specify all of their arguments as keyword arguments. The decorator needs to be fixed so that it works with RPC calls."
1,"Using a property protections file such as:
[.*]
create = @,!
read = @
update = @
delete = @
The create operation has an invalid rule, duplicate values are not allowed. This should probably result in the service refusing to start, however currently the service will start and operations touching this value will return:
500 Internal Server Error
Malformed property protection rule 'some_property': '@' and '!' are mutually exclusive (HTTP 500)
to the end user. My feeling is that the end user should not receive any information about the cause of the error, just the 500 status."
0,"The E-Series driver does not determine the preferred data path to access a newly created lun.
This can cause problems with data access and the iscsi path to the device that is handed to a vm or that is used when copying an image to the device should always use the preferred path. E-Series arrays do something called an implicit transfer when a host accesses data from a non-preferred path for more than 5 minutes. At 5 minutes the array will transfer ownership of the volume to the path over which IO is currently flowing. If this issue is not resolved the current behavior of the driver is to establish every IO path on the first iSCSI target that is returned. In this scenario the controller A will quickly become heavily loaded and the controller B will have no load because of the implicit transfers from B --> A To get better throughput this issued needs to be fixed."
1,"When XenServer disable/enable host, the api set_host_enabled() need to get service info for the target host, but the rpc call of set_host_enabled() did not transfer host as parameter, this will cause the api call failed.
def set_host_enabled(self, host, enabled):
        """"""Sets the specified host's ability to accept new instances.""""""
        # Since capabilities are gone, use service table to disable a node
        # in scheduler
        status = {'disabled': not enabled,
                'disabled_reason': 'set by xenapi host_state'
                }
        cntxt = context.get_admin_context()
        service = self._conductor_api.service_get_by_args(
                cntxt,
                host, <<<<<<<<<
                'nova-compute')
        self._conductor_api.service_update(
                cntxt,
                service,
                status)
        args = {""enabled"": jsonutils.dumps(enabled)}
        response = call_xenhost(self._session, ""set_host_enabled"", args)
        return response.get(""status"", response)
======================================
    def set_host_enabled(self, ctxt, enabled, host):
        cctxt = self.client.prepare(server=host)
        return cctxt.call(ctxt, 'set_host_enabled', enabled=enabled) <<<<<<<<<< No host"
0,"The pci extension for the v3 API does another instance lookup back to the database for instance objects. The issue being that when you are doing something like a list_* operation on instances, this means that we're making a second trip to the database that's distinct from the first lookup in the request handling. If an instance got deleted between the request and the extension hook running, this will generate a database exception, which turns into an InstanceNot found, and 404s the list operation *if any instance was deleted during the request*
We are managing to hit this quite frequently in tempest with our test_list_servers_by_admin_with_all_tenants (even at only concurency 2) - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/console.html#_2014-01-20_01_18_11_102
The explosion looks like this - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/logs/screen-n-api.txt.gz?level=INFO#_2014-01-20_00_57_44_352
Logstash picks up these tracebacks really easily. This kind of explosion doesn't always trigger a Tempest failure, because some times this might be in cleanup code, where we protect against 404s (though it probably means we are leaking resources a lot on a normal run).
Logstash query - http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVFJBQ0Ugbm92YS5hcGkub3BlbnN0YWNrXCIgQU5EIG1lc3NhZ2U6XCJJbnN0YW5jZU5vdEZvdW5kOiBJbnN0YW5jZVwiIEFORCBmaWxlbmFtZTpcImxvZ3Mvc2NyZWVuLW4tYXBpLnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzkwMTgzNzk1ODI1fQ=="
0,"Some codes about 'extract_members' from 'create_server_group' is written in original bp/patch's reviewing (https://review.openstack.org/#/c/62557/29).

But as the design variation, it can't be specified members in create API (see it in PS29 vs PS30).
But some codes are still kept after this feature merged.
So it's necessary to remove them."
0,"Before Grizzly release, data format of OFC ID mapping tables was changed and there are two types of ID mapping tables for old and new format. The old mapping tables are only used for resources (networks, ports, tenants, filters) in pre-Grizzly system. pre-Grizzly system is no longer supported and we no longer need to consider old data format. Migrating data from the old mapping tables to the new tables reduces the code complexity."
0,"Can not update volume by display_name, display_description in cinder Block api v2.
reason:
when updating volume , v2 API allows name instead of display_name,description Instead of display_description. but hasn't get
display_name,display_description,so cann't update volume by the two prameters ,should get the two prameters value first.

filepath:
cinder/api/v2/volumes.py
function :
@wsgi.serializers(xml=VolumeTemplate)
    def update(self, req, id, body):
add code:
        # NOTE(zhuozhe) fix bug : v2 API allows name instead of display_name
        # description Instead of display_description, so should get old parameter first
        valid_update_keys_v1 = (
            'display_name',
            'display_description',
        )

        for key in valid_update_keys_v1:
            if key in volume:
                update_dict[key] = volume[key]"
1,"The pid file path changes from %s.pid to %s/pid during juno, due to this change:

https://github.com/openstack/neutron/commit/7f8ae630b87392193974dd9cb198c1165cdec93b#diff-448060f24c6b560b2cbac833da6a143dL68

That means the l3-agent and dhcp-agent (when isolated metadata is enabled)
will respawn a second neutron-ns-metadata proxy on each namespace/resource
after upgrade (I->J) and agent restart due the inability to find the old PID
file and external process PID."
0,"Import existing volume failed with below ERROR:
2014-06-24 13:04:54.407 110362 ERROR oslo.messaging.rpc.dispatcher [req-7aff78e5-92c2-4864-8388-7884068e3360 86341fc3271b4e2da241dd3b603c52f5 0807a39b955048ed88c123c9b3a0485b - - -] Exception during message handling: taskflow.patterns.linear_flow.Flow: volume_manage_existing_manager; 7 requires ['optional_args'] but no other entity produces said requirements

Commit 971a63bd9cf675a00bce1244ec101577b5c17cac has added a new parameter 'optional_args' to execute method of QuotaReserveTask and QuotaCommitTask. So where those Task used need change to provide the new parameter.

This commit has changed volume create task flow as below to provide the new parameter. We need the samiliar change to get_flow method in cinder\volume\flows\manager\manage_existing.py

diff --git a/cinder/volume/api.py b/cinder/volume/api.py
index 2fd65bf..8e4201c 100644
--- a/cinder/volume/api.py
+++ b/cinder/volume/api.py
@@ -174,6 +174,7 @@ class API(base.Base):
             'scheduler_hints': scheduler_hints,
             'key_manager': self.key_manager,
             'backup_source_volume': backup_source_volume,
+ 'optional_args': {'is_quota_committed': False}
         }

Commit comment of b5c17cac for reference:
     Made provision for providing optional arguments

    The 'quota_committed' attribute of 'RequestContext' object is
    a transient property, so it will not be saved in the taskflow
    persistent storage. The updated value of 'quota_committed'
    attribute will not be available while resuming/reverting the
    flow, if cinder api-service is down/stopped after committing
    the quota.

    Since this 'quota_committed' attribute is not used anywhere
    in cinder project other than in create-volume taskflow api, so
    removed 'quota_committed' from RequestContext and made
    provision to pass it as an optional argument which will be
    passed to api-flow via create_what dictionary, in order to
    make it persistent and use it as and when needed."
1,"On any lvm2 version without lvmetad running, I can get cinder-volume to hang on issuing lvm related commands after deleting snapshots (that are non-thin provisioned LVM snapshots with clear_volume set to zero).
the issue is that lvm locks up due to trying to access suspended device mapper entries, and at some point cinder-volume does a lvm related command and hangs on that. a setting of ignore_suspended_devices = 1 in lvm.conf helps with that, as lvremove hangs on scanning the device state (which it
needs to do because it doesn't have current information available via lvmetad).
I can use this script to trigger the issue:
=== cut hang.sh ===
enable_fix=1
#enable_fix=0
vg=cinder-volumes
v=testvol.$$
lvcreate --name $v $vg -L 1g
sleep 2
lvcreate --name snap-$v --snapshot $vg/$v -L 1g
vgp=/dev/mapper/${vg/-/--}-snap--${v/-/--}
sleep 2
( sleep 10 < $vgp-cow ) &
test ""$enable_fix"" -eq ""1"" && lvchange -y -an $vg/snap-$v
lvremove -f $vg/snap-$v
sleep 1
lvremove -f $vg/$v
=== cut hang.sh ===
vg needs to be set to a lvm VG that exists and can take a few gig of space. whenever enable_fix is set to 0, lvremove -f ends with :
  Unable to deactivate open cinder--volumes-snap--testvol.27700-cow (252:5)
  Failed to resume snap-testvol.27700.
  libdevmapper exiting with 1 device(s) still suspended.
this is because the sleep command before keeps a fd open on the -cow. The script then also never finishes and any other lvm command hangs as well.
apparently in real-life this is either udev or the dd command still having the fd open for some reason I have not yet understood.
The deactivation before removing seems to help."
1,"2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.api_client.client [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Server Error Message: Entity '50d064c4-0591-43ee-8dfd-ac2409c13a5e' not registered.
2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.nsxlib.switch [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Port or Network not found, Error: An unknown exception occurred.
2014-03-03 15:06:33.914 ERROR NeutronPlugin [-] Unable to update port id: 6b7c9502-8e58-4930-b440-7022f47fff89.
2014-03-03 15:06:33.914 TRACE NeutronPlugin Traceback (most recent call last):
2014-03-03 15:06:33.914 TRACE NeutronPlugin File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1307, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin ret_port.get(addr_pair.ADDRESS_PAIRS))
2014-03-03 15:06:33.914 TRACE NeutronPlugin File ""/opt/stack/neutron/neutron/plugins/vmware/nsxlib/switch.py"", line 324, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin port_id=lport_uuid, net_id=lswitch_uuid)
2014-03-03 15:06:33.914 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 6b7c9502-8e58-4930-b440-7022f47fff89 could not be found on network b460b092-9482-48ee-9c5f-04ec384fbc8c"
0,"As discussed in lbaas design session at the summit, it's better to move noop driver to unit tests folder to clearly indicate that this driver is not for production and should not be listed in providers.
This should avoid possible confusion for Neutron deployers."
0,"Most of the code in nova.db.sqlalchemy.utils is also in oslo-incubator.openstack.common.db.sqlalchemy.utils, except for the modify_indexes method which is not actually even used in the nova db migration code anymore now that it's been compacted in icehouse.
Also, the oslo.db code has been getting synced over to nova more frequently lately so rather than keep all of this duplicate code around we should move nova to using the oslo utils code and drop the internal nova one, with maybe moving the modify_indexes method to oslo first, then sync back to nova and then drop nova.db.sqlalchemy.utils from nova.
We will have to make sure that there are no behavior differences in the oslo code such that it would change the nova db schema, but we should be able to use Dan Prince's nova/tools/db/schema_diff.py script to validate that."
0,"sesssion -> session cinder\openstack\common\db\sqlalchemy\models.py
explicity -> explicitly cinder\openstack\common\db\sqlalchemy\models.py
tranfers -> transfers
recurse -> recursive
parens -> parents cinder\openstack\common\periodic_task.py
satisified -> satisfied cinder\taskflow\exceptions.py"
1,"Found in commit: 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
There is a typo in method name ""_notify_voloume_type_error"". It should be ""_notify_volume_type_error""."
0,"When using the per-domain-identity backend usernames could end up colliding when multiple LDAP backends are used since we extract very limited information from the DN.

Example

cn=example user, dc=example1,dc=com
cn=example user, dc=example2,dc=com

Would net the same ""user_id"" of ""example user""

This can also affect groups in the same manner."
0,Use database session from the context wherever possible during database transactions to avoid inconsistencies in the Cisco N1kv plugin.
1,"The ""enable_isolated_metadata = True"" options tells DHCP agents that for each network under its care, a neutron-ns-metadata-proxy process should be spawned, regardless if it's isolated or not.
This is fine for isolated networks (networks with no routers and no default gateways), but for networks which are connected to a router (for which the L3 agent spawns a separate neutron-ns-metadata-proxy which is attached to the router's namespace), 2 different metadata proxies are spawned. For these networks, the static routes which are pushed to each instance, letting it know where to search for the metadata-proxy, is not pushed and the proxy spawned from the DHCP agent is left unused.
The DHCP agent should know if the network it handles is isolated or not, and for non-isolated networks, no neutron-ns-metadata-proxy processes should spawn."
0,"get_host_capabilities() in libvirt driver seems to have a bug that will result in duplicated features.

def get_host_capabilities(self):
        """"""Returns an instance of config.LibvirtConfigCaps representing
           the capabilities of the host.
        """"""
        if not self._caps:
            xmlstr = self._conn.getCapabilities()
            self._caps = vconfig.LibvirtConfigCaps()
            self._caps.parse_str(xmlstr)
            if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
                try:
                    features = self._conn.baselineCPU(
                        [self._caps.host.cpu.to_xml()],
                        libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
                    # FIXME(wangpan): the return value of baselineCPU should be
                    # None or xml string, but libvirt has a bug
                    # of it from 1.1.2 which is fixed in 1.2.0,
                    # this -1 checking should be removed later.
                    if features and features != -1:
                        self._caps.host.cpu.parse_str(features)
                except libvirt.libvirtError as ex:
                    error_code = ex.get_error_code()
                    if error_code == libvirt.VIR_ERR_NO_SUPPORT:
                        LOG.warn(_LW(""URI %(uri)s does not support full set""
                                     "" of host capabilities: "" ""%(error)s""),
                                     {'uri': self.uri(), 'error': ex})
                    else:
                        raise
        return self._caps

The _caps.parse_str() is called in sequence for both capabilites and expand features. Since capabilities will have certain features in a VM, and these will be repeated again in the expand features, the _caps.host.cpu.features will end up with duplicated features. This will cause cpu compare to fail later.

(nova)root@overcloud-novacompute0-un6ckrnp5tzl:~# python
Python 2.7.6 (default, Mar 22 2014, 22:59:38)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import libvirt
>>> conn = libvirt.open(""qemu:///system"")
>>> from nova.virt.libvirt import config as vconfig
>>> caps = vconfig.LibvirtConfigCaps()
>>> xmlstr = conn.getCapabilities()
>>> caps.parse_str(xmlstr)
>>> features = conn.baselineCPU([caps.host.cpu.to_xml()], libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
>>> caps.host.cpu.parse_str(features)
>>> for f in caps.host.cpu.features:
... print f.name
...
hypervisor
popcnt
hypervisor
popcnt
pni
sse2
sse
fxsr
mmx
pat
cmov
pge
sep
apic
cx8
mce
pae
msr
tsc
pse
de
fpu
>>>"
0,"neutron.common.log.log is useful for logging arguments of a method.
It outputs class name and method name, but module path is not output.
A module path is useful to search the log message and it is better to contain a module path
[Current]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {} wrapper /opt/stack/neutron/neutron/common/log.py:33
[Proposed]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] neutron.tests.unit.nec.stub_ofc_driver.StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {} wrapper /opt/stack/neutron/neutron/common/log.py:33"
0,"Duplicate name of LBaaS obj(pool/vip/app_profile) is not allowed on vShield Edge, so here need a naming convention to ensure name uniqueness on the edge side."
1,"L3_NAT_db_mixin add_router_interface() and remove_router_interface() methods check that interface_info is not empty but don't check that it contains any of expected parameters - port_id or subnet_id:
 if not interface_info:
   msg = _(""Either subnet_id or port_id must be specified"")
   raise n_exc.BadRequest(resource='router', msg=msg)

Expected parameters should be explicitly checked."
0,"In a few versions of oslo.db (maybe when we release 1.0.0?), every project using oslo.db should inspect their code and remove usages of 'raw' DB exceptions like IntegrityError/OperationalError/etc from except clauses and replace them with the corresponding custom exceptions from oslo.db (at least a base one - DBError).

Full version

A recent commit to oslo.db changed the way the 'raw' DB exceptions are wrapped (e.g. IntegrityError, OperationalError, etc). Previously, we used decorators on Session methods and wrapped those exceptions with oslo.db custom ones. This is mostly useful for handling them later (e.g. to retry DB API methods on deadlocks).

The problem with Session decorators was that it wasn't possible to catch and wrap all possible exceptions. E.g. SA Core exceptions and exceptions raised in Query.all() calls were ignored. Now we are using a low level SQLAlchemy event to catch all possible DB exceptions. This means that if consuming projects had workarounds for those cases and expected 'raw' exceptions instead of oslo.db ones, they would be broken. That's why we *temporarily* added both 'raw' exceptions and new ones to expect clauses in consuming projects code when they were ported to using of oslo.db to make the transition smooth and allow them to work with different oslo.db versions.

On the positive side, we now have a solution for problems like https://bugs.launchpad.net/nova/+bug/1283987 when exceptions in Query methods calls weren't handled properly.

In a few releases of oslo.db we can safely remove 'raw' DB exceptions like IntegrityError/OperationalError/etc from projects code and except only oslo.db specific ones like DBDuplicateError/DBReferenceError/DBDeadLockError/etc (at least, we wrap all the DB exceptions with our base exception DBError, if we haven't found a better match)."
0,"urllib2.urlopen uses $http_proxy/$HTTP_PROXY environment variables by
default. If set (and pointing to a remote host), then the WSGI tests
that spin up a local server and connect to it using
http://127.0.0.1:$port/ instead connect to $port *on the proxy*,
and (hopefully) fail.
We shouldn't follow HTTP_PROXY when trying to connect to test servers."
0,"Improve unit test coverage for ...
quantum/plugins/cisco/common/cisco_constants 77 0 0 0 0 100%
quantum/plugins/cisco/common/cisco_credentials_v2 30 6 0 4 0 82%
quantum/plugins/cisco/common/cisco_exceptions 37 0 0 2 0 100%
quantum/plugins/cisco/common/cisco_faults 34 34 0 0 0 0%
quantum/plugins/cisco/common/cisco_utils 15 15 0 0 0 0%
quantum/plugins/cisco/common/config 23 2 0 8 1 84%"
0,"The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.

{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
0,"I can reproduce this bug in master and stable havana.
this bug is similar to https://bugs.launchpad.net/nova/+bug/1158897
but this bug is cause by _reserve_quota_delta() method, which will need to access DB if quota delta is not empty,
the trace log is:
2013-12-19 01:39:40.879 ERROR nova.compute [-] No db access allowed in nova-compute:
File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 125, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/opt/stack/nova/nova/service.py"", line 314, in periodic_tasks
    return self.manager.periodic_tasks(ctxt, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/manager.py"", line 101, in periodic_tasks
    return self.run_periodic_tasks(context, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
    task(self, context)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 4535, in _poll_unconfirmed_resizes
    migration=migration)
  File ""/opt/stack/nova/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 216, in _wrapped
    return fn(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2158, in confirm_resize
    reservations = self._reserve_quota_delta(context, deltas)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2238, in _reserve_quota_delta
    return QUOTAS.reserve(context, project_id=project_id, **deltas)
  File ""/opt/stack/nova/nova/quota.py"", line 1272, in reserve
    user_id=user_id)
  File ""/opt/stack/nova/nova/quota.py"", line 487, in reserve
    has_sync=True, project_id=project_id)
  File ""/opt/stack/nova/nova/quota.py"", line 354, in _get_quotas
    usages=False)
  File ""/opt/stack/nova/nova/quota.py"", line 264, in get_project_quotas
    project_quotas = db.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1023, in quota_get_all_by_project
    return IMPL.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 48, in __call__
    stacktrace = """".join(traceback.format_stack())
2013-12-19 01:39:40.879 ERROR nova.compute.manager [-] [instance: 4ffaabaf-a480-421f-a7a7-efd148d7c956] Error auto-confirming resize: nova-compute. Will retry later."
0,"The method create() in /cinder/api/v2/volumes.py has two redundant variable declarations:
 image_href = None
 image_uuid = None
these should be reduced."
0,Shouldn't use magic numbers for http error codes in unit tests: use codes from webop.exc like webob.exc.HTTPConflict.code
0,"Check if network is in use is not needed in bigswitch plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L587-L599
as it is done by db_base_plugin, which is called right after:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L601"
1,"ssatya@devstack:~$ nova boot --image 1e95fe6b-cec6-4420-97d1-1e7bc8c81c49 --flavor 1 testdummay
+--------------------------------------+-----------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-STS:power_state | 0 |
| OS-EXT-STS:task_state | networking |
| OS-EXT-STS:vm_state | building |
| OS-SRV-USG:launched_at | - |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| adminPass | fK8SPGtHLUds |
| config_drive | |
| created | 2014-03-07T14:33:49Z |
| flavor | m1.tiny (1) |
| hostId | 2c1ae30aa2a235d9c0c8b04aae3f4199cd98356e44a03b5c8f878adb |
| id | eae503d9-c6f7-4e3e-9adc-0b8b6803c90e |
| image | debian-2.6.32-i686 (1e95fe6b-cec6-4420-97d1-1e7bc8c81c49) |
| key_name | - |
| metadata | {} |
| name | testdummay |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | BUILD |
| tenant_id | 209ab7e4f3744675924212805db3ad74 |
| updated | 2014-03-07T14:33:50Z |
| user_id | f3756a4910054883b84ee15acc15fbd1 |
+--------------------------------------+-----------------------------------------------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | BUILD | spawning | NOSTATE | |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1 | ERROR | deleting | Shutdown | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ACTIVE | - | Running | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1 | ERROR | deleting | Shutdown | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova stop testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+---------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------------+---------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | SHUTOFF | - | Shutdown | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1 | ERROR | deleting | Shutdown | private=10.0.0.2 |
+--------------------------------------+------------+---------+------------+-------------+------------------+
ssatya@devstack:~$ nova delete testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ERROR | deleting | Shutdown | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1 | ERROR | deleting | Shutdown | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+"
1,"in cinder/exception.py: https://github.com/openstack/cinder/blob/master/cinder/exception.py#L242
message = _(""Invalid metadata"") + "": %(reason)s""
is not consistent i18n usage. In most of cases, we use _() for whole string instead of leaving something out.
More than inconsistent, some text in leaving strings may be not translated, for example, ':' in English is different '? in Chinese."
0,"In order to eliminate the need to have the hp3parclient in the global-requirements project, we need to remove the hp3parclient from being imported in all 3par driver unit tests in cinder.
It should _at least_ be optional for the tests."
0,"The python neutron client for the V2 API expects the neutron API to send information back such as the type and detail of the exception the body of the message (serialized as a dict). See exception_handler_v20 in client.py. However, the neutron v2 api only returns the exception message. This means that the client can only propagate up a generic NeutronClientException exceptions rather than more specific ones related to the actual problem.

All of the necessary information is present in the v2 api resource class at the point the webob exception is raised to include at least the type of the exception (detail appears not to be used in neutron exceptions) so the format of the message returned should be changed to include a type field and a message field rather than just the message."
1,The removal of LBaaS VIP Port (and other DVR Serviced ports except compute port) does not delete the DVR namespace from the service nodes.
0,"Colocate strings used in REST client methods as URIs for APIs to Cisco CSR device, to make it easier to find and maintain them. File is:

neutron/services/vpn/device_drivers/cisco_csr_rest_client.py"
0,"currently, NotFound exception was raised when no volume found in v2 api
this will not be helpful to user especially there is no 'ec2 error code' returned"
1,"check_ssh_injection in cinder/utils.py is disallowing args with spaces even when the arg is quoted. This leads to an SSHInjectionThreat being raised when a volume driver needs to send a quoted arg containing spaces, e.g. when a storage pool name contains a space."
0,Ensure that this is in the correct section (following the HACKING rules)
0,"The test_delete_backup unit test fails sporadically as such:
2014-02-19 06:55:33.645 | Traceback (most recent call last):
2014-02-19 06:55:33.645 | File ""cinder/tests/test_backup.py"", line 360, in test_delete_backup
2014-02-19 06:55:33.645 | self.assertGreater(timeutils.utcnow(), backup.deleted_at)
2014-02-19 06:55:33.645 | File ""cinder/test.py"", line 280, in assertGreater
2014-02-19 06:55:33.645 | f(first, second, msg=msg)
2014-02-19 06:55:33.645 | File ""/usr/lib/python2.7/unittest/case.py"", line 940, in assertGreater
2014-02-19 06:55:33.646 | self.fail(self._formatMessage(msg, standardMsg))
2014-02-19 06:55:33.646 | File ""/usr/lib/python2.7/unittest/case.py"", line 408, in fail
2014-02-19 06:55:33.646 | raise self.failureException(msg)
2014-02-19 06:55:33.646 | AssertionError: datetime.datetime(2014, 2, 19, 6, 53, 14, 473836) not greater than datetime.datetime(2014, 2, 19, 6, 53, 14, 473836)"
1,In order to fix undesired error logs in Neutron (bug 1288188) fixed save_and_reraise_exception() should be synced from oslo.
1,"swift-recon has a filter to query only servers in a specific zone:
  -z ZONE, --zone=ZONE Only query servers in specified zone
If zone is set to ""0"", the filter is not applied."
0,"The windows nova-agent now can trigger a gust reboot during resetnetwork, so the hostname is correctly updated.

Also there was always a reboot during the first stages of polling for the agent version that can cause the need to wait for a call to timeout, rather than detecting a reboot.

Either way, we need to take more care to detect reboots while talking to the agent."
0,"The VPN logging code should use the new marker functions for info, warning, error and critical log levels to separate log messages into different catalogs for translation priority. We also need to ensure that the new i18n guidelines are followed."
1,"Currently when detaching a volume we do:
1. driver.detach() -> detaches the volume in the hypervisor
2. cinder.detach() -> marks the volume detached in cinder
3. bdm.delete() -> stops the nova side from reporting an attached volume.
This leads to bad UX for two reasons:
a. If the cinder detach fails, the bdm still exists so nova reports a volume attached even when the hypervisor
    has detached it.
b. There is a window where cinder reports the volume available but nova still thinks the attachment exists
I propose we reverse the order of 2. and 3.
This reverses a. so that a detach fail will show the volume as attached in cinder but nova will not show it. The nova side more accurately reflects what nova knows about.
This reverses b. so that there is a window where nova has removed the attachment but cinder still reports it in use. This is a fairly minor difference, but leads to a nicer experience when you are detaching and re-attaching the same volume because it can be reattached once it becomes available.
This also may help with https://bugs.launchpad.net/nova/+bug/1172695"
1,"Glance images are not being fetched by glance's API v1 when the size is 0. There are 2 things wrong with this behaviour:

1) Active images should always be ready to be downloaded, regardless they're locally or remotely stored.
2) The size shouldn't be the way to verify whether an image has some data or not.

https://git.openstack.org/cgit/openstack/glance/tree/glance/api/v1/images.py#n455

This is happening in the API v1, but it doesn't seem to be true for v2."
0,"The NetApp zapi for clone create has an undocumented limit of 32 block ranges (of a max 2^24 blocks each).
The driver should check to make sure the number of block range segments does not exceed 32. In the case of an excessively large request, the xml may be rejected by the filer silently and the volume will become stuck in an extending state, with 2 flexVols (the original and the new sized one) present on the filer."
0,"Description of problem:
The action cinder backup-create fails when trying to backup a newly created volume.
- The Cinder is using the cinder.volume.drivers.nfs.NfsDriver driver.
- All the other OS components are installed on 1 host and the cinder on a different host.
Version-Release number of selected component (if applicable):
openstack-cinder-2013.2-1.el6ost.noarch
How reproducible:
evevrytime
Steps to Reproduce:
1. create a volume
2. backup the volume
3.
Actual results:
the backup failed
Expected results:
the backup is available
Additional info:
2013-11-04 09:14:04.278 15089 ERROR cinder.openstack.common.rpc.amqp [req-2ae3d45e-40c6-4422-be59-1bba466f086c c7fdf6f628554d56aad363ad501ce412 add3de2deaa445c1a1e71c1721bc8976] Exception during message handling
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp **args)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 270, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp 'fail_reason': unicode(err)})
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 263, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp backup_service)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/nfs.py"", line 352, in backup_volume
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp raise NotImplementedError()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp NotImplementedError
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp"
1,"Table 'instance_groups' has two columns to unambiguously identify the specific row:
1) id (Integer, primary_key=True, autoincrement=True)
2) uuid (String)

The former is used internally to bind related entities by FKs (InstanceGroupMember, InstanceGroupPolicy and InstanceGroupMetadata), and the latter is accepted by public DB API methods (this must be a miss in the DB schema design, because uuid could be easily used for both use cases).

Having two 'id' columns is both misleading and error-prone. E. g. instance_group_delete() deletes the instance group (and all related entities) given its UUID value:

def instance_group_delete(context, group_uuid):
    """"""Delete an group.""""""
    session = get_session()
    with session.begin():
        count = _instance_group_get_query(context,
                                          models.InstanceGroup,
                                          models.InstanceGroup.uuid,
                                          group_uuid,
                                          session=session).soft_delete()
        if count == 0:
            raise exception.InstanceGroupNotFound(group_uuid=group_uuid)

        # Delete policies, metadata and members
        instance_models = [models.InstanceGroupPolicy,
                           models.InstanceGroupMetadata,
                           models.InstanceGroupMember]
        for model in instance_models:
            model_query(context, model, session=session).\
                    filter_by(group_id=group_uuid).\
                    soft_delete()

Related entities are filtered by 'group_id' column, but 'group_uuid' value is passed. Despite that these two columns are of different types, this statement is executed successfully on MySQL and SQLite (though WHERE clause never evaluates to True, so related entities aren't actually deleted), but fails on PostgreSQL, which is more strict when checking data types of values."
1,"Several exception is not correct in os-migrateLive action:
1.If the server state conflict to live-migrate, it's raises 400 exception.I think we should raise HTTPConflict instead of HTTPBadRequest.
2.the error msg is not accurate while several exception such as :
                exception.NoValidHost,
                exception.InvalidLocalStorage,
                exception.InvalidSharedStorage,
                exception.MigrationPreCheckError"
1,"The BigSwitch plugin incorrectly tries to log the response code it gets from an HTTP request as an integer when it should be a string, resulting in these errors in the output:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 552, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 516, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: %d format: a number is required, not str
Logged from file plugin.py, line 339"
1,"I believe this issue is part of the root cause for this nova bug: https://bugs.launchpad.net/nova/+bug/1255317
Basically, when creating a volume from a vmdk image, the adapter_type is ignored and it defaults to LSI logic.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L368
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L327
So cinder creates a volume with LSI SCSI controller and IDE disk. And eventually, when I try to boot from a volume in Nova, I get ""No operating system found"". I don't think you can boot an IDE disk using SCSI adapter.
However, I am able to boot from an image with IDE adapter. So seems the volume step is causing an issue.
My recreate steps:
1) glance image-create --name cirros-sparse --is-public=True --container-format=bare --disk-format=vmdk --property vmware_disktype=""sparse"" < cirros-0.3.0-i386-disk.vmdk
2) cinder create --name sparse-ide --image-id cd0a8dea-2e2f-43be-97ab-85e21375b757 1
3) Observe in vCenter that the newly created volume has adapter set as LSI logic.
My VMDK image metadata:
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
ericwb@ericwb-virtual-machine:~$ head -n 20disk-vmdk.vmdk
head: 20disk-vmdk.vmdk: invalid number of lines
ericwb@ericwb-virtual-machine:~$ head -n 20 disk-vmdk.vmdk
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
# The Disk Data Base
#DDB
ddb.virtualHWVersion = ""4""
ddb.geometry.cylinders = ""79""
ddb.geometry.heads = ""16""
ddb.geometry.sectors = ""63""
ddb.adapterType = ""ide"""
0,"Meters and security groups use 'test_tenant' while all other resources use 'test-tenant'.
This means that a test that creates multiple types of resources (While using the the default tenant id) would find that some resources were created under one tenant, and other resources under another tenant.
For example, a test creates a network, subnet, port, security group and meter label + rule. Listing all resources that belong to the 'test-tenant' would return a partial list, leading to confusing results."
1,"In nova.conf, the host_ip of the vmware section describes it as being a URL. But the expected input is a hostname or IP address, not a URL. A URL is composed of a scheme, two colons, etc. See http://en.wikipedia.org/wiki/Uniform_resource_locator
https://github.com/openstack/nova/blob/master/etc/nova/nova.conf.sample#L3265
[vmware]
#
# Options defined in nova.virt.vmwareapi.driver
#
# URL for connection to VMware ESX/VC host. (string value)
#host_ip=<None>
# Username for connection to VMware ESX/VC host. (string
# value)
#host_username=<None>
# Password for connection to VMware ESX/VC host. (string
# value)
#host_password=<None>
Also here:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L49"
1,"Test step :
1. Create a qos spec :
cinder qos-create qos-spec qos:IOThrottling=30000
2. Create type-1 with Pool-1 without replication.
3. Create type-2 with Primary Pool-1 and Secondary Pool-2, replication =TRUE.
4. qos-spec associate to type-1 and type-2.
cinder qos-associate qos-spec type-1
cinder qos-associate qos-spec type-2
5. cinder create --volume-type type-1 1 ,
The volume creation will be successed.
6. cinder create --volume-type type-2 1.
The volume creation will be failed.
Check the cinder log , it was found:
screen-c-sch.2014-09-11-151843.log:2014-10-10 14:01:41.497 ERROR cinder.scheduler.filter_scheduler [[[01;36mreq-9d20b04b-94fc-4936-a9c8-12a41690e313 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e] ^[[01;35mError scheduling None from last vol-service: ubuntu247@driver4#driver4 : [u'Traceback (most recent call last):\n', u' File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 35, in execute_task\n result = task.execute(**arguments)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 624, in execute\n **volume_spec)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in _create_raw_volume\n return self.driver.create_volume(volume_ref)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init.py"", line 571, in create_volume\n model_update = self.replication.create_replica(ctxt, volume)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 78, in create_replica\n self.driver.add_vdisk_copy(volume[\'name\'], dest_pool, vol_type)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init_.py"", line 661, in add_vdisk_copy\n self.configuration)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 858, in add_vdisk_copy\n volume_type=volume_type)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 561, in get_vdisk_params\n kvs = qos_specs.get_qos_specs(ctxt, qos_specs_id)[\'specs\']\n', u""UnboundLocalError: local variable 'ctxt' referenced before assignment\n""]"
1,"Right now, when we overrun a quota, we get an ugly stack trace in (at least) nova-api's log:
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack Traceback (most recent call last):
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/__init__.py"", line 119, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return req.get_response(self.application)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack application, catch_exc_info=False)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack app_iter = application(self.environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 545, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return self.app(env, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack response = self.app(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack resp = self.call_func(req, *args, **self.kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return self.func(req, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 917, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack content_type, body, accept)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 976, in _process_stack
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack action_result = self.dispatch(meth, request, action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 1057, in dispatch
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return method(req=request, **action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1248, in _action_resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return self._resize(req, id, flavor_ref, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1113, in _resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack self.compute_api.resize(context, instance, flavor_id, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/compute/api.py"", line 198, in wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return func(self, context, target, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/compute/api.py"", line 188, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return function(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/compute/api.py"", line 215, in _wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return fn(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/compute/api.py"", line 169, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack return f(self, context, instance, *args, **kw)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack File ""/opt/stack/new/nova/nova/compute/api.py"", line 2304, in resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack resource=resource)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack TooManyInstances: Quota exceeded for ram: Requested 51137, but already used 128 of 51200 ram
See this example for more:
http://logs.openstack.org/23/49623/3/check/check-tempest-devstack-vm-full/b0d348a/logs/screen-n-api.txt.gz?level=TRACE
This happens a lot, clearly multiple times per every tempest run:
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIlRvb01hbnlJbnN0YW5jZXM6IFF1b3RhIGV4Y2VlZGVkIGZvciByYW1cIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzgwOTA5MzQ3MjQ3fQ=="
1,"Steps to reproduce:
1) Configure glance to use local filesystem backend, make sure the quota is explicitly configured.
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) Upload image data. Make sure the image data size > quota
 glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-upload --file $HOME/testImg.qcow2 $img_id
Request returned failure status.
HTTPInternalServerError (HTTP 500)
Same thing happens if you use curl:
1) Configure glance to use local filesystem backend
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) curl -i -X PUT -H 'Transfer-Encoding: chunked' \
    -H 'X-Auth-Token: $(cat ~/user-token)' \
    -H 'Content-Type: application/octet-stream' \
    -H 'User-Agent: python-glanceclient' \
    --data-binary @$HOME/testImg.qcow2 http://127.0.0.1:9292/v2/images/$img_id/file
HTTP/1.1 100 Continue
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Thu, 02 Jan 2014 07:27:26 GMT
Connection: close"
1,"The REGEXP_LIKE operator for oracle db backend is not correct.

For the MySql database, the following SQL syntax is right:
---
select xxx from xxx where column REGEXP pattern
---

But for Oracle database, the following SQL syntax is not right:
---
select xxx from xxx where column REGEXP_LIKE pattern
---

It should be:
---
select xxx from xxx where REGEXP_LIKE (column, pattern)
---"
1,"liugya@liugya-ubuntu:~/devstack$ nova aggregate-add-host 1 liugya-ubuntu
Aggregate 1 has been successfully updated.
+----+------+-------------------+--------------------+----------------------------------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+--------------------+----------------------------------+
| 1 | agg1 | zone1 | [u'liugya-ubuntu'] | {u'availability_zone': u'zone1'} |
+----+------+-------------------+--------------------+----------------------------------+
liugya@liugya-ubuntu:~/devstack$ nova aggregate-add-host 2 liugya-ubuntu
ERROR: Cannot perform action 'add_host_to_aggregate' on aggregate 2. Reason: Host already in availability zonezone1.. (HTTP 409) (Request-ID: req-fe50ad9a-f20d-456c-8c1f-f236b8329b0c)
Two issues:
1) contain two period at the end of the log message
2) Should be availability zone zone1"
1,"This is showing up all over the n-cpu logs on teardown of tempest tests:

UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None

For example:

http://logs.openstack.org/06/103206/4/check/check-tempest-dsvm-postgres-full/b5e8f3c/logs/screen-n-cpu.txt.gz?level=TRACE

We have nearly 40K hits on this in logstash in 7 days:

message:""UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None"" AND tags:""screen-n-cpu.txt"""
1,"Currently if a user configures volume_clear='non_existent_volume_clearer' in cinder.conf, the LVM driver will silently delete a volume and not wipe it.
Instead, the delete operation should fail, leaving the volume in the 'error_deleting' state. This prevents a user from believing their volumes are being wiped when they are not due to misconfiguration."
0,"Logs are showing plenty of tracebacks like the following: http://logs.openstack.org/96/66796/15/check/check-tempest-dsvm-neutron-full/d940e56/logs/screen-q-svc.txt.gz?level=DEBUG#_2014-03-06_10_14_46_352
An operation (either update_floatingip_status or get_floating_ips) in update_floatingip_statuses (l3_rpc_base.py) triggers this traceback. Apparently this happens because if a floatingIP is removed, the transaction is aborted because of the exception.
Optimizing as suggested here: https://github.com/openstack/neutron/blob/master/neutron/db/l3_db.py#L680 might solve the issue."
0,The HTTPS class with certificate validation in the Big Switch server manager module references a source_address attribute that isn't present in the python 2.6 httplib.
1,"There are some issues with create_export and remove_export in driver.py:
1) There is a call to a create_export RPC, but the volume manager does not implement create_export
2) remove_export is not called in _detach_volume in driver.py, which means we have exports left over from several calls"
1,"Today... after d5cd6528f361979b073aabd036be0d28dc1c4b95 landed I'm now seeing the following exceptions in /var/log/cinder/scheduler.log:
2013-09-24 17:29:00.771 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 1 seconds
2013-09-24 17:29:01.777 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 2 seconds
2013-09-24 17:29:03.781 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 4 seconds
2013-09-24 17:29:07.787 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 8 seconds
2013-09-24 17:31:47.402 10117 ERROR cinder.volume.flows.create_volume [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Failed to schedule_create_volume: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.428 10117 WARNING cinder.taskflow.utils [-] Activating 3 rollbacks due to <cinder.taskflow.utils.FlowFailure object at 0x3102290>.
2013-09-24 17:31:47.429 10117 ERROR cinder.openstack.common.rpc.amqp [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Exception during message handling
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp **args)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/scheduler/manager.py"", line 94, in create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp flow.run(context)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 105, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(self, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 232, in run
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp run_it(r)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 212, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp self.rollback(context, cause)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp result = runner(context, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp self.result = self.task(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 148, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 264, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 199, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 234, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 177, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1678, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp _error_out_volume(context, db, volume_id, reason=e)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1663, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp filter_properties)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/site-packages/cinder/scheduler/chance.py"", line 81, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp image_id=image_id)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp TypeError: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp"
1,"Missing comma here:
https://github.com/openstack/neutron/blob/30f6f2fa4e6e73cf8045febbd1e8d26360714ac5/neutron/db/migration/alembic_migrations/versions/2eeaf963a447_floatingip_status.py#L38"
0,"Trying to use iSCSI with libvirt/Xen, attaching volumes to instances was failing. I tracked this down to the libvirt XML looking like:
<disk type=""block"" device=""disk"">
  <driver name=""file"" type=""raw"" cache=""none""/>
  <source dev=""/dev/disk/by-path/ip-192.168.8.11:3260-iscsi-iqn.1986-03.com.sun:02:ecd142ab-b1c7-6bcf-8f91-f55b6c766bcc-lun-0""/>
  <target bus=""xen"" dev=""xvdb""/>
  <serial>e8c640c6-641b-4940-88f2-79555cdd5551</serial>
</disk>
The driver name should be ""phy"", not ""file"".
More digging lead to the iSCSI volume driver in nova/virt/libvirt/volume.py, which does:
class LibvirtISCSIVolumeDriver(LibvirtBaseVolumeDriver):
    """"""Driver to attach Network volumes to libvirt.""""""
    def __init__(self, connection):
        super(LibvirtISCSIVolumeDriver,
              self).__init__(connection, is_block_dev=False)
Surely is_block_dev should be ""True"" for iSCSI?? Changing this makes the problem go away - now pick_disk_driver_name() in nova/virt/libvirt/utils.py does the right thing and my volume attaches successfully.
Am I missing something here... ?"
0,"There are a number of places in the neutron code that run an ip command like this:

        if self.network.namespace:
            ip_wrapper = ip_lib.IPWrapper(self.root_helper,
                                          self.network.namespace)
            ip_wrapper.netns.execute(cmd)
        else:
            utils.execute(cmd, self.root_helper)

This code could be simplified if netns.execute simply checked if there was a namespace defined or not."
