BIC,description
0,We need to raise an exception for an disk_format that is not supported
0,"Action ""os-migrateLive"" not defined in actions list in test_actions_with_non_existed_instance func.
It should not test the action ""os-migrateLive""."
1,"When the lioadm is the iscsi helper the target just exists when the volume attached.
I should be able to delete a volume what was never attached.

The volumes which was attached to a vm before the delete request are deletable."
1,"This happens during the upgrade from grizzly to havana using packages from the Ubuntu Cloud Archive on an Ubuntu 12.04.3 system.

root@openstack01:~# cinder-manage db sync
2013-10-01 23:35:04.977 7923 INFO migrate.versioning.api [-] 9 -> 10...
2013-10-01 23:35:05.000 7923 ERROR 010_add_transfers_table [-] Table |Table('transfers', MetaData(bind=Engine(mysql://cinder:eNgoyam3@127.0.0.1/cinder?charset=utf8)), Col
umn('created_at', DateTime(), table=<transfers>), Column('updated_at', DateTime(), table=<transfers>), Column('deleted_at', DateTime(), table=<transfers>), Column('delete
d', Boolean(), table=<transfers>), Column('id', String(length=36), table=<transfers>, primary_key=True, nullable=False), Column('volume_id', String(length=36), ForeignKey
('volumes.id'), table=<transfers>), Column('display_name', String(length=255), table=<transfers>), Column('salt', String(length=255), table=<transfers>), Column('crypt_ha
sh', String(length=255), table=<transfers>), Column('expires_at', DateTime(), table=<transfers>), schema=None)| not created!"
0,"The commands module was deprecated since version 2.6 and it has been
removed in Python 3. Use the subprocess module instead.
See http://docs.python.org/2/library/commands#module-commands"
1,"When updating the metadata for an instance, the values should end up in xenstore for a guest to be able to query. This is related to https://bugs.launchpad.net/nova/+bug/1292181 but with instance objects metadata is now synced down to cells earlier than it used to be. This causes an issue with the metadata diff detection at the cell level so new keys are not pushed to the virt driver. This causes them to not be set in xenstore in the xenserver driver."
0,"I found a testcase bug in nova/tests/api/openstack/compute/plugins/v3/test_availability_zone.py
     def test_create_instance_with_availability_zone(self):
         def create(*args, **kwargs):
             self.assertIn('availability_zone', kwargs)
             return old_create(*args, **kwargs)
         old_create = compute_api.API.create
         self.stubs.Set(compute_api.API, 'create', create)
         image_href = '76fa36fc-c930-4bf3-8c8a-ea2a2420deb6'
         flavor_ref = 'http://localhost/v3/flavors/3'
         body = {
             'server': {
                 'name': 'config_drive_test',
                 'image_ref': image_href,
                 'flavor_ref': flavor_ref,
                 'metadata': {
                     'hello': 'world',
                     'open': 'stack',
                 },
               鈽?'availability_zone': ""nova"",鈽?             },
         }
         req = fakes.HTTPRequestV3.blank('/v3/servers')
         req.method = 'POST'
         req.body = jsonutils.dumps(body)
         req.headers[""content-type""] = ""application/json""
         admin_context = context.get_admin_context()
         service1 = db.service_create(admin_context, {'host': 'host1_zones',
                                          'binary': ""nova-compute"",
                                          'topic': 'compute',
                                          'report_count': 0})
         agg = db.aggregate_create(admin_context,
                 {'name': 'agg1'}, {'availability_zone': 'nova'})鈽?         db.aggregate_host_add(admin_context, agg['id'], 'host1_zones')
The correct request for availability-zone option is 'os-availability-zone:availability_zone'."
1,MlnxException exceptions expect a keyword argument with the err_msg key. This is not the case in the code: https://github.com/openstack/neutron/blob/8a70bfd97a6f27dcae41e0b895d84ce5c19238ad/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py#L61
1,"In _report_final_resource_view() method
 if 'pci_devices' in resources:
            LOG.audit(_(""Free PCI devices: %s"") % resources['pci_devices'])
but in update_available_resource() method
if self.pci_tracker:
            self.pci_tracker.clean_usage(instances, migrations, orphans)
            resources['pci_stats'] = jsonutils.dumps(self.pci_tracker.stats)
        else:
            resources['pci_stats'] = jsonutils.dumps([])
resources has key ""pci_stats"" not ""pci_devices""
https://review.openstack.org/#/c/90671/"
1,"When the XenAPI driver resizes a boot partition, it does not take care to add back the boot partition flag.
With PV images, this is not really needed, because Xen doesn't worry about the partition being bootable, but for HVM images, it is stops the image from booting any more."
1,Assuming this is the culprit given it merged on 6/8 and is related to floating ip's which is what shows up in that n-api log mess.
1,"I viewed the code and found that:
@require_admin_context
def cell_update(context, cell_name, values):
    session = get_session()
    with session.begin():
        cell = _cell_get_by_name_query(context, cell_name, session=session)
        cell.update(values)
    return cell

The method nova.db.sqlalchemy.api.cell_update() returns a 'Query' object.

def _filter_keys(item, keys):
    """"""
    Filters all model attributes except for keys
    item is a dict

    """"""
    return dict((k, v) for k, v in item.iteritems() if k in keys)

The method nova.api.openstack.compute.contrib.cells_filter_keys() uses the 'Query' object as a dict.

This bring on the error."
0,LinuxBridgeManager is adding a new method for checking the existence of a device instead of using the one in ip_lib
1,comment misspelled about volume_clear in cinder/volume/driver.py (volume misspelled)
0,"I created a backup to an instance which had no volume attached.

attached a volume -> rebuild the instance from the backup.

It appears as though the volume is not attached anymore after the rebuild, but if we try to attach it to the same device we get an error that a device is already attached:

2013-10-18 16:54:36.632 2478 DEBUG qpid.messaging [-] RETR[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""dd2a85b63c56498c8f2835f9b96e9bb9""
, ""failure"": null, ""_msg_id"": ""7ce524cebbb34aecab9d608a48103a1c"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) _get /usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py:654
2013-10-18 16:54:36.633 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: MessageFlow(destination='0', unit=0, value=1L, id=serial(5206)) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.634 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: SessionCompleted(commands=[0-5199]) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.639 2478 ERROR nova.openstack.common.rpc.amqp [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Exception during message handling
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp pass
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3624, in reserve_block_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp return do_reserve()
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp return f(*args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3613, in do_reserve
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp context, instance, bdms, device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 135, in get_device_name_for_instance
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp mappings['root'], device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 217, in get_next_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp raise exception.DevicePathInUse(path=device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp DevicePathInUse: The supplied device path (/dev/vdc) is in use.
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp
2013-10-18 16:54:36.641 2478 ERROR nova.openstack.common.rpc.common [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Returning exception The supplied device path (/dev/vdc) is i
n use. to caller"
0,"os-interface' resource name needs to be fixed in V3 code base to work for V2.1 API.

V2 and V3 diff for this resource name is below-

V2 - '/servers/os-interface'
V3 - '/servers/os-attach-interfaces'

V3 resource name needs to be changed to work that for V2.1.

This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1,"In my testbed, ceilometer throws an exception which is caused by the wrong key format of neutron:
2014-04-01 10:52:39.135 6104 ERROR notification [-] Insert msg to Mongodb occured error:Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ceilometer/billing/backend.py"", line 74, in insert_data
    self.db[collection].insert(data, safe=True)
  File ""/usr/lib64/python2.6/site-packages/pymongo/collection.py"", line 357, in insert
    continue_on_error, self.__uuid_subtype), safe)
InvalidDocument: key 'router.interface' must not contain '.'
It seems that mongodb doesn't support ""."" in key.
The workaround is to change ""."" with ""_"" in l3_db.py."
0,"The ""allocated"" variable used in migrate_to_ml2 was defined to be a boolean type and in postgresql this type is enforced,
while in mysql this just maps to tinyint and accepts both numbers and bools.

Thus the migrate_to_ml2 script breaks on postgresql"
0,"bad assertion in nova.tests.virt.vmwareapi.vmwareapi.test_vmops.py:640
self.assertTrue(3, len(mock_mkdir.mock_calls)) should be replaced with assertEqual"
1,"Currently at startup Glance is first creating the stores and then verifying the default store (cmd/api.py)
 30 glance.store.create_stores()
 31 glance.store.verify_default_store()
In both of these calls, the store objects are created (store/base.py) which means that if the methods configure() and configure_add() methods are defined in the store, these methods will be called two times: 1 time in the ""create_stores()"" phase and 1 time in the verify_default_store() phase.
It turns out that the configure()/configure_add() method can contain remote calls (which can potentially be expensive): in which case we do not want them to happen two times. Having configure/configure_add called only one time will speed up the Glance startup time for some of the stores."
0,"The Icehouse release includes a server group REST API. For these groups to actually function properly, the server group scheduler filters must be enabled. So, these filters should be enabled by default since the API is also enabled by default. If the API is not used, the scheduler filters will be a no-op.

http://lists.openstack.org/pipermail/openstack-dev/2014-April/032068.html"
1,"Description of problem:
Unpauseing an instance fails if host has rebooted.
Version-Release number of selected component (if applicable):
RHEL: release 6.5 (Santiago)
openstack-nova-api-2013.2.1-1.el6ost.noarch
openstack-nova-compute-2013.2.1-1.el6ost.noarch
openstack-nova-scheduler-2013.2.1-1.el6ost.noarch
openstack-nova-common-2013.2.1-1.el6ost.noarch
openstack-nova-console-2013.2.1-1.el6ost.noarch
openstack-nova-conductor-2013.2.1-1.el6ost.noarch
openstack-nova-novncproxy-2013.2.1-1.el6ost.noarch
openstack-nova-cert-2013.2.1-1.el6ost.noarch
How reproducible:
Every time
Steps to Reproduce:
1. Boot an instance
2. Pause that instance
3. Reboot host
4. Unpause instance
Actual results:
can't unpause instance stuck in status paused, power state - shutdown
Expected results:
Instance should unpause, return to running state
Additional info:
virsh list -all --managed-save
ID is missing from paused instance ""-"" (pausecirros), state -> shut off.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id Name State
----------------------------------------------------
 1 instance-00000003 running
 2 instance-00000002 running
 - instance-00000001 shut off
[root@orange-vdse ~(keystone_admin)]# nova list (notice nova status paused)
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros | ACTIVE | None | Running | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros | PAUSED | None | Shutdown | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None | Running | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
Testing without rebooting host, ID/state (""1""/paused) instance (cirros) are ok and it unpauses ok.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id Name State
----------------------------------------------------
 1 instance-00000003 paused
 2 instance-00000002 running
 - instance-00000001 shut off
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros | PAUSED | None | Paused | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros | PAUSED | None | Shutdown | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None | Running | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+"
1,"There is an AttributeError when we try to use the command ""nova migration-list""
Traceback (most recent call last):
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 721, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 657, in main
    args.func(self.cs, args)
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 71, in do_migration_list
    args.cell_name))
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 53, in list
    return self._list(""/os-migrations%s"" % query_string, ""migrations"")
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 80, in _list
    for res in data if res]
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 426, in __init__
    self._add_details(info)
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 449, in _add_details
    for (k, v) in six.iteritems(info):
  File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 439, in iteritems
    return iter(getattr(d, _iteritems)(**kw))
AttributeError: 'unicode' object has no attribute 'iteritems'
ERROR: 'unicode' object has no attribute 'iteritems'"
1,"1st) Some configuration options are not registered on the tool, but they're used in neutron.agent.linux.dhcp during execution
$ neutron-netns-cleanup --debug --force --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini --config-file /etc/neutron/plugins/ml2/ml2_conf.ini
2014-03-12 14:55:44.791 INFO neutron.common.config [-] Logging enabled!
2014-03-12 14:55:44.792 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list'] from (pid=1785) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-03-12 14:55:45.001 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
Exit code: 0
Stdout: 'qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2\nqrouter-acc5f724-a169-4ffc-9e81-f00d43954509\nqrouter-5ed23337-9538-4994-823f-c64720506e54\n'
Stderr: '' from (pid=1785) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 14:55:47.006 ERROR neutron.agent.linux.dhcp [-] Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
2nd) When we try to destroy a network, there's a dependency on the .namespace attribute of the network, that wasn't before.
Stderr: '' from (pid=1969) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 15:08:53.048 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 131, in destroy_namespace
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util kill_dhcp(conf, namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 86, in kill_dhcp
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util dhcp_driver.disable()
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util self.device_manager.destroy(self.network, self.interface_name)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 814, in destroy
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util self.driver.unplug(device_name, namespace=network.namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util AttributeError: 'FakeNetwork' object has no attribute 'namespace'
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util
3rd) This error will happen because no plugin rpc connection is provided,
and that's used in /opt/stack/neutron/neutron/agent/linux/dhcp.py as self.plugin.release_dhcp_port
2014-03-13 12:00:07.880 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-388a37af-556d-4f4c-98b4-0ba41f944e32
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 132, in destroy_namespace
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util kill_dhcp(conf, namespace)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 87, in kill_dhcp
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util dhcp_driver.disable()
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util self.device_manager.destroy(self.network, self.interface_name)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 816, in destroy
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util self.plugin.release_dhcp_port(network.id,
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util AttributeError: 'NoneType' object has no attribute 'release_dhcp_port'
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util"
1,"In begin_detaching, the detach will be attempted if volume state is ""in-use"" OR attach_status is ""attached"" but the error message states that detach will only be attempted if volume state is ""in-use"" AND attach_status is ""attached"""
1,"NotImplementedError is the name of the exception (https://docs.python.org/2/library/exceptions.html).
NotImplemented is the name of a constant (https://docs.python.org/2/library/constants.html).
It makes no sense to raise a constant. The exception should be raised instead."
1,Downgrade in migration b7a8863760e_rm_cisco_vlan_bindin fails http://paste.openstack.org/show/87396/
0,"For several plugins the security group update notification on port-update is sent twice.
1) https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L634
2) https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L101
the notification at #1 is sent but not that at #2 if the security group on the port was not updated.
However if the notication at #2 is sent, then the notification at #1 will always be sent too.
Therefore the notification at #2 is probably redundant
This affects at least the following plugins:
- nec
- ryu
- linuxbridge
- openvswitch
- ryu
This causes a lot more traffic on the agent side thus slowing all the operations on the agent, including wiring VIF ports."
1,"Callers of wait_for_block_job_info may loop infinitely if the job doesn't exist, since libvirt returns an empty dict which this function interprets as cur=0 and end=0 => return True.
I think it should do:
if not any(status):
    return False
Affects online deletion of Cinder GlusterFS snapshots, and possibly other callers of this (live_snapshot).
See http://libvirt.org/git/?p=libvirt-python.git;a=commit;h=f8bc3a9ccc
(Encountered issue on Fedora 19 w/ virt-preview repo, libvirt 1.1.3.)"
1,"There is configuration item in /etc/nova/nova.conf that controls how often the instance info should be updated. By default the value is 60 seconds. However, the current implementation only uses that value to prevent over clocked. Configure it to a different value in nova.conf does not has impact how often the task is executed.
If I change the code in /usr/lib/python2.6/site-packages/nova/compute/manager.py with the spacing parameter, the configured value will be in action. Please fix this bug.
@periodic_task.periodic_task(spacing=CONF.heal_instance_info_cache_interval)
    def _heal_instance_info_cache(self, context):"
1,"when shelve a vm and CONF.shelved_offload_time == 0,the vm would be offloaded right now.
referring to nova/compute/manager.py:def shelve_instance(self, context, instance, image_id):
        if CONF.shelved_offload_time == 0:
            instance.task_state = task_states.SHELVING_OFFLOADING
        instance.power_state = current_power_state
        instance.save(expected_task_state=[
                task_states.SHELVING,
                task_states.SHELVING_IMAGE_UPLOADING])
        if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)
        self._notify_about_instance_usage(context, instance, 'shelve.end')
thus,notification may occurs like:
compute.instance.shelve.start
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
compute.instance.shelve.end
in fact,order should be like:
compute.instance.shelve.start
compute.instance.shelve.end
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
I suggest that ""self._notify_about_instance_usage(context, instance, 'shelve.end')"" should be moved before
 "" if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)"""
1,"dhcp-agent notifier outputs ERROR log ""No DHCP agents are associated with network""
but it is not an error and valid cases in most cases. It is so annoying for debugging and monitoring.
No dhcp-agent association for a network was logged as ERROR level. However it is completely incorrect for network_create_end, and usually wrong for subnet_create_end (because a subnet is created just after a network is created in most cases). We should not log it for these cases.
For other cases, a dhcp-agent is usually associated with a network, and no dhcp-agent assocation might be a symptom of some error. On the other hand there are valid cases where no dhcp-agent is associated (e.g., delete dhcp-agent association intentionally, network/subnet_update_end before a port is created). Considering these and the fact that error will be logged in
dhcp-agent scheduler, it would be better to be logged as INFO level now.
2014-03-07 02:29:08.533 5415 ERROR neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-0babab8c-d638-4f1e-aef3-d58ad6af8b86 None] No DHCP agents are associated with network '6c3c60c7-2fce-4513-971c-b49b98d153c8'. Unable to send notification for 'network_create_end' with payload: {'network': {'status': 'ACTIVE', 'subnets': [], 'name': u'private', 'provider:physical_network': None, 'admin_state_up': True, 'tenant_id': u'33f39be0287d4316a29a4247dcd6db5c', 'provider:network_type': u'local', 'shared': False, 'id': '6c3c60c7-2fce-4513-971c-b49b98d153c8', 'provider:segmentation_id': None}}
http://logs.openstack.org/35/78835/1/check/check-tempest-dsvm-neutron/cd96f2f/logs/screen-q-svc.txt.gz?level=ERROR"
1,"Migration 5446f2a45467_set_server_default try to set incorrect default boolean value on Integer column cisco_network_profiles.
http://paste.openstack.org/show/86383/"
0,"if setup_rpc is too early, the dispatch maybe dispatch the rpc message to an unready agent. take ovs plugin agent for instance,
after setup_rpc is called, many of the initialization work are still needed to be done. If the message is coming during this time, the instance will not be fully initialized:
    def __init__(self, integ_br, tun_br, local_ip,
                 bridge_mappings, root_helper,
                 polling_interval, tunnel_types=None,
                 veth_mtu=None, l2_population=False,
                 minimize_polling=False,
                 ovsdb_monitor_respawn_interval=(
                     constants.DEFAULT_OVSDBMON_RESPAWN)):
        '''Constructor.
        :param integ_br: name of the integration bridge.
        :param tun_br: name of the tunnel bridge.
        :param local_ip: local IP address of this hypervisor.
        :param bridge_mappings: mappings from physical network name to bridge.
        :param root_helper: utility to use when running shell cmds.
        :param polling_interval: interval (secs) to poll DB.
        :param tunnel_types: A list of tunnel types to enable support for in
               the agent. If set, will automatically set enable_tunneling to
               True.
        :param veth_mtu: MTU size for veth interfaces.
        :param minimize_polling: Optional, whether to minimize polling by
               monitoring ovsdb for interface changes.
        :param ovsdb_monitor_respawn_interval: Optional, when using polling
               minimization, the number of seconds to wait before respawning
               the ovsdb monitor.
        '''
        self.veth_mtu = veth_mtu
        self.root_helper = root_helper
        self.available_local_vlans = set(xrange(q_const.MIN_VLAN_TAG,
                                                q_const.MAX_VLAN_TAG))
        self.tunnel_types = tunnel_types or []
        self.l2_pop = l2_population
        self.agent_state = {
            'binary': 'neutron-openvswitch-agent',
            'host': cfg.CONF.host,
            'topic': q_const.L2_AGENT_TOPIC,
            'configurations': {'bridge_mappings': bridge_mappings,
                               'tunnel_types': self.tunnel_types,
                               'tunneling_ip': local_ip,
                               'l2_population': self.l2_pop},
            'agent_type': q_const.AGENT_TYPE_OVS,
            'start_flag': True}
        # Keep track of int_br's device count for use by _report_state()
        self.int_br_device_count = 0
        self.int_br = ovs_lib.OVSBridge(integ_br, self.root_helper)
        self.setup_rpc()
        self.setup_integration_br()
        self.setup_physical_bridges(bridge_mappings)
        self.local_vlan_map = {}
        self.tun_br_ofports = {constants.TYPE_GRE: {},
                               constants.TYPE_VXLAN: {}}
        self.polling_interval = polling_interval
        self.minimize_polling = minimize_polling
        self.ovsdb_monitor_respawn_interval = ovsdb_monitor_respawn_interval
        if tunnel_types:
            self.enable_tunneling = True
        else:
            self.enable_tunneling = False
        self.local_ip = local_ip
        self.tunnel_count = 0
        self.vxlan_udp_port = cfg.CONF.AGENT.vxlan_udp_port
        self._check_ovs_version()
        if self.enable_tunneling:
            self.setup_tunnel_br(tun_br)
        # Collect additional bridges to monitor
        self.ancillary_brs = self.setup_ancillary_bridges(integ_br, tun_br)
        # Security group agent supprot
        self.sg_agent = OVSSecurityGroupAgent(self.context,
                                              self.plugin_rpc,
                                              root_helper)
        # Initialize iteration counter
        self.iter_num = 0"
1,"Full day is 86400 sec

diff --git a/nova/utils.py b/nova/utils.py
index 65d99aa..0b9afe4 100644
--- a/nova/utils.py
+++ b/nova/utils.py
@@ -90,7 +90,7 @@ TIME_UNITS = {
     'SECOND': 1,
     'MINUTE': 60,
     'HOUR': 3600,
- 'DAY': 84400
+ 'DAY': 86400
 }

Based on code from master branch 2014-08-14 HEAD commit 374e9766c20c9f83dbd8139aa9d95a66b5da7295"
0,"When I using 'meter-label-rule-create' command with neutronclient, excluded option is doesn't work.
The option exclude this cidr from the label.
You have to fix below code:
neutron / neutron / services / metering / drivers / iptables / iptables_driver.py
(line number is 157)
def _process_metering_label_rules(self, rm, rules, label_chain,
                                      rules_chain):
        im = rm.iptables_manager
        ext_dev = self.get_external_device_name(rm.router['gw_port_id'])
        if not ext_dev:
            return
        for rule in rules:
            remote_ip = rule['remote_ip_prefix']
            dir = '-i ' + ext_dev
            if rule['direction'] == 'egress':
                dir = '-o ' + ext_dev
            if rule['excluded'] == 'true': ------> fix it : True (boolean type)
                ipt_rule = dir + ' -d ' + remote_ip + ' -j RETURN'
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule, wrap=False,
                                           top=True)
            else:
                ipt_rule = dir + ' -d ' + remote_ip + ' -j ' + label_chain
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule,
                                           wrap=False, top=False)"
1,"PLUMgrid Plugin needs to fix error message typo in ""_network_admin_state"" function where
""Network Admin State Validation Falied"" needs to have typo fixed."
1,"Looks like an intermittent failure:
http://logs.openstack.org/25/64725/4/check/gate-nova-python27/e603e9e/testr_results.html.gz
2014-01-06 21:49:45.870 | Traceback (most recent call last):
2014-01-06 21:49:45.870 | File ""nova/tests/api/ec2/test_cloud.py"", line 2343, in test_create_image_with_reboot
2014-01-06 21:49:45.870 | self._do_test_create_image(False)
2014-01-06 21:49:45.871 | File ""nova/tests/api/ec2/test_cloud.py"", line 2316, in _do_test_create_image
2014-01-06 21:49:45.871 | no_reboot=no_reboot)
2014-01-06 21:49:45.871 | File ""nova/api/ec2/cloud.py"", line 1709, in create_image
2014-01-06 21:49:45.872 | name)
2014-01-06 21:49:45.872 | File ""nova/compute/api.py"", line 161, in inner
2014-01-06 21:49:45.872 | method=f.__name__)
2014-01-06 21:49:45.873 | InstanceInvalidState: Instance b1d4d924-069c-409c-bbdb-4f0478526057 in task_state powering-off. Cannot snapshot_volume_backed while the instance is in this state."
1,"If the deletion of physical image failed with OSError for some reason, it is not possible to delete the image using image-delete api call.
In the longevity test we have encountered this issue, IMO there is some issue with OS and it is throwing ""Forbidden"" exception and somehow its not deleting that file, so in order to reproduce this issue please follow the below mentioned steps:
Steps to reproduce:
1. image creation, upload
    openstack@opencloud1:~$ glance image-create
    +------------------+--------------------------------------+
    | Property | Value |
    +------------------+--------------------------------------+
    | checksum | None |
    | container_format | None |
    | created_at | 2014-01-29T02:27:07 |
    | deleted | False |
    | deleted_at | None |
    | disk_format | None |
    | id | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public | False |
    | min_disk | 0 |
    | min_ram | 0 |
    | name | None |
    | owner | de4a6631051a4df09bc1b12923244296 |
    | protected | False |
    | size | 0 |
    | status | queued |
    | updated_at | 2014-01-29T02:27:07 |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ glance image-update 675a82ab-4515-451a-932b-6da7f8bce056 --file images/CorePlus4.7.1.qcow2 --disk-format qcow2 --container-format bare
    +------------------+--------------------------------------+
    | Property | Value |
    +------------------+--------------------------------------+
    | checksum | a5282e9259f822c782bc7aea8a8870c6 |
    | container_format | bare |
    | created_at | 2014-01-29T02:27:07 |
    | deleted | False |
    | deleted_at | None |
    | disk_format | qcow2 |
    | id | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public | False |
    | min_disk | 0 |
    | min_ram | 0 |
    | name | None |
    | owner | de4a6631051a4df09bc1b12923244296 |
    | protected | False |
    | size | 43450368 |
    | status | active |
    | updated_at | 2014-01-29T02:27:20 |
    +------------------+--------------------------------------+
2. write protect the image file
    openstack@opencloud1:~$ sudo chattr +i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
3. delete the image using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
4. Image is logically deleted, but physically remaining
    openstack@opencloud1:~$ glance image-show 675a82ab-4515-451a-932b-6da7f8bce056
    +------------------+--------------------------------------+
    | Property | Value |
    +------------------+--------------------------------------+
    | checksum | a5282e9259f822c782bc7aea8a8870c6 |
    | container_format | bare |
    | created_at | 2014-01-29T02:27:07 |
    | deleted | True |
    | deleted_at | 2014-01-29T02:30:49 |
    | disk_format | qcow2 |
    | id | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public | False |
    | min_disk | 0 |
    | min_ram | 0 |
    | owner | de4a6631051a4df09bc1b12923244296 |
    | protected | False |
    | size | 43450368 |
    | status | deleted |
    | updated_at | 2014-01-29T02:30:49 |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance 43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
5. Remove the write-protect from image file
    openstack@opencloud1:~$ sudo chattr -i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
6. Try to delete the image again using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
7. Image file will not get deleted
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance 43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
This issue is reproducible only in v1 version. In case of v2 version, it doesn't delete the meta data associated with the image before actually deleting it from the backend store.
In glance, if you enable delayed_delete = True and run glance-scrubber service, glance api will put the image in the queue when the image is deleted by the user and this image will be deleted asynchronously. If it encounters the above reported issue while deleting the image in the glance-scrubber, then this service will simply log error message and keep on retrying deleting the image until it is deleted finally.
Possible solution:
1. When delayed_delete is enabled.
delete_image_metadata method should be moved to the scrubber and called immediately after setting the image status from ""pending_delete"" to ""deleted"".
2. When delayed_delete is not enabled.
delete_image_metadata method should be called after deleting the actual image from the backend.
i.e. after upload_utils.initiate_deletion(req, image['location'], id, CONF.delayed_delete) is called
Also the status of the image should be set back to the previous state in the forbidden exception block."
0,"When the HP LeftHand driver is configured in legacy mode it will fail with the following exception, if paramiko 1.13.0 is installed:
2014-03-27 13:33:22.189 DEBUG cinder.openstack.common.lockutils [req-c2080e55-ec3e-40e3-a7a6-329e48d22295 None None] Released file lock ""lefthand"" at /opt/stack
/data/cinder/cinder-lefthand for method ""get_volume_stats""... from (pid=35801) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/semaphore.py"", line 121, in _do_acquire
    waiter.switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/openstack/common/service.py"", line 483, in run_service
    service.start()
  File ""/opt/stack/cinder/cinder/service.py"", line 103, in start
    self.manager.init_host()
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 308, in init_host
    self.publish_service_capabilities(ctxt)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1105, in publish_service_capabilities
    self._report_driver_status(context)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1094, in _report_driver_status
    volume_stats = self.driver.get_volume_stats(refresh=True)
  File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_iscsi.py"", line 121, in get_volume_stats
    data = self.proxy.get_volume_stats(refresh)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 421, in get_volume_stats
    self._update_backend_status()
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 436, in _update_backend_status
    'clusterName': self.configuration.san_clustername})
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 109, in _cliq_run_xml
    result_xml = etree.fromstring(out)
  File ""lxml.etree.pyx"", line 3003, in lxml.etree.fromstring (src/lxml/lxml.etree.c:67314)
  File ""parser.pxi"", line 1724, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:101147)
ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.
This bug also exists with the Havana version of the driver. cinder.volume.drivers.san.hp_lefthand.HpSanISCSIDriver
Workaround: install paramiko 1.10.0"
1,"If volume backed snapshot is available for an user euca-describe-images crashes due to 'unknown server error'.
Environment: DevStack
Steps to reproduce:
1 Create a volume backed snapshot
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy:::1 inst
2 List images to ensure the created snapshot is available.
$ glance image-list
3 Describe images by euca2ools
$ euca-describe-images
TypeError: Unknown error occurred.
nova-api.log
2014-05-26 23:16:18.070 ERROR nova.api.ec2 [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] Unexpected TypeError raised: int() argument must be a string or a number,
not 'NoneType'
2014-05-26 23:16:18.071 DEBUG nova.api.ec2.faults [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] EC2 error response: TypeError: Unknown error occurred. ec2_error_response /opt/stack/nova/nova/api/ec2/faults.py:29"
1,"In the dhcp-agent, it is not possible to set a dhcp_lease_duration of 'infinite' since the variable is set as an IntOpt in neutron/common/config.py. There are instances, however, where an infinite lease duration may be desirable (particularly in a tripleo installation where the seed may go away but the undercloud and overcloud are expected to persist). Additionally, dnsmasq itself supports the 'infinite' value for the lease-time in the dhcp-range configuration."
1,"1) nova suspend vm2
2) nova attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
3) nova volume-attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
4) nova resume vm2
VM failed to resume and nova compute report the following errors.
2013-10-18 12:16:33.175 ERROR nova.openstack.common.rpc.amqp [req-a8b196e3-dbd5-45f4-814e-56a715b07fdf admin admin] Exception during message handling
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 354, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp payload)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp pass
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 295, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp e, sys.exc_info())
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 3314, in resume_instance
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1969, in resume
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3206, in _create_domain_and_network
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp {'connection_info': jsonutils.dumps(connection_info)})
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/compute/manager.py"", line 420, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp context, bdm_id, values)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/conductor/api.py"", line 170, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp context, values, create=False)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 244, in block_device_mapping_update_or_create
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp values=values, create=create)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return cast_or_call(ctxt, msg, **self.kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp result = rpc.call(context, real_topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp return _get_impl().call(CONF, context, topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp rpc_amqp.get_connection_pool(conf, Connection))
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 572, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp rv = multicall(conf, context, topic, msg, timeout, connection_pool)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 558, in multicall
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp pack_context(msg, context)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 308, in pack_context
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp for (key, value) in context.to_dict().iteritems()])
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'to_dict'
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp"
1,"In an environment with multiple neutron-servers, I have observed that a router can get scheduled to an l3-agent more than once. A ""neutron l3-agent-list-hosting-router <router id>"" will show the router scheduled twice to the same l3-agent or perhaps to two different agents. This can be reproduced using devstack. A second neutron-server on another host has to be configured. Executing a script against each of the neutron-servers which adds (neutron l3-agent-router-add) and removes (neutron l3-agent-router-remove) a router from an l3 agent is the quickest way to reproduce the race condition. There is no locking or other coordination across multiple neutron-server processes to prevent this."
0,"We should update Cinder for using Oslo when we generate configuration files.

Here are the steps :

- Add generator.py from Oslo
- Add generator bash script used by other projects to generate configuration
files
- Update openstack-common.conf file
- Generate configuration file"
1,"server's action confirm_resize return 204 now, but it should be 202
    @wsgi.response(202)
    @wsgi.serializers(xml=FullServerTemplate)
    @wsgi.deserializers(xml=ActionDeserializer)
    @wsgi.action('confirm_resize')
    def _action_confirm_resize(self, req, id, body):
        context = req.environ['nova.context']
        instance = self._get_server(context, req, id)
        try:
            self.compute_api.confirm_resize(context, instance)
        except exception.MigrationNotFound:
            msg = _(""Instance has not been resized."")
            raise exc.HTTPBadRequest(explanation=msg)
        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                    'confirm_resize')
        return exc.HTTPNoContent()
The 'return exc.HTTPNoContent()' overwrite the '@wsgi.response(202)'"
0,Bug #1219530 has been fixed and workaround for it stands in a way of putting together support for rootwrap daemon mode.
0,"In nova/utils.py, a func use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.
The code is:
def convert_version_to_int(version):
    try:
        if type(version) == str:
            version = convert_version_to_tuple(version)
       if type(version) == tuple:
           return reduce(lambda x, y: (x * 1000) + y, version)
    except Exception:
           raise exception.NovaException(message=""Hypervisor version invalid."")

this bug is fixed in glance:
https://review.openstack.org/#/c/65611/4"
0,"Interface VLAN name have a '@' character in their names when iproute2 utility list them.
But the usable interface name (for iproute2 commands) is the string before the '@' character, so this interface need a special parse.

$ ip link show
1: wlan0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
    inet 169.254.10.78/16 brd 169.254.255.255 scope link wlan0:avahi
       valid_lft forever preferred_lft forever
2: wlan0.10@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
3: vlan100@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff"
1,"The following command results in a successful vm creation:
nova boot --boot-volume <non-bootable-volume-id> --flavor <test-flavor> <test-name>
we should validate that the --boot-volume passed is actually a bootable volume and if it's not we should return an error explaining what went wrong"
1,"If get_ports is called in the Big Switch plugin without 'id' being one of the included fields, _extend_port_dict_binding will fail with the following error.
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_restproxy_plugin.py"", line 87, in test_get_ports_no_id
    context.get_admin_context(), fields=['name'])
  File ""neutron/plugins/bigswitch/plugin.py"", line 715, in get_ports
    self._extend_port_dict_binding(context, port)
  File ""neutron/plugins/bigswitch/plugin.py"", line 361, in _extend_port_dict_binding
    hostid = porttracker_db.get_port_hostid(context, port['id'])
KeyError: 'id'"
0,"test_delete_port in test_db_plugin does not test delete operation:
    def test_delete_port(self):
        with self.port() as port:
            req = self.new_show_request('port', self.fmt, port['port']['id'])
            res = req.get_response(self.api)
            self.assertEqual(res.status_int, webob.exc.HTTPNotFound.code)"
1,"Performing attach/detach interface on a VM sometimes results in an interface that can't be detached from the VM.
I could triage it to the corrupted instance cache info due to non-atomic update of that information.
Details on how to reproduce the bug are as follows. Since this is due to a race condition, the test can take quite a bit of time before it hits the bug.
Steps to reproduce:
1) Devstack with trunk with the following local.conf:
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
enable_service q-metering
RECLONE=yes
# and other options as set in the trunk's local
2) Create few networks:
$> neutron net-create testnet1
$> neutron net-create testnet2
$> neutron net-create testnet3
$> neutron subnet-create testnet1 192.168.1.0/24
$> neutron subnet-create testnet2 192.168.2.0/24
$> neutron subnet-create testnet3 192.168.3.0/24
2) Create a testvm in testnet1:
$> nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm
3) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:
--------
#! /bin/bash
c=10000
netid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`
netid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`
while [ $c -gt 0 ]
do
   echo ""Round: "" $c
   echo -n ""Attaching two interfaces... ""
   nova interface-attach --net-id $netid1 testvm
   nova interface-attach --net-id $netid2 testvm
   echo ""Done""
   echo ""Sleeping until both those show up in interfaces""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       if [ $count -eq 7 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   echo ""Waited for "" $waittime "" seconds""
   echo ""Detaching both... ""
   nova interface-list testvm | grep $netid1 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   nova interface-list testvm | grep $netid2 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   echo ""Done; check interfaces are gone in a minute.""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       echo ""line count: "" $count
       if [ $count -eq 5 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   if [ $waittime -ge 60 ]
   then
      echo ""bad case""
      exit 1
   fi
   echo ""Interfaces are gone""
   (( c-- ))
done
---------
Eventually the test will stop with a failure (""bad case"") and the interface remaining either from testnet2 or testnet3 can not be detached at all."
0,"The _test_{compute,console,network,scheduler}_api methods, found in nova/tests/{compute,console,network,scheduler}/test_rpcapi.py, all have the following line in the beginning:
    expected_retval = 'foo' if method == 'call' else None
However, the ""method"" parameter is never equal to 'call'. It is probably meant to read:
    expected_retval = 'foo' if rpc_method == 'call' else None
instead."
1,"When user without admin permission wants to get a list of servers
which are in 'deleted' state, currently it raises HTTPBadRequest.
The code is:
 231 if search_opts.get(""vm_state"") == ['deleted']:
 232 if context.is_admin:
 233 search_opts['deleted'] = True
 234 else:
 235 msg = _(""Only administrators may list deleted instances"")
 236 raise exc.HTTPBadRequest(explanation=msg)
This should be changed to HTTPForbidden exception."
1,"When an exception occurs during exception handling, it lose the
information of the first exception.
In SwiftBackupDriver.backup(), in some cases, before re-transmission
of the exception, the exception is rewritten."
0,"Currently, in neutron code, there still are some codes to enable translation tag for debug level log."
0,"The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead."
0,"Some filters are using the same logic to handle per-aggregate options. We should create helper to remove this duplicate code and help to implement new filters based on aggregates
Filters that needs to be addressed:
 * AggregateRamFilter
 * AggregateCoreFilter,
 * AggregateTypeAffinityFilter"
1,"Bug #1200271 reported a bug when using X-Copy-From together with account_quotas resulting in an over-quota.
This also affects container_quotas; additionally container_quotas doesn't check POST operations, for example from formpost middleware."
1,"BigSwitch references should be changed to ""Big Switch""."
1,"The Ironic Hostmager does not include the compute node hypervisor values such as type, version, hostname.
Including these values, which are included by the normal HostManager, is needed to allow the capabilities filter to work in a combined Ironic / virt Nova"
1,"The segment ranges for VLAN and VXLAN are being populated using an in memory dictionary. The segment allocation table is emptied on deleting any network profile.
This change allows the use of segment range from the network profile table.
By using the network profile UUID as a foreign key in the segment allocations table, tables are cleaned up only for the segments associated with
the deleted network profile via CASCADE, leaving no inconsistencies.
Add more UT along with this."
1,"commit 1ec6e18007691c92fc27235c677d11b0fe1c1f6b in tempest adds a validation for neutron port binding extension, but the check is too strict. binding:profile attribute in port binding extension is designed to allow plugin specific field but the check assumes binding:profile attribute implemented in ML2 plugin. This leads to test failures with neutron plugins which has different keys in binding:profile like NEC plugin or Mellanox plugin (though mellanox plugin does not test the related test now).
The failed test is test_list_ports_binding_ext_attr. It only affects XML.
  tempest.api.network.test_ports.PortsAdminExtendedAttrsIpV6TestXML test_list_ports_binding_ext_attr
  tempest.api.network.test_ports.PortsAdminExtendedAttrsTestXML test_list_ports_binding_ext_attr
Tempest test should allow plugin-specifc binding:profile attribute. If not, it should provide an options to disable these tests."
0,"Using the latest Nova Ironic compute drivers (either from Ironic or Nova) I'm hitting scheduling ERRORS:

Sep 08 15:26:45 localhost nova-scheduler[29761]: 2014-09-08 15:26:45.620 29761 DEBUG nova.scheduler.filters.compute_capabilities_filter [req-9e34510e-268c-40de-8433-d7b41017b54e None] extra_spec requirement 'amd64' does not match 'x86_64' _satisfies_extra_specs /opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filters/compute_capabilities_filter.py:70

I've gone ahead and patched in https://review.openstack.org/#/c/117555/.

The issue seems to be that ComputeCapabilitiesFilter does not itself canonicalize instance_types when comparing them which will breaks existing TripleO baremetal clouds using x86_64 (amd64)."
1,"When router_gateway_clear happens, the schedule_router calls the unbind_snat_servicenode in the plugin. This will clear the agent binding from the binding table. But the l3-agent was expecting the ex_gw_port binding to be present. The agent needs to check its cache of the router['gw_host_port'] value now. SNAT namespaces will not be deleted in all cases without this fix."
1,context module imported in l3_agent conflicts with argument name.
0,"We should check whether the para ""volumeId"" is in request body."
1,"RequestContext initialization failed in cinder because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call cinder api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown."
1,"When I request a v3 API request of ""GET /versions/:(id)"" occurs ""Unexpected API Error."".
--------------------
$ curl -i 'http://192.168.1.36:8774/v3/versions/123' -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: <TOKEN>""
HTTP/1.1 500 Internal Server Error
Content-Length: 193
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-baf6f903-b3f1-4c80-9e59-93f1440609a4
Date: Wed, 19 Feb 2014 07:08:13 GMT
{""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<type 'exceptions.TypeError'>"", ""code"": 500}}
--------------------
nova-api logged an error:
--------------------
2014-02-19 16:14:37.075 ERROR nova.api.openstack.extensions [req-282a97a5-8f42-4410-a8dd-ff97457d0241 admin demo] Unexpected exception in API method
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions TypeError: show() got an unexpected keyword argument 'id'
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions
--------------------"
1,"when the transport type for a network gateway connection is vlan, the neutron code does not validate that the segmentation id is between 0 and 4095.
The requests is then sent to NSX where it fails. However a 500 error is returned to the neutron API user because of the backend failure.
The operation should return a 400 and possibly not go at all to the backend."
1,"I deployed openstack with icehouse rc1 and booted 100 vms on my nodes. After my testing, i tried to delete my vms at the same time. Then i fount all of my vms` status change to deleting but cannot be deleted. I checked my openstack, the rabbitmq-server crashed . Then i restart rabbitmq-server and my openstack nova services, sended the delete requests again and again, the vms still cannot be deleted. While , in havana, the vms can be deleted if received duplicate delete requests .
I think icehouse should handle duplicate delete requests like havana ."
0,"lvm backend, with tgtadm ISCSI helper.

1. nova boot test2 --image cirros-0.3.2-x86_64-uec --flavor 42 # wait for 'active'
2. cinder create 1 # wait for 'available'
3. cinder snapshot-create <vol_id> # wait for create
4. cinder snapshot-delete <snap_id> # wait for delete
5. nova volume-attach test2 <vol_id> /dev/vdc
6. cinder list # says it is in 'attaching' status for long time (>10s)
7. cinder list # the volume in 'available' status

The cinder snapshot-delete <vol_id> causes the volume looses it (a)ctive lvm flag.

 LV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert
volume-c86347f9-3f42-4824-aee1-ae4aa33a2cf9 stack-volumes -wi------- 1.00g

This command has effect also to the original volume, even if it used on the snapshot:
$ lvchange -y -an stack-volumes/_snapshot-1b5cfb86-be8e-4ad0-89f5-ecc4e64a5f5d"
0,In review https://review.openstack.org/#/c/63100 extra logging is done when no devices match the updated security group.
1,"One of the new error conditions that shows up in the gate now that we are failing on unknown errors is a failure by c-vol to deactivate the LVM snapshot.
The c-vol error condition is the following:
[req-798a0f63-4a42-4f68-9087-54a90ec50376 1b40a47efb8243579e1830d573ac30ed f6fcf023ac3c4fa0a670e28917331b3f] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22, RESPONSE: Unable to deactivate open stack--volumes-_snapshot--8a0e5904--d394--4193--97fe--3658e0964c22 (252:4)
  Unable to deactivate logical volume ""_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22""
http://logs.openstack.org/14/62514/1/check/check-tempest-dsvm-postgres-full/18e9cf9/logs/screen-c-vol.txt.gz#_2013-12-17_02_10_46_029
It looks like it actually succeeds on attempt 2 after the quiece, so the the fix is probably to reduce the level on these to make them not error/warn unless there is a real failure."
1,"Uploading a 2Gi volume to image service resulted in volume being stuck in uploading state for a long time.
As per the logs, the image service raised an internal server error. In this case, the volume should return back to available state.
Cinder volume logs:
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-]
HTTP/1.1 500 Internal Server Error
date: Thu, 14 Aug 2014 12:26:27 GMT
content-length: 91
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-f741d884-6cdd-4304-b809-63469b12aaa8
500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
 from (pid=35027) log_http_response /opt/stack/python-glanceclient/glanceclient/common/http.py:167
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-] Request returned failure status: 500 from (pid=35027) _http_request /opt/stack/python-glanceclient/glanceclien
t/common/http.py:263
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 112, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 320, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 318, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 167, in call
    return getattr(client.images, method)(*args, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 332, in update
    'PUT', url, headers=hdrs, body=image_data)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 328, in raw_request
    return self._http_request(url, method, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 264, in _http_request
    raise exc.from_response(resp, body_str)
HTTPInternalServerError: 500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
    (HTTP 500)
Removing descriptor: 10"
1,"Proxy server generates error log for every request when cache middleware is disabled.
Here I attach a sample.
Apr 11 20:26:11 swift-proxy proxy-server: STDOUT: ERROR:root:ERROR: swift.cache could not be found in env! (txn: tx61382a02457d47d7b8085-005347d153)"
0,"While the HA property is update-able, and resulting router-get
invocations suggest that the router is HA, the migration
itself fails on the agent. This is deceiving and confusing
and should be blocked until the migration itself is fixed
in a future patch."
1,"There are a couple of places in the volume manager (initialize_connection and terminate_connection) where driver.remove_export() is called with a user context rather than an elevated context.
This appears to break the assumption used in delete_volume() where an elevated context is passed.
This causes the LVM LIO driver to fail with an error removing an export in an initialize_connection() error case:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/manager.py"", line 801, in initialize_connection
    self.driver.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/drivers/lvm.py"", line 540, in remove_export
    self.target_helper.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/iscsi.py"", line 232, in remove_export
    volume['id'])
  File ""/usr/lib/python2.7/site-packages/cinder/db/api.py"", line 232, in volume_get_iscsi_target_num
    return IMPL.volume_get_iscsi_target_num(context, volume_id)
  File ""/usr/lib/python2.7/site-packages/cinder/db/sqlalchemy/api.py"", line 116, in wrapper
    raise exception.AdminRequired()
AdminRequired: User does not have admin privileges"
0,"https://github.com/openstack/neutron/blob/14cb886809e5cccbf799a0dc2e5b99f31b1ab3be/neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py#L577
has:
mock.patch.object(self.agent.tun_br, 'setup_tunnel_port')
but setup_tunnel_port is an attribute of self.agent and not self.agent.tun_br."
1,"I created a flavor with quotas on the network bandwidth and created a VM with this flavor: upload limited 1 MB/sec, download limited to 500 kB/sec. The limits are ignored: copy from DevStack to the VM is faster than 13 MB/sec.
I'm using Neutron with Open vSwitch (OVS) for the network.
It looks like a regression in nova/virt/libvirt/vif.py. According to a colleague, it was maybe introduced when the OVS, LinuxBridge and HyperV classes were merged into one LibvirtGenericVIFDriver class. designer.set_vif_bandwidth_config() is only called for bridge types:
- get_config_ovs_hybrid()
- get_config_ivs_hybrid()
- network_model.VIF_TYPE_BRIDGE
Command to create the flavor and create a VM with this flavor:
---
nova flavor-create --ephemeral=1 victor_test_vif 50 256 1 1
nova flavor-key victor_test_vif set quota:vif_inbound_average=1000
nova flavor-key victor_test_vif set quota:vif_inbound_peak=1000
nova flavor-key victor_test_vif set quota:vif_outbound_peak=500
nova flavor-key victor_test_vif set quota:vif_outbound_average=500
nova boot --flavor=victor_test_vif --image=cirros-0.3.1-x86_64-uec victor_test
---
Command to start a small and fast TCP server on DevStack, uploading a file of 10 MB:
---
ip netns
# copy the qrouter-xxx name
sudo ip netns exec qrouter-128db593-a0db-40c3-84c7-e6383d40c75f bash
# following commands are executed in the qrouter namespace to reach the VM network
dd if=/dev/urandom of=random10MB bs=1024 count=10240
nc -l 0.0.0.0 12345 < random
---
Command to download the file on the VM:
---
time nc 10.0.0.1 12345 > /dev/null
---
Current result: timing smaller than 1 second (faster than 10 MB/sec)
Expected result: timing higher than 10 second (1 MB/sec or slower)
The problem is that the <bandwidth> tag is not generated in the libvirt.xml file of the VM.
I will provide a patch."
0,"Typos in few files like
1. nova/availability_zones.py +100
Return available and unavailable zones on demands -> Return available and unavailable zones on demand.
2. nova/exception.py +578
Either Network uuid %(network_uuid)s is not present -> Either network uuid %(network_uuid)s is not present
3. nova/exception.py +944
Instance Type %(instance_type_id)s has no extra specs with -> Instance type %(instance_type_id)s has no extra specs with
4. novaclient/base.py +108, 122
typicaly -> typically
And few other"
1,"after I create an vpnservice without a name, and then I want to update it with a name, the following msg come out:
2013-12-02 12:09:18.691 3623 ERROR neutron.api.v2.resource [req-aab03b48-08a4-415f-9cd1-7ee4f5a16b19 8fb8b278e48e4d0cbde162e5626032a1 c3b9072bc0f741aa98f72e794dba7ea6] update failed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource File ""/mnt/data/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource File ""/mnt/data/opt/stack/neutron/neutron/api/v2/base.py"", line 492, in update
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource obj = obj_updater(request.context, id, **kwargs)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 579, in update_vpnservice
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource self.assert_update_allowed(vpns_db)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 198, in assert_update_allowed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource raise vpnaas.VPNStateInvalid(id=id, state=status)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource VPNStateInvalid: Invalid state PENDING_CREATE of vpnaas resource <built-in function id>
There are two problems in above msg:
1. built-in function id
2. the msg itself: I think we can use: the vpn service cannot be updated when it is in state PENDING_CREATE
but I think we should be able to modify the vpn service when it is not used by IPsecSiteConnection"
1,"When call create volume api with image id """", it will create an empty volume.
But when create volume with snapshot id """" or srv volume """", it will return an 404 error.
So we should create an empty volume when call create volume api with snapshot id """" or srv volume """"."
0,"https://review.openstack.org/#/c/27818/ introduces thee spot of assertEquals() which is deprecated
see https://review.openstack.org/#/c/27818/8/neutron/tests/unit/openvswitch/test_ovs_db.py"
1,"We can not stop router forwarding packets by admin_state_up false.
Master branch has this problem (stable/grizzly branch don't have the problem).
I know the cause. When run router-update --admin_state_up false, transitions as follows:
sync_routers(l3_rpc_base)鈫抣ist_active_sync_routers_on_active_l3_agent(agent schedulers_db)鈫抔et_sync_data(l3_db)鈫抇get_sync_routers(l3_db)
list_active_sync_routers_on_active_l3_agent pass key 'active=True', and router that set admin_state_up false is ignored by filters['admin_state_up'] = [active] in _get_sync_routers.
I think that active=True is not identical admin_state_up True and that is more similar status Active.
I will modify here."
1,"I would like to use ""glance-replicator livecopy"" command to replicate the images from one openstack instance to anthor, it goes to error while running the livecopy command:
2014-09-24 17:32:09.082 24953 CRITICAL glance [-] ServerErrorException: 400 Bad Request
The server could not comply with the request since it is either malformed or otherwise incorrect.
 Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance Traceback (most recent call last):
2014-09-24 17:32:09.082 24953 TRACE glance File ""/usr/bin/glance-replicator"", line 10, in <module>
2014-09-24 17:32:09.082 24953 TRACE glance sys.exit(main())
2014-09-24 17:32:09.082 24953 TRACE glance File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 739, in main
2014-09-24 17:32:09.082 24953 TRACE glance command(options, args)
2014-09-24 17:32:09.082 24953 TRACE glance File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 516, in replication_livecopy
2014-09-24 17:32:09.082 24953 TRACE glance image_response)
2014-09-24 17:32:09.082 24953 TRACE glance File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 238, in add_image
2014-09-24 17:32:09.082 24953 TRACE glance response = self._http_request('POST', url, headers, image_data)
2014-09-24 17:32:09.082 24953 TRACE glance File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 125, in _http_request
2014-09-24 17:32:09.082 24953 TRACE glance raise ServerErrorException(response.read())
2014-09-24 17:32:09.082 24953 TRACE glance ServerErrorException: 400 Bad Request
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance The server could not comply with the request since it is either malformed or otherwise incorrect.
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance"
1,"During testing, we've found some instances of leftover router ports. The ports are not properly cleaned up when a router is removed. The user is able to manually remove the ports to work around the issue."
1,"when trying to delete a subnet, sometimes the following error comes out.
Icehouse, Database in use is DB2, but I guess it might happen for other databases too.
================================
2013-12-04 03:49:48.275 26604 TRACE neutron.plugins.ml2.plugin
2013-12-04 03:49:48.277 26604 ERROR neutron.api.v2.resource [req-e8e78c50-25b0-4e19-b5f0-796041d7b464 f53f4f5b40154ad6b1ec1ac08f88ecf2 b93ff0
8b44da407185a26033768101f5] NT-C3C9C57 delete failed
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource result = method(request=request, **args)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource obj_deleter(request.context, id, **kwargs)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 443
, in delete_network
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource self.delete_subnet(context, subnet.id)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 530, in delete_subnet
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource break
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449,in __exit__
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource self.commit()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource self._prepare_impl()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340,in _prepare_impl
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource self.session.flush()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 545, in _wrap
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource raise exception.DBError(e)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource DBError: (Error) ibm_db_dbi::Error:
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Error 1: [IBM][CLI Driver][DB2/LINUXX8664] SQL0100W No row was found for FETCH,UPDATE or DELETE; or the result of a query is an empty table. SQLSTATE=02000 SQLCODE=100
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource 'DELETE FROM ipavailabilityranges WHERE ipavailabilityranges.allocation_pool_id= ? AND ipavailabilityranges.first_ip = ? AND ipavailabilityranges.last_ip = ?' (('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.2', '10.100.0.2'), ('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.4', '10.100.0.14'))"
1,"The DELETE verb applies to a single resource, and doesn't define any semantics for the body.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.7
The swift Bulk Delete command affects multiple resources specified in a DELETE body.
http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.bulk
While Bulk Delete is a welcome operation, its usage of DELETE is unusual: affecting multiple resources and relying on reading content.
More typically, such an operation employs POST (or PUT), which folks including api-craft usually agree is the best ""catch-all"" verb for behaviors such as those affecting multiple resources. That's the TL;DR; of the thread below.
https://groups.google.com/forum/#!searchin/api-craft/Regarding$20Bulk$20actions/api-craft/wY-W1NdZDRs/7YDwMhCR608J
Note that this topic isn't nasal or abstract. The current behavior is unsupported using the built-in java http client. Even if third-party libraries can work around this behavior, it is probably best to not be a snowflake wrt http verb semantics where possible!
http://stackoverflow.com/questions/9100776/http-delete-with-request-body-issues"
1,Datatsore regex support is missing for the ESX driver
0,"HTTPSClientAuthConnection uses httplib.HTTPSConnection objects. In Python 2.x those do not perform CA checks so client connections are vulnerable to MiM attacks.

This should be changed to use the requests lib."
0,"""Migrate a volume to the specified host."" as the description of migrate_volume_completion func is inappropriate."
1,"midonet ""delete_dhcp"" function doesn't check subnet length when searching for the DHCP entries to delete. This means it can delete the wrong subnet entry if two are nested (ie: have the same prefix, but different lengths).
Disclaimer: I don't have midonet equipment and don't know if it is even capable of supporting nested DHCP subnets - I just found this while wondering why 'net_len' variable was unused."
1,"VCNS related error should VcnsBadRequest other than BadRequest.
When creating a firewall without policy, it would report the following error:
Traceback (most recent call last):
  File ""neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""neutron/api/v2/base.py"", line 411, in create
    obj = obj_creator(request.context, **kwargs)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 893, in create_firewall
    self._vcns_update_firewall(context, fw, router_id)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 842, in _vcns_update_firewall
    self.vcns_driver.update_firewall(context, edge_id, fw_with_rules)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 231, in update_firewall
    fw_req = self._convert_firewall(context, firewall)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 131, in _convert_firewall
    for rule in firewall['firewall_rule_list']:
TypeError: 'NoneType' object is not iterable
}}}"
1,"Steps to reproduce:
nova resize <UUID> 2
Error:
 ERROR nova.openstack.common.rpc.amqp [req-762f3a87-7642-4bd3-a531-2bcc095ec4a5 demo demo] Exception during message handling
  Traceback (most recent call last):
    File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
      **args)
    File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
      result = getattr(proxyobj, method)(ctxt, **kwargs)
    File ""/opt/stack/nova/nova/exception.py"", line 99, in wrapped
      temp_level, payload)
    File ""/opt/stack/nova/nova/exception.py"", line 76, in wrapped
      return f(self, context, *args, **kw)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 218, in decorated_function
      pass
    File ""/opt/stack/nova/nova/compute/manager.py"", line 204, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 269, in decorated_function
      function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 246, in decorated_function
      e, sys.exc_info())
    File ""/opt/stack/nova/nova/compute/manager.py"", line 233, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 2633, in resize_instance
      block_device_info)
    File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 410, in migrate_disk_and_power_off
      dest, instance_type)
    File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 893, in migrate_disk_and_power_off
      raise exception.HostNotFound(host=dest)
  HostNotFound:"
1,"It's similar to https://bugs.launchpad.net/neutron/+bug/1251422, but wrt routers.
If we delete a router from neutron that is already deleted in nvp, it throw 404 error. The correct behavior should be to delete it from neutron, if it's already deleted in nvp.
rainbow:~ bhuvan$ neutron router-interface-delete tempest-router 67056b2d-a924-4456-9050-ed0baa0eaf1a
404-{u'NeutronError': {u'message': u'Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on subnet 67056b2d-a924-4456-9050-ed0baa0eaf1a', u'type': u'RouterInterfaceNotFoundForS
ubnet', u'detail': u''}}
neutron server log. Note: 404 error from nvp is logged at INFO level. It should be a WARNING.
2014-03-12 22:42:26,149 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest-admin with project_id : csi-tenant-tempest and ro
les: csi-tenant-admin,csi-role-admin
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Matched PUT /routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Route path: '/routers/:(id)/remove_router_interface.:(format)', defaults: {'action': u'remove_router_interface', 'c
ontroller': <wsgify at 68316752 wrapping <function resource at 0x410fb18>>}
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'remove_router_interface', 'controller': <wsgify at 68316752 wrapping <function resource at
 0x410fb18>>, 'id': u'd6f3c0c6-6884-467f-9a84-5a64b88f8936', 'format': u'json'}
2014-03-12 22:42:26,208 (neutron.api.v2.resource): INFO resource resource remove_router_interface failed (client error): Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on s
ubnet 67056b2d-a924-4456-9050-ed0baa0eaf1a
2014-03-12 22:42:26,210 (neutron.wsgi): INFO log write 17.199.81.86 - - [12/Mar/2014 22:42:26] ""PUT /v2.0/routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json HTTP/1
.1"" 404 329 0.066915"
0,"ML2 statically sets __native_bulk_support to True. However, some drivers do not support bulk operations so there should be a way to turn off bulk support when using those drivers."
0,"Seeing this in the gate-glance-python27 jobs - 8 hits in the last 7 days.
2014-01-30 01:29:36.436 | ======================================================================
2014-01-30 01:29:36.436 | FAIL: glance.tests.unit.v2.test_tasks_resource.TestTasksController.test_index_with_sort_key
2014-01-30 01:29:36.436 | ----------------------------------------------------------------------
2014-01-30 01:29:36.436 | _StringException: Traceback (most recent call last):
2014-01-30 01:29:36.436 | File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_tasks_resource.py"", line 233, in test_index_with_sort_key
2014-01-30 01:29:36.436 | self.assertEqual(UUID4, actual[0])
2014-01-30 01:29:36.437 | File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
2014-01-30 01:29:36.437 | self.assertThat(observed, matcher, message)
2014-01-30 01:29:36.437 | File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-01-30 01:29:36.437 | raise mismatch_error
2014-01-30 01:29:36.437 | MismatchError: !=:
2014-01-30 01:29:36.437 | reference = '6bbe7cc2-eae7-4c0f-b50d-a7160b0c6a86'
2014-01-30 01:29:36.437 | actual = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'
2014-01-30 01:29:36.437 |
Logstash URL:
http://logstash.openstack.org/#eyJzZWFyY2giOiJcImluIHRlc3RfaW5kZXhfd2l0aF9zb3J0X2tleVwiIEFORCBcInRlc3RfdGFza3NfcmVzb3VyY2UucHlcIiBBTkQgZmlsZW5hbWU6XCJjb25zb2xlLmh0bWxcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM5MTA0NzE4ODMyNywibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==
Logstash Query:
message:""in test_index_with_sort_key"" AND message:""test_tasks_resource.py"" AND filename:""console.html"""
1,"OpenStack allows an instance to be resized in either a SHUTOFF or ACTIVE state. However, the resize behavior between the states is not consistent. In particular, resize of an ACTIVE instance will result in the instance's status to be mapped to RESIZE while a resize task is in progress. The same is not true of SHUTOFF
Before resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1 | SHUTOFF | None | Shutdown | VLAN164=10.164.0.14 |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1 | ACTIVE | None | Running | VLAN164=10.164.0.16 |
During resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1 | SHUTOFF | resize_finish | Shutdown | VLAN164=10.164.0.14 |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1 | RESIZE | resize_prep | Running | VLAN164=10.164.0.16 |
After resize and confirmation:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1 | SHUTOFF | None | Shutdown | VLAN164=10.164.0.14 |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1 | ACTIVE | None | Running | VLAN164=10.164.0.16 |"
1,"The PCI filter for scheduling runs on the basis of pci stats pools available in a compute node. If the PCI requests match one or more pools, the number of devices will be subtracted from the total number of devices available in those pools.
On the compute node, PCI device allocation is performed based on the list of free devices that are available on the node. The PCI requests are used to match against the device itself, rather than the pci stats pools.
The unsymmetrical handling of scheduling versus allocating could cause incorrect pci stats, and thus incorrect scheduling, and nova instances failed to boot."
0,VMware ESX driver was deprecated in J.This console class will break.
1,Primary key cannot be null therefore setting NULL on column that is primary key is wrong and then generates wrong migration script using autogenerate.
0,"There are several jobs that only run if you have sqlite configured in nova.tests.db.test_migration_utils but there are two which will run and fail if you don't have sqlite configured:
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_check_shadow_table_with_unsupported_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 391, in test_check_shadow_table_with_unsupported_type
    engine = self.engines['sqlite']
KeyError: 'sqlite'
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_util_drop_unique_constraint_with_not_supported_sqlite_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 215, in test_util_drop_unique_constraint_with_not_supported_sqlite_type
    for i in xrange(0, len(values)):
UnboundLocalError: local variable 'values' referenced before assignment
Those should be skipped if sqlite isn't configured like the other sqlite-specific tests."
0,"db_plugin.delete_ports() can lead to long transaction if plugin.delete_port talks with external system.
it is observed first in nec plugin (bug 1282922), but it affects multiple plugins/drivers.

Note that it is about delete_ports and not about delete_port.

The detail is described in bug 1282922. Quoted from the original bug report.
----
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.

After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout."
1,"On a fresh installation, without spinning up any instance, if one boots a number of instances via nova there are a number of default security groups created for the admin user:
mysql> select * from securitygroups;
+----------------------------------+--------------------------------------+---------+-------------+
| tenant_id | id | name | description |
+----------------------------------+--------------------------------------+---------+-------------+
| 67e87d2ac4944c3eb93283637b6e4000 | 0fe7f583-7d9e-4be0-9ade-6a4ee9a07daa | default | default |
| 67e87d2ac4944c3eb93283637b6e4000 | 1a237a3c-39c3-4a92-bd7b-55a3bc5ef7e9 | default | default |
| 67e87d2ac4944c3eb93283637b6e4000 | 51c83ff8-97f9-426d-836e-10bf631219e4 | default | default |
| 67e87d2ac4944c3eb93283637b6e4000 | 8882d943-4309-410c-bd56-981da8d5d095 | default | default |
| 67e87d2ac4944c3eb93283637b6e4000 | 893b2462-800a-4936-9b35-fc47822237e9 | default | default |
| d639ee58de4f4db7b29f30a7b142a047 | baf7ac95-efaf-427d-8fd9-f8210d23cab1 | default | default |
+----------------------------------+--------------------------------------+---------+-------------+
This should never happen as the admin user is not even spinning up the vm's...."
1,"Wrong arguments orders in super call of cinder.brick.initiator.connector.RemoteFSConnector

class InitiatorConnector(executor.Executor):
    def __init__(self, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        super(InitiatorConnector, self).__init__(root_helper, execute,
                                                 *args, **kwargs)
        if not driver:
            driver = host_driver.HostDriver()
        self.set_driver(driver)

class RemoteFsConnector(InitiatorConnector):
    """"""Connector class to attach/detach NFS and GlusterFS volumes.""""""

    def __init__(self, mount_type, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        self._remotefsclient = remotefs.RemoteFsClient(mount_type,
                                                       execute, root_helper)
        super(RemoteFsConnector, self).__init__(driver, execute, root_helper,
                                                *args, **kwargs)

driver and execute arguments should be after root_helper"
1,"when configuring ISER ( at /etc/cinder/cinder.conf: volume_driver=cinder.volume.drivers.lvm.LVMISERDriver)
volumes cannot be attached.
this happens also in Icehouse (stable).
i will add more logs soon."
1,"See: http://logs.openstack.org/69/49169/2/check/check-tempest-devstack-vm-full/473539f/console.html
2013-10-07 18:57:09.859 | ======================================================================
2013-10-07 18:57:09.859 | FAIL: tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | ----------------------------------------------------------------------
2013-10-07 18:57:09.860 | _StringException: Empty attachments:
2013-10-07 18:57:09.861 | stderr
2013-10-07 18:57:09.861 | stdout
2013-10-07 18:57:09.861 |
2013-10-07 18:57:09.862 | pythonlogging:'': {{{
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,059 Request: GET http://127.0.0.1:8774/v2/3b02395d4eec44958ffcc10ac2673fd2/servers?changes-since=2013-10-07T18%3A38%3A31.034080
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,490 Response Status: 200
2013-10-07 18:57:09.863 | 2013-10-07 18:38:33,490 Nova request id: req-906f2cf2-6508-4cd2-bf62-3595b18d223a
2013-10-07 18:57:09.863 | }}}
2013-10-07 18:57:09.863 |
2013-10-07 18:57:09.863 | Traceback (most recent call last):
2013-10-07 18:57:09.864 | File ""tempest/api/compute/servers/test_list_servers_negative.py"", line 191, in test_list_servers_by_changes_since
2013-10-07 18:57:09.864 | self.assertEqual(num_expected, len(body['servers']))
2013-10-07 18:57:09.864 | File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-07 18:57:09.865 | self.assertThat(observed, matcher, message)
2013-10-07 18:57:09.865 | File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-07 18:57:09.865 | raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-07 18:57:09.865 | MismatchError: 3 != 2"
0,"Use model_query() instead of session.query in db.volume_destroy, db.volume_type_destroy, db.transfer_destroy and db.snapshot_destroy."
1,"During my OVS-related prototyping I spotted this. For bug verification I used 'ovs_lib' as a standalone python package (without Openstack runnig).
OVS info:
    ovs-ofctl (Open vSwitch) 1.4.0+build0
    Compiled Feb 18 2013 13:13:22
    OpenFlow versions 0x1:0x1
I can define flow with no problems:
    from neutron.agent.linux import ovs_lib
    int_br = ovs_lib.OVSBridge('br-int', root_helper='sudo')
    int_br.add_flow(priority=777,dl_type=2048, actions='normal')
Run 'ovs-ofctl dump-flows br-int'
    'cookie=0x0, duration=115.956s, table=0, n_packets=0, n_bytes=0, priority=777,ip actions=NORMAL'
However when I try to modify this flow:
    int_br.mod_flow(dl_type=2048, actions='drop')
I see
    'Command: ['sudo', 'ovs-ofctl', 'mod-flows', 'br-int', 'hard_timeout=0,idle_timeout=0,priority=0,dl_type=2048,actions=drop']
    Exit code: 1
    Stdout: ''
    Stderr: 'ovs-ofctl: unknown keyword hard_timeout\n'"
0,"mysql> select memory_mb,memory_mb_used,free_ram_mb,free_disk_gb from compute_nodes where free_ram_mb > 8000;
+-----------+----------------+-------------+--------------+
| memory_mb | memory_mb_used | free_ram_mb | free_disk_gb |
+-----------+----------------+-------------+--------------+
| 131026 | 121640 | 9386 | 810 |
| 131026 | 121621 | 9405 | 990 |
| 131026 | 121636 | 9390 | 790 |
+-----------+----------------+-------------+--------------+
3 rows in set (0.00 sec)
which indicates three hosts that can handle an 8GB instance, but the capacity reported in the cell log shows:
capacities: {'ram_free': {'units_by_mb': {'8192': 6,
What's happening is that two flavors have memory_mb set to 8192 so the slots are being counted twice."
0,"A previous refactor [1] of SolarisISCSIDriver and SanDriver renamed
the `_execute` function to `san_execute`. However, SolarisISCSIDriver
still called the _execute function, which no longer exists in it's
parent classes."
1,"When call create volume api with image id """", it will create an empty
volume.
But when create volume with snapshot id """" or srv volume """", it will
return an 404 error.
So it should create volume fail when call create volume api with image id
""""."
1,"For some reason, nova.volume.cinder.API derives from nova.db.base.Base, which looks like this (in its entirety):
class Base(object):
    """"""DB driver is injected in the init method.""""""
    def __init__(self, db_driver=None):
        super(Base, self).__init__()
        if not db_driver:
            db_driver = CONF.db_driver
        self.db = importutils.import_module(db_driver) # pylint: disable=C0103
I checked and nova.volume.cinder.API makes no reference at all to self.db, therefore unless I am mistaken, there's no reason for this inheritance."
1,"A datastore can be in maintenance mode. The driver does not ignore it both in stats update and while spawing instances.

During stats update, a wrong stats update is returned if a datastore is in maintenance mode.

Also during spawing, if a datastore in maintenance mode gets choosen, since it had the largest disk space, the spawn would fail.

The driver should ignore datastore in maintenance mode"
0,"The Open vSwitch and Linuxbridge plugins are being removed from the Neutron tree in Juno.
See https://bugs.launchpad.net/neutron/+bug/1323729

The Cisco Nexus monolithic plugin does not work without the Open vSwitch plugin, so it also needs to be removed from the tree. The Cisco monolithic plugin contains code for both the Nexus hardware switches and the N1KV virtual switch. The N1KV code will remain in the tree (it does not depend on the OVS plugin).

Note: the Cisco Nexus is now supported in Neutron via the Nexus mechanism driver in the ML2 plugin."
0,"The current ironic states file, nova/virt/ironic/ironic_states.py, is out-of-date, and was recently updated in ironic with this change:

https://review.openstack.org/118467

Ideally, we should keep these in sync to prevent confusion."
1,"During compute manager startup init_host is called. One of the functions there is to delete instance data that doesn't belong to this host i.e. _destroy_evacuated_instances. But this function only checks if the local instance belongs to the host or not. It doesn't check the task_state or vm_state.
If at this time a resize migration is taking place and the destination compute manager is restarted it might destroy the resizing instance. Alternatively, if the resize has completed (vm_state = RESIZED) but has not been confirmed/reverted, then a restart of the source compute manager might destroy the original instance.
A similar bug concerning just the migrating state is outlined here: https://bugs.launchpad.net/nova/+bug/1319797 and a fix is proposed here: https://review.openstack.org/#/c/93903
It was intended to have that fix deal with resize migrating instances as well as those just in the migrating state but as pointed out in a review comment this solution will work for migrating but a fix for resize would require further changes so I have raised this bug to highlight that."
1,"If two or more compute hosts have instances which are
sharing a given VLAN on a Nexus switch, and then
all instances on one of the hosts which are using that
VLAN are terminated, while instances which are using
that VLAN on other hosts remain active, then
the VLAN is not being untrunked from the
corresponding interface on the Nexus switch as
expected.
Note that the VLAN is correctly untrunked from
the Nexus interface when the instance being
terminated is the last instance which is using that
VLAN on the Nexus switch.
The correct logic should be:
If this the last instance using this VLAN on this switch interface:
____untrunk the vlan from the switch interface
____If this the last instance using this VLAN on this switch:
_________delete the VLAN from the switch
Note that this bug also exists in the Cisco ML2
mechanism driver, but the code which implements
this is being redesigned, so it will be addressed for
the ML2 separately."
0,"""Returns detailed information for all public, available images"" is inappropriate on detail, because the private image can be returned also.
""Returns detailed information for all available images"" is better."
1,"If an instance's vifs cannot be deleted (because, for example, they were never plugged in the first place), then compute manager will fail trying to delete the instance:
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/compute/manager.py"", line 2154, in _shutdown_instance
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher block_device_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 956, in destroy
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher destroy_disks)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 992, in cleanup
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher self.unplug_vifs(instance, network_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 864, in unplug_vifs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher self.vif_driver.unplug(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher self.unplug_ovs(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher self.unplug_ovs_hybrid(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher v2_name)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/network/linux_net.py"", line 1317, in delete_ovs_vif_port
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher _ovs_vsctl(['del-port', bridge, dev])
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher raise exception.AgentError(method=full_args)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo81ce661d-1a']"
1,The quota flags in l3 extensions [1] are not defined in the configuration file sample [2].
1,"Performance issue with brcd_fc_zone_client_cli.py
Every ssh command creates a new switch connection login and teardown which needs to be optimized
The more fiber channel zones the longer it takes, we added logging to capture this.
Add logging which tracks zoning on attach cinder/volume/manager.py
           vol_type = conn_info.get('driver_volume_type', None)
           if (vol_type == 'fibre_channel'):
               import uuid
               myid = uuid.uuid4()
               LOG.warn(""START ZONING!!!! %s"" % myid)
               self._add_or_delete_fc_connection(conn_info)
               LOG.warn(""END ZONING!!!! %s"" % myid)
           return conn_info
21 seconds to zone Brocade switch with no zones on the switch
2014-02-07 15:24:08.334 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING!!!!
<mdenny> 2014-02-07 15:24:29.391 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
48 seconds to zone Brocade switch with 230 zones on the switch
<mdenny> 2014-02-08 15:22:05.219 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING !!!!
<mdenny> 2014-02-08 15:22:53.258 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
This occurred on FOS v6.4.3 and v7.0.2
Re-factor connection by creating one login connection with locking and performing all or a group of ssh commands might be a solution."
0, for example because nova's network interfaces got renamed after a reboot
0,"************* Module destroy_cached_images
W: 31, 0: Unused import logging (unused-import)"
1,"If an instance is deleted while a resize operation is in progress (i.e. before the VM state is RESIZED), the temporary files created during the resize operation (e.g. <instance_id>_resize with libvirt) are not cleaned up. This would seem to be related to bug 1285000, except in that case the temporary files are on a different node. Ideally a fix would address both."
1,"Compute instance.update messages that are not triggered by a state change (e.g. setting the host in the resource tracker) have default (None) values for task_state, old_vm_state and old_ task_state.

This can make the instance state sequence look wrong to anything consuming the messages (e.g stacktach)

 compute.instance.update None(None) -> Building(none)
 scheduler.run_instance.scheduled
 compute.instance.update building(None) -> building(scheduling)
 compute.instance.create.start
 compute.instance.update building(None) -> building(None)
 compute.instance.update building(None) -> building(networking)
 compute.instance.update building(networking) -> building(block_device_mapping)"
0,"The test ""test_get_dss_rp"" in test_vmware_vmdk.py is not testing the positive case as intended. Rather, it tests the negative case (no datastores) which is already covered by ""test_get_dss_rp_without_datastores""."
0,"    plugin = LinuxBridgeNeutronAgentRPC(interface_mappings, ------> agent =
                                        polling_interval,
                                        root_helper)
    LOG.info(_(""Agent initialized successfully, now running... ""))
    plugin.daemon_loop() -----> agent.daemon_loop()"
1,"The behaviour to manage naming conflicts is different between aggregate creation and aggregate update.
Aggregate create doesn't let you create 2 aggregates with the same name.
Aggregate update lets you update an aggregate to a name that already exists.
It seems to me it should be consistent, and probably both check for conflict.
Here's an example, using a recent devstack:
$ nova aggregate-create test
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 14 | test | - | | |
+----+------+-------------------+-------+----------+
$ nova aggregate-create test
ERROR: There was a conflict when trying to complete your request. (HTTP 409) (Request-ID: req-6711e05e-4efc-4a2d-9117-52d034c74a4f)
$ nova aggregate-create test2
+----+-------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+-------+-------------------+-------+----------+
| 15 | test2 | - | | |
+----+-------+-------------------+-------+----------+
$ nova aggregate-update 15 test
Aggregate 15 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 15 | test | - | | |
+----+------+-------------------+-------+----------+
$ nova aggregate-list
+----+--------------------+-------------------+
| Id | Name | Availability Zone |
+----+--------------------+-------------------+
| 14 | test | - |
| 15 | test | - |
+----+--------------------+-------------------+
Nova api logs from when the aggregate creation fails as expected:
2014-04-05 14:45:34.865 INFO nova.api.openstack.compute.contrib.aggregates [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Aggregate test already exists.
2014-04-05 14:45:34.865 INFO nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] HTTP exception thrown: There was a conflict when trying to complete your request.
2014-04-05 14:45:34.865 DEBUG nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Returning 409 to user: There was a conflict when trying to complete your request. from (pid=1517) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1223"
0,"Several service plugins are inheritig from CommonDBMixin which has a few utulity methods.

Right now CommonDBMixin resides in db_base_plugin_v2.py so those plugins require to import it.
In some cases it is undesirable and can lead to cycles in imports,
so CommonDBMixin needs to be extracted into a different module"
0,"Error importing module nova.openstack.common.sslutils: duplicate option: ca_file
This is seen in the nova gate - for unrelated patches - it might be a bad slave I guess, or it might be happening to all subsequent patches, or it might be a WTF.
http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkVycm9yIGltcG9ydGluZyBtb2R1bGUgbm92YS5vcGVuc3RhY2suY29tbW9uLnNzbHV0aWxzOiBkdXBsaWNhdGUgb3B0aW9uOiBjYV9maWxlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTM5MTUyNTE4ODl9 suggest it has only happened once so far.
commit 5188052937219badaa692f67d9f98623c15d1de2
Merge: af626d0 88b7380
Author: Jenkins <email address hidden>
Date: Tue Mar 4 02:47:02 2014 +0000
    Merge ""Sync latest config file generator from oslo-incubator""
Was the latest merge prior to this, but it may be coincidental."
0,"Replace assertEqual(None, *) with assertIsNone in tests to have
more clear messages in case of failure."
1,"Current code intends to disallow assigning a fixed ip to a port when that ip
overlaps with one of the addresses in the allowed-address-pairs list. However, it is an unnecessary check and also the current code does not enforce it in all cases.
Cases where it enforces:
1) A port-update with allowed-address-pairs list containing an IP address which is *exactly* same as one of the fixed-ips on the port. For example, for a port with fixed-ip of 10.10.8.6, the following fails:
$> neutron port-update 58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
2) A port-update with a fixed-ip which is exactly same as one of the allowed
IP addresses in the allowed-address-pairs list. For example, for a port
with allowed-address-pairs with """", the following fails:
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6"", ""mac_address"": ""fa:16:3e:7f:3c:06""} |
$> neutron port-update 58062310-ee5a-4b14-b554-5df699064bc9 -- --fixed-ips type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
However, allowed-address-pairs can work with IP CIDRs and the overlap check
is *not* properly enforced when IP addresses are specified in the CIDR notation.
Case where the current code is incomplete:
Same as case (1) above but IP address specified in cidr notation. In this case, the code does not check for overlaps.
$> neutron port-update 58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6/32
Updated port: 58062310-ee5a-4b14-b554-5df699064bc9
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6/32"", ""mac_address"": ""fa:16:3e:7f:3c:06""} |
Functionally, it is incorrect to allow overlaps in one type of inputs and not allow in other types of input.
On the other hand, if we fix this bug and check overlaps in all cases, then the API will become hard to use. For example, if a fixed IP 10.10.1.1 exists on a port and we want to allow addresses in 10.10.1.0/24 cidr on that port, then one has to configure a list of 8 cidrs ([10.10.1.0/32, 10.10.1.2/31,
10.10.1.4/30, ..., 10.10.1.128/25]) on the allowed-address-pairs.
In any case, this is an unnecessary check as the overlap does not have any negative effect. Allowed-address-pairs is ADDING on to what is allowed because of the fixed IPs. So, there is no possibility of conflict. The check will probably make sense if we are maintaining denied addresses (instead of allowed addresses).
My suggestion is to remove this check entirely.
https://review.openstack.org/#/c/94508/"
1,"report_interval is how often an agent sends out a heartbeat to the service. The Neutron service responds to these 'report_state' RPC messages by updating the agent's heartbeat DB record. The last heartbeat is then compared to the configured agent_down_time to determine if the agent is up or down. The agent's status is used when scheduling networks on DHCP and L3 agents.
The defaults are 4 seconds for report_interval and 9 for agent_down_time.
On a setup with 18 agents (15 layer 2, L3, DHCP, metadata) sitting on 16 nodes, and a Neutron service sitting on a dedicated powerful machine, the service was idle with 20% CPU usage. Changing the report_interval to 28 seconds and agent_down_time to 60 seconds changed the CPU usage to 1%, and allowed bulk operations on a larger scale. (In this case: Creating 30 instances at the same time with 60 ports). With the original values the operation failed (The instances did not get IP addresses), and with the new values we were able to boot 60 instances successfully. Side note: This flow will work better once the Nova-Neutron race is resolved, but that's orthogonal to this proposal."
0,"Earlier Cisco was using the list events api to poll policies from VSM.
It was inefficient and caused delay in processing.
So, now Cisco switched to list profiles to poll policies from VSM."
1,"Steps:
Using the MidoNet plugin:
* Create a private network
* Create a VM on the private network
* Create a router
* Create an interface for the router on the private network
* Create an external network
* Set a router's gateway to the external network
* Ping a publicly routable address from a VM on a network attached the router
Observed:
Source NAT is not applied to the outbound traffic
Expected:
Outbound traffic should be source NATted at the router"
0," no other sync attempts will be made because the server manager clears the hash from the DB before the sync operation. It shouldn't do this because the backend ignores the hash on a sync anyway."""
1,"The nova/compute/manager.py - pre_live_migration method is checking to ensure that there is at least a single fixed IP Address on the instance. The comment above it indicates that this shouldn't be required (and it is not clear that it is even required). Live migrations are blocked for systems that do not have a fixed IP Address.
If a system does not have a fixed IP, then the following error is thrown:
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp event_type, level, payload)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 76, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp return f(self, context, *args, **kw)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3850, in pre_live_migration
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp instance_uuid=instance['uuid'])
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp FixedIpNotFoundForInstance: Instance eb586597-3565-4283-88d3-d0664f4d5747 has zero fixed ips."
1,"When creating a subnet in the Midonet plugin, the dns name servers and host routes are not set."
0,"The following migration 50e86cb2637a called
op.rename_table('neutron_nvp_port_mapping', 'neutron_nsx_port_mappings')
though the table name was actually quantum_nvp_port_mapping. Because of this
the quantum_id->nvp_id mapping was never migrated over to the new table and
you would be left with a quantum_nvp_port_mapping table hanging around.
In addition, the downgrade would rename the table to neutron_nvp_port_mapping
instead of quantum_nvp_port_mapping. This patch addresses this issues."
1,"[neutron bandwidth metering]
I delete a label using neutronclient CLI (neutron meter-label-delete).
Then router's tenant_id is omitted.
I don' t know why.
it's a bug maybe"
0,"After evacuate a host with one instance to a target host it still report there is an instance in that hypervisor
Pre evacuate report:
$ nova hypervisor-stats
+----------------------+-------+
| Property | Value |
+----------------------+-------+
| count | 2 |
| current_workload | 0 |
| disk_available_least | 22 |
| free_disk_gb | 50 |
| free_ram_mb | 3860 |
| local_gb | 50 |
| local_gb_used | 0 |
| memory_mb | 4948 |
| memory_mb_used | 1088 |
| running_vms | 1 |
| vcpus | 3 |
| vcpus_used | 1 |
+----------------------+-------+
$ nova hypervisor-list
+----+---------------------+
| ID | Hypervisor hostname |
+----+---------------------+
| 1 | jmolle-Controller |
| 2 | jmolle-Node1 |
+----+---------------------+
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property | Value |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch | x86_64 |
| cpu_info_features | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model | Westmere |
| cpu_info_topology_cores | 1 |
| cpu_info_topology_sockets | 2 |
| cpu_info_topology_threads | 1 |
| cpu_info_vendor | Intel |
| current_workload | 0 |
| disk_available_least | 10 |
| free_disk_gb | 25 |
| free_ram_mb | 3378 |
| host_ip | 192.168.41.101 |
| hypervisor_hostname | jmolle-Controller |
| hypervisor_type | QEMU |
| hypervisor_version | 1000000 |
| id | 1 |
| local_gb | 25 |
| local_gb_used | 0 |
| memory_mb | 3954 |
| memory_mb_used | 576 |
| running_vms | 1 |
| service_host | jmolle-Controller |
| service_id | 4 |
| vcpus | 2 |
| vcpus_used | 1 |
+---------------------------+-------------------------------------------------------+
$ nova hypervisor-servers jmolle-Controller
+--------------------------------------+-------------------+---------------+---------------------+
| ID | Name | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 1 | jmolle-Controller |
+--------------------------------------+-------------------+---------------+---------------------+
But after evacuate the instanse we get:
$ nova hypervisor-stats
+----------------------+-------+
| Property | Value |
+----------------------+-------+
| count | 2 |
| current_workload | 1 |
| disk_available_least | 22 |
| free_disk_gb | 50 |
| free_ram_mb | 3796 |
| local_gb | 50 |
| local_gb_used | 0 |
| memory_mb | 4948 |
| memory_mb_used | 1152 |
| running_vms | 2 |
| vcpus | 3 |
| vcpus_used | 2 |
+----------------------+-------+
here we see that now there are 2 running instances instead of one
and if we use show command we get:
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property | Value |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch | x86_64 |
| cpu_info_features | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model | Westmere |
| cpu_info_topology_cores | 1 |
| cpu_info_topology_sockets | 2 |
| cpu_info_topology_threads | 1 |
| cpu_info_vendor | Intel |
| current_workload | 0 |
| disk_available_least | 10 |
| free_disk_gb | 25 |
| free_ram_mb | 3378 |
| host_ip | 192.168.41.101 |
| hypervisor_hostname | jmolle-Controller |
| hypervisor_type | QEMU |
| hypervisor_version | 1000000 |
| id | 1 |
| local_gb | 25 |
| local_gb_used | 0 |
| memory_mb | 3954 |
| memory_mb_used | 576 |
| running_vms | 1 |
| service_host | jmolle-Controller |
| service_id | 4 |
| vcpus | 2 |
| vcpus_used | 1 |
+---------------------------+-------------------------------------------------------+
we also see 1 running instance
but if we list servers we get 0
$ nova hypervisor-servers jmolle-Controller
+----+------+---------------+---------------------+
| ID | Name | Hypervisor ID | Hypervisor Hostname |
+----+------+---------------+---------------------+
+----+------+---------------+---------------------+
and the instance was evacuate to the other host correctly
$ nova hypervisor-servers jmolle-Node1
+--------------------------------------+-------------------+---------------+---------------------+
| ID | Name | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 2 | jmolle-Node1 |
+--------------------------------------+-------------------+---------------+---------------------+
I think this is a nova bug due to the inconcistency of hypervisor-show and hypervisor-servers nova commands"
1,"The nova DB migration script for 215 executes:

result = conn.execute(
        'update compute_node_stats set deleted = id, '
        'deleted_at = current_timestamp where compute_node_id not in '
        '(select id from compute_nodes where deleted <> 0)')

Need to remove the 'not' part so that we delete the right stats since the nested select finds all DELETED nodes. The current SQL actually deletes all active compute nodes' stats."
1,The neutron nova notifier (neutron/notifiers/nova.py) hardcodes the project_id to the novaclient as 'None'. This prevents the ability to use a nova admin tenant name in place of a nova admin tenant id in the event that the tenant id is not known/available at the time that the neutron service is being configured. An example of such a case is with tripleo (see related bug https://bugs.launchpad.net/tripleo/+bug/1293782).
1,"If you don't define the ""url"" parameter in the [ml2_odl] section of the ML2 configuration file, the ODL driver will run happily but no request will be made to the ODL service. See http://git.openstack.org/cgit/openstack/neutron/tree/neutron/plugins/ml2/drivers/mechanism_odl.py?id=a57dc2c30ab78ba74cfc51b8fdb457d3374cc87d#n313
The parameter should have a sane default value (eg http://127.0.0.1:8080/controller/nb/v2/neutron) and/or a message should be logged to warn the deployer."
1,"In alembic alter_column have a parameter type_ not _type.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 47, in upgrade
    _type=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/util.py"", line 271, in go
    raise TypeError(""Unknown arguments: %s"" % "", "".join(names))
TypeError: Unknown arguments: _type"
0,"2014-02-27 14:08:23.013 ERROR nova.network.neutronv2.api [req-598b0d2f-e4e9-40eb-a9d4-027975d08b39 demo demo] Failed to delete neutron port 153f472b-f662-497b-bc7c-3cc362157ab1
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api Traceback (most recent call last):
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 420, in deallocate_for_instance
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api neutron.update_port(port, port_req_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 111, in with_params
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api ret = self.function(instance, *args, **kwargs)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 321, in update_port
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api return self.put(self.port_path % (port), body=body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1245, in put
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1221, in retry_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1164, in do_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api self._handle_fault_response(status_code, replybody)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1134, in _handle_fault_response
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api exception_handler_v20(status_code, des_error_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 84, in exception_handler_v20
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api message=error_dict)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api NeutronClientException: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 ERROR neutron.api.v2.resource [req-3f133c17-198f-412d-b57e-66bbf0fcfbcb neutron abd2b56aa998417ba5af609a680a138d] update failed
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 466, in update
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource allow_bulk=self._allow_bulk)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 600, in prepare_request_body
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource raise webob.exc.HTTPBadRequest(msg)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource HTTPBadRequest: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource"
0,"CHAP encryption is supported in the xiv_ds8k cinder driver, but the value specified in the cinder conf file is not passed and the feature does not work.
The fix is minimal (patch attached)."
1,"When user tries to delete an instance, if the nova compute driver for the instance's host is down, the nova api code goes into _local_delete(). The following code creates a fake ""connector"" without a 'host' key/vale.
        for bdm in bdms:
            if bdm['volume_id']:
                # NOTE(vish): We don't have access to correct volume
                # connector info, so just pass a fake
                # connector. This can be improved when we
                # expose get_volume_connector to rpc.
                connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'}
                self.volume_api.terminate_connection(context,
                                                     bdm['volume_id'],
                                                     connector)
When code flow reaches the cinder driver, which expects a 'host', a ""KeyError: 'host'"" exception is raised, and deletion of the instance fails.
The failure in deletion is as expected since the nova driver is not up. This issue is requesting a modification in nova api code, so that if the nova compute server is not available, a more meaningful error message (""nova compute server is not available"") could be returned, before getting to the cinder code."
0,should have been removed long ago here: Ifcb4f093c92904ceb896438987d53e692eb7fb26
0,"Currently if a unit test creates a patch and does not stop it, the patch will hang around and could potentially affect other tests that rely on the mocked class/method. This can make it difficult for developers creating new tests as unrelated tests could be causing new ones to sporadically fail or vice versa depending on concurrency and test order."
1,"Seeing this in conductor logs when migrating a VM with a floating IP assigned:
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 1019, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 527, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher self.network_api.migrate_instance_start(context, instance, migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 94, in wrapped
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher return func(self, context, *args, **kwargs)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 543, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher self.network_rpcapi.migrate_instance_start(context, **args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/nova/network/rpcapi.py"", line 350, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher floating_addresses=floating_addresses)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher wait_for_reply=True, timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/transport.py"", line 90, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 409, in send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 402, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher raise result
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'FixedIP' object has no attribute '_sa_instance_state'"
1,Flavor name is restricted to [a-zA-Z0-9_.- ] during create. This will not allow characters from languages other than english.
1,"auto_schedule_routers can call bind_router within a transaction. If a DBDuplicateEntry exception happens in bind_router in such a case, the access to chosen_agent.id causes another exception as the outer transaction is already aborted, resulting in a useless debug log."
0,"The L2Pop feature seems to have broken due to OVS Concurrent DeferredBridge implementation approved here:
https://review.openstack.org/77578

On the OVS Agent logs on compute hosts, could see that dynamic add_tunnel_port creation is failing:

2014-08-07 01:05:37.618 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-0561d1e7-87a9-43cf-b7d4-c60e560d34f5 ^[[00;36mNone None^[[01;31m] ^[[01;35m^[[01;31mException during message handling: add_tunnel_port^[[00m
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m incoming.message))
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 45, in add_fdb_entries
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m self.fdb_add(context, fdb_entries)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 347, in fdb_add
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m agent_ports, self.tun_br_ofports)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 179, in fdb_add_tun
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m lvm.network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1061, in setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1017, in _setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m ofport = br.add_tunnel_port(port_name,
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 486, in __getattr__
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m raise AttributeError(name)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mAttributeError: add_tunnel_port"
1,"on RHEL7 beta agent set to use VXLAN tunneling does not start.
I'm using rdo packages, if you want I can check the upstream version somewhere, but seems that code that checks the version is still in github.
in openvswitch-agent.log I see:
2014-05-21 13:53:21.762 1814 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [req-ce4eadcb-4cbb-4c09-a404-98ecb5383fa5 None] Agent terminated
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 231, in _check_ovs_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 551, in check_ovs_vxlan_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent 'kernel', 'VXLAN')
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 529, in _compare_installed_and_required_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent raise SystemError(msg)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent SystemError: Unable to determine kernel version for Open vSwitch with VXLAN support. To use VXLAN tunnels with OVS, please ensure that the version is 1.10 or newer!
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
It seems that the minimum kernel version required to use VXLAN is set to 3.13 (shouldn't 3.9 be enough ?). RHEL7 ships only 3.10. Vxlan module however is present and working, and even if the only module properly working is in 3.13 kernel, the check doesn't take into consideration backported features."
0,"After rebasing with community I'm seeing this failure, but I haven't changed the Network object or any of it's base classes.
======================================================================
FAIL: nova.tests.objects.test_objects.TestObjectVersions.test_versions
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""nova/tests/objects/test_objects.py"", line 941, in test_versions
    self._test_versions_cls(obj_name)
  File ""nova/tests/objects/test_objects.py"", line 937, in _test_versions_cls
    'has been bumped, and then update this hash') % obj_name)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = '1.1-faba26d0290395456f9a040584c4364b'
actual = '1.1-6d5f3c575cfc4b25db53ee5f071207ce'
: Network object has changed; please make sure the version has been bumped, and then update this hash
======================================================================
FAIL: process-returncode
tags: worker-0
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 2 tests in 3.262s (+2.164s)
FAILED (id=100, failures=2)
error: testr failed (1)
Looks like it's also happening in the check queue:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTmV0d29yayBvYmplY3QgaGFzIGNoYW5nZWQ7IHBsZWFzZSBtYWtlIHN1cmUgdGhlIHZlcnNpb24gaGFzIGJlZW4gYnVtcGVkLCBhbmQgdGhlbiB1cGRhdGUgdGhpcyBoYXNoXCIgQU5EIChidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNiBPUiBidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNykgQU5EIHRhZ3M6Y29uc29sZSIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiMTcyODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMTU2MjMwMzg3Mn0=
10 hits in 48 hours, all failures, check queue only but different changes."
1,"When listing all volumes, the field volume_image_metadata is missing from the response.

When enabling debug, this error is logged:

Problem retrieving volume image metadata. It will be skipped. Error: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id' _get_all_images_metadata /usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py:43

If code is updated to log the whole traceback instead, this can be found:

Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py"", line 39, in _get_all_images_metadata
    all_metadata = self.volume_api.get_volumes_image_metadata(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/volume/api.py"", line 686, in get_volumes_image_metadata
    db_data = self.db.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/api.py"", line 552, in volume_glance_metadata_get_all
    return IMPL.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2447, in volume_glance_metadata_get_all
    return _volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2436, in _volume_glance_metadata_get_all
    session=session).\
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 195, in model_query
    query = query.filter_by(project_id=context.project_id)
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 1249, in filter_by
    for key, value in kwargs.iteritems()]
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/util.py"", line 1218, in _entity_descriptor
    (description, key)
InvalidRequestError: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id'

The problem is caused by a filtering done on project_id.

VolumeGlanceMetadata does not have the project_id field, filtering should be done on the Volume associated to the VolumeGlanceMetadata instead."
1,"In several places in the source tree we imports logging

https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L16

but instead it should do:

https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L52
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L63

arosen@arosen-desktop:/opt/stack/neutron/neutron$ grep -R ""import logging"" *
common/utils.py:import logging as std_logging
db/migration/alembic_migrations/heal_script.py:import logging
plugins/cisco/models/virt_phy_sw_v2.py:import logging
plugins/cisco/common/cisco_credentials_v2.py:import logging as LOG
plugins/cisco/network_plugin.py:import logging
plugins/cisco/nexus/cisco_nexus_snippets.py:import logging
plugins/cisco/nexus/cisco_nexus_plugin_v2.py:import logging
plugins/cisco/nexus/cisco_nexus_network_driver_v2.py:import logging
plugins/ml2/drivers/cisco/nexus/nexus_snippets.py:import logging
plugins/vmware/plugins/base.py:import logging
service.py:import logging as std_logging
tests/base.py:import logging
tests/unit/cisco/test_network_plugin.py:import logging
tests/unit/db/metering/test_db_metering.py:import logging
tests/unit/db/firewall/test_db_firewall.py:import logging
tests/unit/db/loadbalancer/test_db_loadbalancer.py:import logging
tests/unit/ml2/test_helpers.py:import logging
tests/unit/test_servicetype.py:import logging
tests/unit/vmware/apiclient/test_api_eventlet_request.py:import logging"
0,"I noticed this while reviewing https://review.openstack.org/#/c/102103/ for bug 1298131, the 3 OnsetFile*LimitExceeded exceptions from nova.compute.api.API._check_injected_file_quota are not handled in the os-compute rebuild APIs (v2 or v3), and I'm not even seeing specific unit testing for those exceptions in the _check_injected_file_quota method."
1,"A recent API change in Cisco N1KV controller requires L3 params for port creation.
N1KV Neutron plugin does not pass these params in the REST call to the controller.
The fix is to add the missing params in the REST body."
1,"When testing weather the instance can fit into the host topology will currently not take into account the number of cells hte instance has, and will only claim matching cells and pass an instance if the matching cells fit.
So for example a 4 NUMA cell isntance would pass the claims test on a 2 NUMA cell host, as long as the first 2 cells fit, without considering that the whole instance will not actually fit."
0,"The API change guidelines (https://wiki.openstack.org/wiki/APIChangeGuidelines) describe as ""generally not acceptable"": ""A change such that a request which was successful before now results in an error response (unless the success reported previously was hiding an existing error condition)"". That is exactly what this is."
1,"How to reproduce:
1) create a pool with a VIP
2) check which lbaas-agent that pool is scheduled to
3) shut down lbaas-agent on the node
4) remove the pool/VIP via the API
5) restart lbaas-agent
It turns out the VIP would remain on the lbaas-agent host forever.
Looks like the instance_mapping variable is reset to empty restart the agent restarted, so it essentially lost track of that removed pool/vip."
1,"Traceback while scheduling both overcloud nodes on tripleo ci
Last succesfull run was 05-Sep-2013 01:54:10 (UTC)
So something changed after this run https://review.openstack.org/#/c/43968/
although scheduling of baremetal node seems to work on seed ....
INFO nova.scheduler.filter_scheduler [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Attempting to build 1 instance(s) uuids: [u'f71e3e47-f2a2-4a13-9
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Failed to scheduler_run_instance: 'service'
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] [instance: f71e3e47-f2a2-4a13-92c0-c3397acaf409] Setting instance to ERR
ERROR nova.openstack.common.rpc.amqp [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Exception during message handling
TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
TRACE nova.openstack.common.rpc.amqp **args)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 160, in run_instance
TRACE nova.openstack.common.rpc.amqp context, ex, request_spec)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 147, in run_instance
TRACE nova.openstack.common.rpc.amqp legacy_bdm_in_spec)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
TRACE nova.openstack.common.rpc.amqp filter_properties, instance_uuids)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 326, in _schedule
TRACE nova.openstack.common.rpc.amqp hosts = self.host_manager.get_all_host_states(elevated)
TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/host_manager.py"", line 432, in get_all_host_states
TRACE nova.openstack.common.rpc.amqp service = compute['service']
TRACE nova.openstack.common.rpc.amqp KeyError: 'service'
TRACE nova.openstack.common.rpc.amqp"
1,"In cinder code : /cinder/transfer/api.py . Below line of code used random.random() to generate a random number, Standard random number generators should not be used to generate randomness used for security reasons. Could we use a crytographic randomness generator to provide sufficient entropy to instead of it?

rndstr = """"
random.seed(datetime.datetime.now().microsecond)
while len(rndstr) < length:
 rndstr += hashlib.sha224(str(random.random())).hexdigest() ---------------> This line has described issues.

 return rndstr[0:length]"
1,"When user passes bad keys to update quota api, it returns 200 OK response. It should return 400 Bad Request error for bad keys.
PUT
http://127.0.0.1:8776/v2/ad5d29fef329473598031ca4080288a7/os-quota-sets/ad5d29fef329473598031ca4080288a7
Request Body
{
""quota_set"": {
""gigabytes"": 5,
""xyz"": 2
}
}
Actual Response: 200 OK
{""quota_set"": {""gigabytes"": 5, ""snapshots"": 1000, ""volumes"": 10}}
Expected Response: 400
{""badRequest"": {""message"": ""Bad key(s) xyz in quota_set"", ""code"": 400}}
Note: Nova doesn't allow bad keys in the os-quota-sets put method."
0,"Temp variable _val in _extend method is not being used, so remove this '_val' variable."
1,"https://bugs.launchpad.net/oslo/+bug/1095346 tracked a fix in service.py: wait() to prevent the cinder-volume process from hogging the CPU while looping in _wait_child()
However, the wait() function will also loop in _wait_child() when the parent catched SIGTERM and the parent is waiting to reap the children. This also causes excessive CPU usage if the child does not die quickly for some reason.
I've an instance of cinder-volume that has been running for several days in this wait state. An strace of the parent shows:
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
......forever....
this is because the parent has caught SIGTERM is and is looping on wait_child without the eventlet.greenthread.sleep(.01)
during the normal running state of cinder-volume the parent does an epoll for the eventlet.greenthread.sleep(.01):
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984}) = 0 (Timeout)
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984}) = 0 (Timeout)
My CPU is pegged at 100% with this process (shown in top):
  PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
27313 cinder 20 0 103m 25m 4080 R 100 0.0 56343:11 cinder-volume
To fix this, the second call to _wait_child() needs the same sleep as the first:
    def wait(self):
        """"""Loop waiting on children to die and respawning as necessary.""""""
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(.01)
                continue
            LOG.info(_('wait wrap.failed %s'), wrap.failed)
            while (self.running and len(wrap.children) < wrap.workers
                   and not wrap.failed):
                self._start_child(wrap)
        if self.sigcaught:
            signame = {signal.SIGTERM: 'SIGTERM',
                       signal.SIGINT: 'SIGINT'}[self.sigcaught]
            LOG.info(_('Caught %s, stopping children'), signame)
        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise
        # Wait for children to die
        if self.children:
            LOG.info(_('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()
Patch is :
--- a/cinder/service.py
+++ b/cinder/service.py
@@ -323,7 +323,10 @@ class ProcessLauncher(object):
         if self.children:
             LOG.info(_('Waiting on %d children to exit'), len(self.children))
             while self.children:
- self._wait_child()
+ wrap = self._wait_child()
+ if not wrap:
+ eventlet.greenthread.sleep(.01)
+ continue"
1,"Windows Server 2012 R2 does not support vhd images as iSCSI disks, requiring VHDX images. For this reason, the cinder driver fails to create volumes. For the moment, the default format is vhd. On WSS 2012 R2, we should use vhdx images as default."
1,"If the dhcp-agent machine restarts and openvswitch logs the following
warning message for all tap interfaces that have not been recreated yet:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the dhcp-agent starts he recreates the interfaces and readds them to the
ovs-bridge. Unfortinately, ovs does not reinitalize the interface as its
already in ovsdb and does not assign it an ofport number.
In order to correct this we should first remove interfaces that exist and
then readd them.
root@arosen-desktop:~# ovs-vsctl -- --may-exist add-port br-int fake1
# ofport still -1
root@arosen-desktop:~# ovs-vsctl list inter | grep -A 2 fake1
name : ""fake1""
ofport : -1
ofport_request : []
root@arosen-desktop:~# ip link add fake1 type veth peer name fake11
root@arosen-desktop:~# ifconfig fake1
fake1 Link encap:Ethernet HWaddr 56:c3:a1:2b:1f:f4
          BROADCAST MULTICAST MTU:1500 Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)
root@arosen-desktop:~# ovs-vsctl list inter | grep -A 2 fake1
name : ""fake1""
ofport : -1
ofport_request : []
root@arosen-desktop:~# ovs-vsctl -- --may-exist add-port br-int fake1
root@arosen-desktop:~# ovs-vsctl list inter | grep -A 2 fake1
name : ""fake1""
ofport : -1
ofport_request : []"
1,"Description of problem:
=======================
pyhton-novaclient produces a wrong exception when user tries to boot an instance without specific network UUID.
The issue will only reproduce when an external network is shared with the tenant, but not created from within it (I created it in admin tenant).
Version-Release:
================
python-novaclient-2.17.0-2
How reproducible:
=================
Always
Steps to Reproduce:
===================
1. Have 2 tenants (admin + additional tenant would do).
2. In tenant A (admin), Create a network and mark it as both shared and external.
3. In tenant B, Create a network which is not shared or external.
4. Boot an instance within tenant B (I tested this via CLI), do not use the --nic option.
Actual results:
===============
DEBUG (shell:783) It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-caacfa72-82f8-492a-8ce2-9476be8f3e0c)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
Forbidden: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
ERROR: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
Expected results:
=================
This is what happens if:
1. The shared network is no longer marked as external.
2. The tenant itself has two networks.
(+ no network UUID is speficied in the 'nova boot' command)
DEBUG (shell:783) Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
ERROR: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)"
1,"Similar to https://review.openstack.org/#/c/95467/
According to https://wiki.openstack.org/wiki/Python3 dict.iteritems()
should be replaced with six.iteritems(dict).
All metadata definition code added should ensure that six.iteritems is used."
0,Tracking the need of removing deprecation warning and the _EXTRA_STORES global from stores/__init__
1,"SNAT rules with IPv6 prefixes are added into the NAT table, which causes failure with the call to iptables-restore:
Stderr: ""iptables-restore v1.4.18: invalid mask `64' specified\nError occurred at line: 22\nTry `iptables-restore -h' or 'iptables-restore --help' for more information.\n"""
1,"Query Deadlock when creating >200 servers at once in sqlalchemy.
--------
This bug occurred when I test this bug:
https://bugs.launchpad.net/nova/+bug/1270725
The original info is logged here:
http://paste.openstack.org/show/61534/
--------------
After checking the error-log, we can notice that the deadlock function is 'all()' in sqlalchemy framework.
Previously, we use '@retry_on_dead_lock' function to retry requests when deadlock occurs.
But it's only available for session deadlock(query/flush/execute). It doesn't cover some 'Query' actions in sqlalchemy.
So, we need to add the same protction for 'all()' in sqlalchemy."
1,"When using VC Driver, booting an ISO will fail when a flavor with a 0 GB root disk size is specified.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1703, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 598, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 562, in spawn
    _create_virtual_disk(dest_vmdk_path, root_gb_in_kb)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 370, in _create_virtual_disk
    self._session._wait_for_task(vmdk_create_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 906, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A specified parameter was not correct.
The error message seen in vSphere client is ""The virtual disk size is too small"".
Log available here: http://paste.openstack.org/show/73516/"
0,"Change:
https://github.com/openstack/neutron/commit/634fd1d23fb241bc4990275d5a4da0c3ab66e2de
Tweaked base create_router in l3_db.py to process the extensions or not; however it was chosen at the time, to set the process_extensions flag to False. This effectively disable the ability for extensions made to create_router's method to handle extension's attributes correctly.
A more appropriate value should be True, so that extension attributes can be shown during the create process."
0,"When a user requests non-existing resource or a bad parameter, neutron API module logs it as TRACE level (log.exception). It is so annoying for debugging or log monitoring. These kinds of events are usual behaviors and are not errors of neutron-server.
They should be logged as TRACE level. Errors mapped into 4xx HTTP response are categorized into this category."
0,"Some usages of assertTrue()/assertFalse() are incorrect, improve them to more explicit assert from the unit test suite (like assertIsNotNone or assertIn)."
0,"The One Convergence plugin doesn't currently support IPv6 so every new IPv6 test has to be explicitly skipped in the plugin's tests. This is a burden to IPv6 developers. As an interim until v6 support is added, a way to skip IPv6 tests by default should be added."
0," so NVP advanced LBaaS should also change its operation when deleting a healthmonitor."""
1,The exception raised is inappropriate. It just returns the instance object. This should be a coherent message.
1,"Hi all,
There is no quota for allowed address pair, user can create unlimited allowed address pair, in the backend, there will be at least 1 iptables rule for one allowed address pair. I tested if we use the attachment script to add about 10,000 allowed address pair. It will cost 30 sec to reflesh iptables rules in kernel... I think that bad man can use this api to attack compute nodes. This will make the compute nodes crash or very slow only if we add enough allowed address pair rules...
Thanks.
Liping Mao"
1,"VMs' drives placement in Ceph option has been chosen (libvirt.images_types == 'rbd').
When user creates a flavor and specifies:
   - root drive size >0
   - ephemeral drive size >0 (important)
and tries to boot a VM, he gets ""no valid host was found"" in the scheduler log:
Error from last host: node-3.int.host.com (node node-3.int.host.com): [u'Traceback (most recent call last):\n', u'
 File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1305, in _build_instance\n set_access_ip=set_access_ip)\n', u' File ""/usr/l
ib/python2.6/site-packages/nova/compute/manager.py"", line 393, in decorated_function\n return function(self, context, *args, **kwargs)\n', u' File
 ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1717, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instanc
e)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\n six.reraise(self.type_, self.value, se
lf.tb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1714, in _spawn\n block_device_info)\n', u' File ""/usr/lib/py
thon2.6/site-packages/nova/virt/libvirt/driver.py"", line 2259, in spawn\n admin_pass=admin_password)\n', u' File ""/usr/lib/python2.6/site-packages
/nova/virt/libvirt/driver.py"", line 2648, in _create_image\n ephemeral_size=ephemeral_gb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/
libvirt/imagebackend.py"", line 186, in cache\n *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebackend.py"",
line 587, in create_image\n prepare_template(target=base, max_size=size, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/opens
tack/common/lockutils.py"", line 249, in inner\n return f(*args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebac
kend.py"", line 176, in fetch_func_sync\n fetch_func(target=target, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvir
t/driver.py"", line 2458, in _create_ephemeral\n disk.mkfs(os_type, fs_label, target, run_as_root=is_block_dev)\n', u' File ""/usr/lib/python2.6/sit
e-packages/nova/virt/disk/api.py"", line 117, in mkfs\n utils.mkfs(default_fs, target, fs_label, run_as_root=run_as_root)\n', u' File ""/usr/lib/pyt
hon2.6/site-packages/nova/utils.py"", line 856, in mkfs\n execute(*args, run_as_root=run_as_root)\n', u' File ""/usr/lib/python2.6/site-packages/nov
a/utils.py"", line 165, in execute\n return processutils.execute(*cmd, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/commo
n/processutils.py"", line 193, in execute\n cmd=\' \'.join(cmd))\n', u""ProcessExecutionError: Unexpected error while running command.\nCommand: sudo
 nova-rootwrap /etc/nova/rootwrap.conf mkfs -t ext3 -F -L ephemeral0 /var/lib/nova/instances/_base/ephemeral_1_default\nExit code: 1\nStdout: ''\nStde
rr: 'mke2fs 1.41.12 (17-May-2010)\\nmkfs.ext3: No such file or directory while trying to determine filesystem size\\n'\n""]"
1,"I originally thought this was bug 1306342 but that's a different issue, that was when not having neutron configured properly for calling back to nova and nova would timeout on spawn waiting for a notification from neutron that networking was setup.
This is a different issue where resize/migrate fails if you started from a stopped instance and using neutron. In this case, the _create_domain_and_network method in the libvirt driver passes in power_on=False since the instance was stopped before the resize/migration. The virtual interface isn't plugged in that case so we're waiting on a neutron event that's not going to happen, and we hit the eventlet timeout which then tries to destroy the non-running domain, and that fails with a libvirtError telling you that the domain isn't running in the first place.
The fix is to check the power_on flag in _create_domain_and_network and if it's False, don't wait for neutron events, same as if vifs_already_plugged=False is passed in."
0,"Since the test is `test_create_missing_specs_name` [1], we should define the body like `body = {'qos_specs': {'a': 'b'}}`. If we use body={'foo': {'a': 'b'}}, the code even do not reach [2], and it is actually raised by a not valid body [3].
[1]: https://github.com/openstack/cinder/blob/master/cinder/tests/api/contrib/test_qos_specs_manage.py#L326
[2]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L131
[3]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L127"
0,"The test code in glance/tests/unit/v1/test_api.py:TestGlanceAPI.test_get_images_unauthorized requests for the wrong URL - instead of requesting for ""/images"" it requests for ""/images/detail"" (making it identical to the test_get_images_unauthorized test)."
1,"In nova/virt/libvirt/imagebackend.py, the images_rbd_pool and images_rbd_ceph_conf have deprecated_name set to libvirt_images_rdb_pool and libvirt_images_rdb_ceph_conf, respectively, but the actual option names prior to commit 25a7de2054ba6ae5eb318c86fe165f4302fbfff8 are libvirt_images_rbd_pool and libvirt_images_rbd_ceph_conf. (Note the transposition of the d and b in rbd.)"
0,"Port this change in the cisco plugin, https://review.openstack.org/#/c/50389/ (Detect and process live-migration in Cisco plugin) to ML2. Changes under this bug were made to the cisco plugin update_port() method."
1,"Default value for FirewallDriver set to None in security_group_rpc.py.
L2Agent fails when using default value with following error:
/opt/stack/neutron/neutron/agent/securitygroups_rpc.py:129
2014-03-07 08:15:09.120 31995 CRITICAL neutron [req-63f8e61b-9b71-4178-95b9-ab070a4e3b26 None] 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron Traceback (most recent call last):
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/usr/local/bin/neutron-linuxbridge-agent"", line 10, in <module>
2014-03-07 08:15:09.120 31995 TRACE neutron sys.exit(main())
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 987, in main
2014-03-07 08:15:09.120 31995 TRACE neutron root_helper)
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 787, in __init__
2014-03-07 08:15:09.120 31995 TRACE neutron self.init_firewall()
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/opt/stack/neutron/neutron/agent/securitygroups_rpc.py"", line 130, in init_firewall
2014-03-07 08:15:09.120 31995 TRACE neutron self.firewall = importutils.import_object(firewall_driver)
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
2014-03-07 08:15:09.120 31995 TRACE neutron return import_class(import_str)(*args, **kwargs)
2014-03-07 08:15:09.120 31995 TRACE neutron File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 26, in import_class
2014-03-07 08:15:09.120 31995 TRACE neutron mod_str, _sep, class_str = import_str.rpartition('.')
2014-03-07 08:15:09.120 31995 TRACE neutron AttributeError: 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron
This can be fixed by setting default firewall_driver = neutron.agent.firewall.NoopFirewallDriver or verification on L2 Agent start-up for firewall_driver is not being None."
1,"server's controller load the update extension point as below:
       # Look for implmentation of extension point of server update
        self.update_extension_manager = \
            stevedore.enabled.EnabledExtensionManager(
                namespace=self.EXTENSION_UPDATE_NAMESPACE,
                check_func=_check_load_extension('server_resize'),
                invoke_on_load=True,
                invoke_kwds={""extension_info"": self.extension_info},
                propagate_map_exceptions=True)
        if not list(self.update_extension_manager):
But it's checking function with wrong params, it should be _check_load_extension('server_update')."
1,"The python neutron client for the V2 API expects the neutron API to send information back such as the type and detail of the exception the body of the message (serialized as a dict). See exception_handler_v20 in client.py. However, the neutron v2 api only returns the exception message. This means that the client can only propagate up a generic NeutronClientException exceptions rather than more specific ones related to the actual problem.

All of the necessary information is present in the v2 api resource class at the point the webob exception is raised to include at least the type of the exception (detail appears not to be used in neutron exceptions) so the format of the message returned should be changed to include a type field and a message field rather than just the message."
1,"1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot
4. Extend the volume.
5. Horizon shows the volume with status = Error Extending
2014-02-27 15:27:55.028 ERROR cinder.volume.manager [req-bb0716d0-5d3c-430a-83fc-df0e2d85444c dd968af1de0d457b8d43217a821edf1a 63be499ec75e48d597b83679112d32bd] volume 7f440946-8b67-4b0a-85de-9b94add2d258: Error trying to extend volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager Traceback (most recent call last):
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1114, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager volume_id)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-27 15:27:55.028 TRACE cinder.volume.manager retval = f(*args, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_iscsi.py"", line 437, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager try:
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 251, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager self.client.growVolume(volume_name, growth_size)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-02-27 15:27:55.028 TRACE cinder.volume.manager six.reraise(self.type_, self.value, self.tb)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 248, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/client.py"", line 452, in growVolume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager response, body = self.http.put('/volumes/%s' % name, body=info)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 306, in put
2014-02-27 15:27:55.028 TRACE cinder.volume.manager return self._cs_request(url, 'PUT', **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 239, in _cs_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager resp, body = self._do_reauth(url, method, ex, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 216, in _do_reauth
2014-02-27 15:27:55.028 TRACE cinder.volume.manager resp, body = self._time_request(self.api_url + url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 205, in _time_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager resp, body = self.request(url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 199, in request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager raise exceptions.from_response(resp, body)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager HTTPForbidden: Forbidden (HTTP 403) 150 - invalid operation: Cannot grow this type of volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager"
1,"HI
in admin role, I create External Network and router. and create tenant A and userA.
Now the userA login, create network and router, create VM1 and assign Floating IP , access well ,perfectly.
Now I try to in admin roles delete it.
1: delete userA , no problem
2: delete tenantA , no problem
3: delete vm1, no problem
4: delete router, no problem
5: delete External Networknet, report error, I try to delete the port in sub panel, also fail.
check the Neutrion server log
TRACE neutron.api.v2.resource L3PortInUse: Port 2e5fa663-22e0-4c9e-87cc-e89c12eff955 has owner network:floatingip and therefore cannot be deleted directly via the port API."
0,"from nova/objects/network_request.py:

class NetworkRequest(obj_base.NovaObject):
    # Version 1.0: Initial version
    # Version 1.1: Added pci_request_id
    VERSION = '1.0'

VERSION should be 1.1, per the comment above it."
1,"I am running Juno int code on multi node devstack environment. I am trying to boot 10 VMs in each different network. If so, few VMs, will fail to boot. If i delete & recreate a VM in the same network, it will fail to get the ip address.
Here are the steps followed to hit this issue
1. Create a network
2. Create a subnet
3. Create a distributed router. Add the subnet to this DVR
4. Boot a VM.
5. Follow the steps 1-4 for 9 more VMs.
6. In this case, few VMs will not get the ip address.
7. I see that dhcp request is not reaching the NN. Please note that few more VMs hosted on the same CN are getting the ip address.
On both Openstack Controller & Compute node logs does not show any Errors."
1,"TerminateInstance returns the currentstate name and previousstate name are same.
In the below sample response elements show the currnentstate name and previoustate name as ""running"".
Ideally the currentstate name should be ""terminated"".
==
<TerminateInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2013-10-15/"">
  <requestId>req-c15f5c7d-2551-4a08-b8b8-255462a09592</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>16</code>
        <name>running</name>
      </currentState>
      <previousState>
        <code>16</code>
        <name>running</name>
      </previousState>
    </item>
  </instancesSet>
</TerminateInstancesResponse>
=="
1,"This is bug 1011134 again happening in a cloud that does not have the ipv6 flag set,
so the previous patch from https://review.openstack.org/14017
is not used.
Guest VMs will try to configure IPv6 link-local addrs even without the outer parts supporting it
and can throw errors when they see inbound packets with their own MAC address.
Note: I think, this bug can not be unit-tested as it requires a complex setup including running a VM in a cloud."
1,"This nova commit: http://git.openstack.org/cgit/openstack/nova/commit/?id=a8a5d44c8aca218f00649232c2b8a46aee59b77e
made VNIC_TYPE a compulsory port bindings attribute.
This broke the NSX plugin which is now not able to boot VMs anymore. Probably other plugins are affected.
Whether VNIC_TYPE is really a required attribute questionable; the fact that port bindings is such a messy interface that can cause this kind of breakages is at least annoying.
Regardless, all plugins must now adapt.
This will also be fixed once a general fix for bug 1370077 is introduced - nevertheless, the NSX plugin can't risk staying broken for more time, and also its 3rd party integration tests are disabled because of this. For this reason we're opening a bug specific for this plugin to fast-track a fix for it."
0,"During cisco-network-profile-update, if a tenant id is being added to the network profile, the current behavior is to remove all the tenant-network profile bindings and add the new list of tenants. This works well with horizon since all the existing tenant UUIDs, along with the new tenant id, are passed during update network profile.
If you try to update a network profile and add new tenant to the network profile via CLI, this will replace the existing tenant-network profile bindings and add only the new one.

Expected behavior is to not delete the existing tenant bindings and instead only add new tenants to the list."
0,"I'm guessing (though I'm not really sure) that these two items in cinder/tests/test_iscsi.py are inaccurate:
from TgtAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'tgt-admin --update iqn.2011-09.org.foo.bar:blaa',
            'tgt-admin --force '
            '--delete iqn.2010-10.org.openstack:volume-blaa',
            'tgtadm --lld iscsi --op show --mode target'])
from LioAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'cinder-rtstool create '
            '/foo iqn.2011-09.org.foo.bar:blaa test_id test_pass',
            'cinder-rtstool delete iqn.2010-10.org.openstack:volume-blaa'])
Note how the IQNs for creation and deletion don't match.
The trouble is that if you replace those hardcoded IQNs with %(target)s, the unit tests break, possibly indicating an issue with the underlying brick implementations themselves."
0,"If you run nova unit tests outside of a virtualenv, like with using nosetests on python 2.6 (which should be supported but not really enforced), then the ec2 test_cloud test fails due to not having the consoleauth_manager config option in scope:
Traceback (most recent call last):
  File ""/root/nova/nova/tests/api/ec2/test_cloud.py"", line 178, in setUp
    self.consoleauth = self.start_service('consoleauth')
  File ""/root/nova/nova/test.py"", line 295, in start_service
    svc = self.useFixture(ServiceFixture(name, host, **kwargs))
  File ""/usr/lib/python2.6/site-packages/testtools/testcase.py"", line 628, in useFixture
    fixture.setUp()
  File ""/root/nova/nova/test.py"", line 174, in setUp
    self.service = service.Service.create(**self.kwargs)
  File ""/root/nova/nova/service.py"", line 272, in create
    manager = CONF.get(manager_cls, None)
  File ""/usr/lib64/python2.6/_abcoll.py"", line 336, in get
    return self[key]
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1626, in __getitem__
    return self.__getattr__(key)
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1622, in __getattr__
    raise NoSuchOptError(name)
NoSuchOptError: no such option: consoleauth_manager
There is a mailing list thread related to this, but not for ec2:
http://lists.openstack.org/pipermail/openstack-dev/2013-September/014896.html
Simply importing this fixes the problem:
CONF.import_opt('consoleauth_manager', 'nova.consoleauth.manager')"
0,"When Nova Scheduler is installed via packstack as the only explicitly installed service on a particular node, it will fail to start. This is because it depends on the Python cinderclient library, which is not marked as a dependency in 'nova::scheduler' class in Packstack."
0,"cinder.tests.test_db_api.DBAPIVolumeTestCase.test_volume_get_all_filters_limit
This test indirectly asserts that the metadata on a volume is returned in a certain order. This is incorrect because python dictionaries to not maintain ordering. This causes spuratic unit test failures with this error:
http://paste.openstack.org/show/73692/"
1,"The traceback observed in many of tempest jobs:
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 87, in _ensure_host_set_on_ports
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher interface, router['id'])
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 100, in _ensure_host_set_on_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher {'port': {portbindings.HOST_ID: host}})
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 891, in update_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher return bound_port._port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute '_port'
Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkF0dHJpYnV0ZUVycm9yOiAnZGljdCcgb2JqZWN0IGhhcyBubyBhdHRyaWJ1dGUgJ19wb3J0J1wiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3Njk1MjI1MzM1fQ=="
1,"Shutting down a libvirt-lxc based instance will leak the nbd device. This happens because _teardown_container will only be called when libvirt domain's are running. During a shutdown, the domain is not running at the time of the destroy. Thus, _teardown_container is never called and the nbd device is never disconnected.
Steps to reproduce:
1) Create devstack using local.conf: https://gist.github.com/ramielrowe/6ae233dc2c2cd479498a
2) Create an instance
3) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
4) Shutdown instance
5) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
6) Delete instance
7) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
Nova has now leaked the nbd device."
0,"Remove unused code in test_admin_actions.py - in _migrate_volume_comp_exec(), admin_ctx is not used."
0,"In order to gain more flexibility we decide to remove the ""workflows"" from the driver ""side"" and to load them on the ""vDirect side"".
This will give us more flexibility when we need to customize our solution.
With this change the driver will no longer updload the workflows into vDirect server.
It will only make sure that thw workflows are loaded on the vDirect server side."
1,"For every export in the Datera driver, there is an export object associated with it.
Problem: When a detach operation happens, the export object hangs around.
To Reproduce: Attach a Datera volume to a virtual machine. Then detach it and notice in the backend the object is still there.
Expected Behavior: If the lun that exists in the export object is removed disassociated, the export object should be destroyed."
1,If an older node does an Instance.refresh() it will fail because conductor will overwrite the info_cache field with a new InstanceInfoCache object. This happens during the LifecycleEvent handler in nova-compute.
1,"the flow _provision_local_vlan_inbound_for_tunnel installs needs push_vlan.
while the current code happens to work with older versions of OVS,
the latest OVS correctly rejects the flow."
1,"The exception NoMoreFixedIps in nova/exception.py has a very generic error message:
""Zero fixed ips available.""
When performing a deploy with multiple networks, it can become difficult to determine which network has been exhausted. Slight modification to this error message will help simplify the debug process for operators."
1,"Description of problem: Bug in quota calculation, if an image upload fails due to quota limit, the failed image size is still added to total storage sum figure! Thus future images may fail to upload even if it looks as quota hasn鈥檛 been reached yet.
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch
How reproducible:
Steps to Reproduce:
1. vim /etc/glance/glance-api.conf user_storage_quota = 250mb (in byets)
2. service glance-api restart
3. Upload test small image - would be ok
4. Upload large image say 4Giga, - should fail with ""Error unable to create new image""
5. Try to upload another small file say 49MB.
Actual results:
If the large i,age file or sum of failed uploaded images are more than the quota, any image size will fail to upload.
Expected results:
I should be able to upload as long as the sum of all my images is less than configured qouta.
Additional info:
Mysql show databases;
connect glance;
SELECT * FROM images;
Noticed all the images i tired, initial successful uploaded image status=鈥漚ctive鈥? images that i deleted status=鈥漝eleted鈥? images that failed to upload due to quota status=鈥漦illed鈥?I than calculated the sum of all the 鈥渒illed鈥?images.
Set a new quota of the above calculated value + 100MB, restarted glance-api service.
Only than i was able to upload another image of 49MB.
When i set a lower quota value (below the calculated sum of all the killed images) wasn鈥檛 able to upload any image.
Images of status killed, which fail upload for any reason, should not be added to total storage sum calcualtion or quota."
0,"jichen@cloudcontroller:~$ nova backup jitest3 jiback1 daily 2
ERROR (Conflict): Cannot 'createBackup' while instance is in vm_state paused (HTTP 409) (Request-ID: req-7554dea8-92aa-480c-a1f4-e3d7e479c6b3)
jichen@cloudcontroller:~$ nova list
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| cb7c6742-7b7a-44de-ad5a-8570ee520f9e | jitest1 | ACTIVE | - | Running | private=10.0.0.2 |
| 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 | jitest3 | PAUSED | - | Paused | private=10.0.0.200 |
+--------------------------------------+---------+--------+------------+-------------+--------------------+

jichen@cloudcontroller:~$ nova image-create --show jitest3 test3image1
+-------------------------------------+--------------------------------------+
| Property | Value |
+-------------------------------------+--------------------------------------+
| OS-EXT-IMG-SIZE:size | 0 |
| created | 2014-08-26T04:06:41Z |
| id | 96a5284c-5feb-4231-8b01-9a522a7c5aab |
| metadata base_image_ref | 94e061fb-e628-4deb-901c-9d44c059ecd9 |
| metadata clean_attempts | 2 |
| metadata image_type | snapshot |
| metadata instance_type_ephemeral_gb | 0 |
| metadata instance_type_flavorid | 1 |
| metadata instance_type_id | 2 |
| metadata instance_type_memory_mb | 512 |
| metadata instance_type_name | m1.tiny |
| metadata instance_type_root_gb | 1 |
| metadata instance_type_rxtx_factor | 1.0 |
| metadata instance_type_swap | 0 |
| metadata instance_type_vcpus | 1 |
| metadata instance_uuid | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| metadata kernel_id | 20be8b63-5a84-4440-a0bd-8f69898d5965 |
| metadata ramdisk_id | 07f6f85f-c1dc-4790-98b5-14ab86f21b59 |
| metadata user_id | 256dc6db4b5c45ae90fee8132cbaad7c |
| minDisk | 1 |
| minRam | 0 |
| name | test3image1 |
| progress | 25 |
| server | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| status | SAVING |
| updated | 2014-08-26T04:06:41Z |
+-------------------------------------+--------------------------------------+"
0,"The simple_tenant_usage extension gets the flavor data from the instance and then looks up the flavor from the database to return usage information. Since we now store all of the flavor data in the instance itself, we should use that information instead of what the flavor currently says is right. This both (a) makes it more accurate and (b) avoids us failing to return usage info if a flavor disappears."
1,"when rebuild an instance, and nova-compute died unexpectedly, then the instance will be in REBUILD state forever unless admin take actions
[root@controller ~]# nova list
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| a9dd1fd6-27fb-4128-92e6-93bcab085a98 | test11 | REBUILD | rebuilding | Running"
1,"The TrustedFilter uses host_state.host as the name that will be checked against the remote attestation service.
This works for the KVM case because the compute node and the hypervisor are the same; however we must be checking host_state.nodename which is the hostname for the hypervisor which will be registered with the attestation server."
0,The library openstack.common.exceptions is deprecated in Oslo and should be removed.
0,"Ports, networks and subnets have a do_delete=True parameter. By default, these resources are deleted at the end of the context manager scope. All other resources use a different semantic: no_delete=False.
This causes confusing situations such as:
with self.subnet(network, do_delete=False) as subnet:
    with self.security_group(no_delete=True) as sg:
        pass
I personally fell to the pitfall of using do_delete for the security group and was surprised when it wasn't deleted at the end of the scope.
Finally, the double negative of no_delete=False is confusing and should be avoided."
1,"cinder/backup/api.py in the case of a restore to a new volume, creates the volume and then casts the restore request to the backup service. Unfortunately it casts it to the backup host name, not the volume host name, and since the lvm driver uses localpath in the restore code, this means the volume isn't found and the restore fails (lvm, multi-node case only)
i.e. the existing:
        self.backup_rpcapi.restore_backup(context,
                                          backup['host'],
                                          backup['id'],
                                          volume_id)
needs to be:
        self.backup_rpcapi.restore_backup(context,
                                          extract_host_from_volume(volume['host']),
                                          backup['id'],
                                          volume_id)"
1,"when I port flaovr tempest test in nova v3. I find when create flavor with default value of ephemeral_gb and swap (both of them are 0). the response is """". I think it's bug. I look into the code find that the issue is
swap"": flavor.get(""swap"") or """",
""ephemeral"": flavor.get(""ephemeral_gb"") or """",
it's a logic bug in nova/api/openstack/compute/views/flavors.py, I think.
the tempest log is the following:
2013-08-26 21:45:10.222 31704 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Body: {""flavor"": {""disk"": 10, ""vcpus"": 1, ""ram"": 512, ""name"": ""test_flavor_832042179"", ""id"": 1663317675}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-26 21:45:10.628 31704 INFO tempest.common.rest_client [-] Response Status: 200
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Mon, 26 Aug 2013 13:45:10 GMT', 'content-length': '369', 'content-type': 'application/json', 'x-compute-request-id': 'req-7ba21477-fda1-41f0-a782-3d58b05f293a'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Body: {""flavor"": {""name"": ""test_flavor_832042179"", ""links"": [{""href"": ""http://192.168.1.101:8774/v3/flavors/1663317675"", ""rel"": ""self""}, {""href"": ""http://192.168.1.101:8774/flavors/1663317675"", ""rel"": ""bookmark""}], ""ram"": 512, ""ephemeral"": """", ""disabled"": false, ""vcpus"": 1, ""swap"": """", ""os-flavor-access:is_public"": true, ""rxtx_factor"": 1.0, ""disk"": 10, ""id"": ""1663317675""}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314"
1,"When an exceptin occurs in _wait_for_task and a failure occurs, for example a file is requested and it does not exists then another exception is also thrown:
013-10-31 10:49:52.617 WARNING nova.virt.vmwareapi.driver [-] In vmwareapi:_poll_task, Got this error Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 78, in _inner
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall self.f(*self.args, **self.kw)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 941, in _poll_task
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall done.send_exception(excep)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 208, in send_exception
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall return self.send(None, args)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 150, in send
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall assert self._result is NOT_USED, 'Trying to re-send() an already-triggered event.'
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall AssertionError: Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall"
1,"Log stash:
message: ""Object GET failed"" AND filename:""logs/screen-g-api.txt""
53 failure in the past 7 days.
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/console.html#_2014-03-14_18_26_47_575
Test runner worker pid: 1541
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/tempest.txt.gz#_2014-03-14_17_55_47_916
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912"
1,"Currently we have a situation where if an instance fails to delete,
instead of having its state reverted, like we do in most places we set
it to error,deleting. This was intentionally done in
https://review.openstack.org/#/c/58829/ . We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(https://review.openstack.org/#/c/55444/). The combination of these two
things means that if an instance fails to delete for some reason a
tenant is unable to delete that instance.
It turns out this is really bad because instances in deleting state
count against quota, so the tenant slowly looses usable quota.
To fix this, allow duplicate delete calls to go through if the instance
is in error state."
1,Currently the VMDK driver picks a datastore for volume creation based on space utilization. The shell VM corresponding to the volume is migrated to a different host and datastore if the volume is attached to an instance created in a host which cannot access the volume's current datastore. These migrations could be minimized if a datastore connected to multiple hosts is picked at the time of volume creation.
0,"The nova-manage command exposes the action_args options during the usage output for command.
E.g.
$ nova-manage network modify -h
usage: nova-manage network modify [-h] [--fixed_range <x.x.x.x/yy>]
                                  [--project <project name>] [--host <host>]
                                  [--disassociate-project]
                                  [--disassociate-host]
                                  [action_args [action_args ...]]
positional arguments:
  action_args
<snip>
This can cause confusion as users naturally expect there to be more ""actions"" on commands like ""modify"". Even in straightforward cases, this positional argument leaks into usage.
$ nova-manage db version -h
usage: nova-manage db version [-h] [action_args [action_args ...]]
positional arguments:
  action_args
Please consider suppressing documentation on action_args. In addition, expose the __doc__ strings for these functions, which is done in the nova command."
1,"I was trying to add a json field to DB but forget to dumps the json to string, and nova api report the following error.
2013-12-17 12:37:51.615 TRACE object Traceback (most recent call last):
2013-12-17 12:37:51.615 TRACE object File ""/opt/stack/nova/nova/objects/base.py"", line 70, in setter
2013-12-17 12:37:51.615 TRACE object field.coerce(self, name, value))
2013-12-17 12:37:51.615 TRACE object File ""/opt/stack/nova/nova/objects/fields.py"", line 166, in coerce
2013-12-17 12:37:51.615 TRACE object return self._type.coerce(obj, attr, value)
2013-12-17 12:37:51.615 TRACE object File ""/opt/stack/nova/nova/objects/fields.py"", line 218, in coerce
2013-12-17 12:37:51.615 TRACE object raise ValueError(_('A string is required here, not %s'),
2013-12-17 12:37:51.615 TRACE object ValueError: (u'A string is required here, not %s', 'dict') <<<<<<<<<<<
The error should be <string is required here, not dict>"
1,"The bug is when you resize a VM to another compute node using Hyper-V VMUtilsV2, there will be an exception in the compute node which the VM located before resizing.
The exception is ""Cannot find boot VHD file for instance: instance-0000000e"".
After debuged, the issue maybe in funtion get_vm_storage_paths of vmutils.py.
If using Hyper-V v1, the get_vm_storage_paths function in vmutils.py can find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
[u'Microsoft Virtual Keyboard',
u'Microsoft Virtual PS2 Mouse',
u'Microsoft S3 Display Controller',
u'Microsoft Synthetic Diskette Drive',
None,
u'Microsoft Serial Controller',
u'Microsoft Serial Port',
u'Microsoft Serial Port',
u'Microsoft Synthetic Disk Drive',
u'Microsoft Virtual Hard Disk',
u'Microsoft Synthetic DVD Drive',
u'Microsoft Virtual CD/DVD Disk',
u'Microsoft Emulated IDE Controller',
u'Microsoft Emulated IDE Controller',
u'Microsoft Synthetic Mouse',
u'Microsoft Synthetic Display Controller',
u'Microsoft Synthetic SCSI Controller']
If using Hyper-V v2, the get_vm_storage_paths function in vmutils.py can not find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
Microsoft:Hyper-V:Virtual Keyboard
Microsoft:Hyper-V:Virtual PS2 Mouse
Microsoft:Hyper-V:S3 Display Controller
Microsoft:Hyper-V:Synthetic Diskette Drive
None
Microsoft:Hyper-V:Serial Controller
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Synthetic Disk Drive
Microsoft:Hyper-V:Synthetic DVD Drive
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Synthetic Mouse
Microsoft:Hyper-V:Synthetic Display Controller
Microsoft:Hyper-V:Synthetic SCSI Controller
I also find in Hyper-V v2 I can find Microsoft Virtual Hard Disk from class Msvm_StorageAllocationSettingData.
Maybe the Hyper-V v2 api changed, but the codes in nova didn't change."
1,"In neutron/api/extensions.py, there are some typos, no needed blank lines and docstring needed to be improved (according to HACKING.rst)"
0,"According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation."
1,"binding:profile attribute is a dict, but there is no way to clear it.
None is used to indicate to clear the corresponding attribute in general.
The validator for port binding should accept None.
It can be fixed by changing the type:dict to type:dict_or_none in the extension.
It is required by NEC plugin portbinding support ( https://blueprints.launchpad.net/neutron/+spec/nec-port-binding ) but it requires a change in the common code, so this bug will be fixed in a separate patch."
0,"Currently there is import-specific validation of the input field hard-coded into the create method of the tasks resource. Since this field is provider specific, validation should be left up to the actual import process. The task resource should only validate the fields common to all tasks (type and input - presence only)."
0,"mock.assert_called_once() is a noop, it doesn't test anything.
Instead it should be mock.assert_called_once_with()
This occurs in the following places:
  Nova
    nova/tests/virt/hyperv/test_ioutils.py
    nova/tests/virt/libvirt/test_driver.py
  Cliff
    cliff/tests/test_app.py
  Neutron
    neutron/tests/unit/services/l3_router/test_l3_apic_plugin.py
    neutron/tests/unit/services/loadbalancer/drivers/radware/test_plugin_driver.py
    neutron/tests/unit/test_l3_agent.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_sync.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_mechanism_driver.py"
1,"Do the below test:
1. Set the maximum number of volumes to 10 for user admin
2. Create type-1 on IBM-svc
3. Create type-7 on IBM-v7k
4. Create 10 volumes on type-1
5. Retype the volume from type-1 to type-7
The cinder will return error:
stack@ubuntu1310:/etc/cinder$ cinder retype --migration-policy on-demand 82cbafa7-949f-4c90-bde9-170507baa32f type-7
ERROR: VolumeLimitExceeded: Maximum number of volumes allowed (12) exceeded (HTTP 413) (Request-ID: req-15e2c0bb-e493-4ceb-a3f1-59d3c5282eff)
6. Check the volume status after that, it was found the status of the volume change to retyping , and keep on retyping after that.
| ID | Status | Name | Size | Volume Type | Bootable |
| 82cbafa7-949f-4c90-bde9-170507baa32f | retyping | 111 | 1 | type-1 | false |
It may because when retyping for migrate, it will have a template volume creation.
If the volumes has reached the maximum number, it can't create the template volume.
But if the retype command failed, the status for the volume should not change to retyping.
It is better keeping on available."
0,The ESX driver was deprecated in Icehouse and should be removed in Juno. This bug is for the removal of the ESX virt driver in nova.
0,"The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead."
1,"Fails here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/console.html
The error in the n-api log is here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/logs/screen-n-api.txt.gz#_2014-06-05_06_50_26_679
From the console log, it looks like it goes into an error state after going to verify_resize state:
2014-06-05 06:50:26.714 | 2014-06-05 06:50:26,573 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 200 GET http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55 0.103s
2014-06-05 06:50:26.714 | 2014-06-05 06:50:26,573 State transition ""RESIZE/resize_finish"" ==> ""VERIFY_RESIZE/None"" after 16 second wait
2014-06-05 06:50:26.714 | 2014-06-05 06:50:26,681 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 400 POST http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55/action 0.106s
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTWlncmF0aW9uc0FkbWluVGVzdFwiIEFORCBtZXNzYWdlOlwiSFRUUCBleGNlcHRpb24gdGhyb3duXFw6IEluc3RhbmNlIGhhcyBub3QgYmVlbiByZXNpemVkXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDE5NzIzNDQyMjh9
8 hits in 7 days, looks like this started on 6/5. Fails in check and gate queues."
0,"during we cold-migrating or resizing an instance to another host(or during deleting it), the get_console_output
method may raise an InstanceNotFound excetpion if the instance is not on the hypervisor, this is an expected error,
so we should add the InstanceNotFound excetpion to expected exceptions list in the compute manager.
2014-03-14 11:59:58.884 AUDIT nova.compute.manager [req-6414594a-7fcd-427e-9ed6-1d78f12d40e0 demo demo] [instance: c68b2e95-8299-415a-a837-ff9f7303e6db] Get console output
2014-03-14 11:59:58.901 ERROR oslo.messaging._executors.base [-] Exception during message handling
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base Traceback (most recent call last):
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base incoming.reply(self.callback(incoming.ctxt, incoming.message))
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base return self._dispatch(endpoint, method, ctxt, args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/server.py"", line 153, in inner
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base return func(*args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base payload)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base six.reraise(self.type_, self.value, self.tb)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base return f(self, context, *args, **kw)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/compute/manager.py"", line 293, in decorated_function
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base return function(self, context, *args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/compute/manager.py"", line 3932, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base output = self.driver.get_console_output(context, instance)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2268, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base virt_dom = self._lookup_by_name(instance.name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3437, in _lookup_by_name
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base raise exception.InstanceNotFound(instance_id=instance_name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base InstanceNotFound: Instance instance-0000000c could not be found.
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base"
1,"There are a couple of places in the driver where we use the keys() method without checking for None.
I have seen several times the following exception:
2014-09-22 11:45:07.312 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute 'keys'
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 198, in run_periodic_tasks
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task task(self, context)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/compute/manager.py"", line 5909, in update_available_resource
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task nodenames = set(self.driver.get_available_nodes())
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 426, in get_available_nodes
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task self._update_resources()
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 306, in _update_resources
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task added_nodes = set(self.dict_mors.keys()) - set(self._resource_keys)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task AttributeError: 'NoneType' object has no attribute 'keys'"
0,"Neutron, currently does a strict validation code in https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L618
so that for non-shared network the subnets and ports must belong to the same tenant as the network. In the case of a “service VM” created by an admin user, this function should return thus allowing admin users to create ports and networks in a tenant network.

Original code: https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L604

Proposed Fix:

    def _validate_network_tenant_ownership(self, request, resource_item):
        # TODO(salvatore-orlando): consider whether this check can be folded
        # in the policy engine
        if self._resource not in ('port', 'subnet') or request.context.is_admin:
            return
        network = self._plugin.get_network(
            request.context,
            resource_item['network_id'])
        # do not perform the check on shared networks
        if network.get('shared'):
            return

        network_owner = network['tenant_id']

        if network_owner != resource_item['tenant_id']:
            msg = _(""Tenant %(tenant_id)s not allowed to ""
                    ""create %(resource)s on this network"")
            raise webob.exc.HTTPForbidden(msg % {
                ""tenant_id"": resource_item['tenant_id'],
                ""resource"": self._resource,
            })"
1," we do not want to delete the original VM."""
0,"Lots of tempest tests fail after upgrade
http://logs.openstack.org/51/94351/3/check/check-grenade-dsvm-neutron/ac837a8/logs/testr_results.html.gz
2014-05-26 21:47:20.109 364 INFO neutron.wsgi [req-7c96bf86-6845-4143-92d0-2bb32f5767d7 None] (364) accepted ('127.0.0.1', 60250)
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:619
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:678
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1041
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Storing token in cache _cache_put /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1151
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 47d465f7c2e44c048f63066dff93093c with project_id : d3e7af8cf42d4613beb315dc19444d40 and roles: _member_ _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:940
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] No route matched for GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:97
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Matched GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Match dict: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>, 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2014-05-26 21:47:20.122 364 DEBUG neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to find ':' as separator in tenant_id. __call__ /opt/stack/new/neutron/neutron/policy.py:243
2014-05-26 21:47:20.123 364 ERROR neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.policy Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.policy File ""/opt/stack/new/neutron/neutron/policy.py"", line 239, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.policy parent_res, parent_field = do_split(separator)
2014-05-26 21:47:20.123 364 TRACE neutron.policy File ""/opt/stack/new/neutron/neutron/policy.py"", line 234, in do_split
2014-05-26 21:47:20.123 364 TRACE neutron.policy separator, 1)
2014-05-26 21:47:20.123 364 TRACE neutron.policy ValueError: need more than 1 value to unpack
2014-05-26 21:47:20.123 364 TRACE neutron.policy
2014-05-26 21:47:20.123 364 ERROR neutron.api.v2.resource [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] index failed
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 309, in index
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource return self._items(request, True, parent_id)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 264, in _items
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource request.context, obj_list[0])
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 145, in _exclude_attributes_by_policy
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource might_not_exist=True):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/policy.py"", line 346, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource return policy.check(*(_prepare_check(context, action, target)))
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 169, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource result = rule(target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 366, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource if rule(target, cred):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource File ""/opt/stack/new/neutron/neutron/policy.py"", line 261, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource reason=err_reason)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource PolicyCheckError: Failed to check policy tenant_id:%(tenant_id)s because Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource"
1,"When there is a delete_port event occassionally we are seeing a TRACE in dhcp_rpc.py file.
2014-10-07 12:31:39.803 DEBUG neutron.api.rpc.handlers.dhcp_rpc [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Update dhcp port {u'port': {u'network_id': u'12548499-8387-480e-b29c-625dbf320ecf', u'fixed_ips': [{u'subnet_id': u'88031ffe-9149-4e96-a022-65468f6bcc0e'}]}} from ubuntu. from (pid=4414) update_dhcp_port /opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py:290
2014-10-07 12:31:39.803 DEBUG neutron.openstack.common.lockutils [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Got semaphore ""db-access"" from (pid=4414) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-07 12:31:39.832 ERROR oslo.messaging.rpc.dispatcher [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Exception during message handling: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher 'update_port')
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher net_id = port['port']['network_id']
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher KeyError: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Returning exception 'network_id' to caller
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] ['Traceback (most recent call last):\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n incoming.message))\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n return self._do_dispatch(endpoint, method, ctxt, args)\n', ' File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n result = getattr(endpoint, method)(ctxt, **new_args)\n', ' File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port\n \'update_port\')\n', ' File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action\n net_id = port[\'port\'][\'network_id\']\n', ""KeyError: 'network_id'\n""]
2014-10-07 12:31:39.839 DEBUG neutron.context [req-7d40234b-6e11-4645-9bab-8f9958df5064 None None] Arguments dropped when creating context: {u'project_name': None, u'tenant': None} from (pid=4414) __init__ /opt/stack/neutron/neutron/context.py:83"
1,"It is possible to send two DELETE requests via api and in_use.floating_ips will be decreased by two. It can be changed even to value below zero:
Logs:
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0797279
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47613)
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0804379
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47615)
<0>Dec 19 17:52:08 node-2 <BF><180>nova-nova.db.sqlalchemy.api WARNING: Change will make usage less than 0 for the fol
lowing resources: ['floating_ips']
Database:
mysql> select resource,in_use from quota_usages;
+-----------------+--------+
| resource | in_use |
+-----------------+--------+
| security_groups | 0 |
| instances | 0 |
| ram | 0 |
| cores | 0 |
| fixed_ips | 0 |
| floating_ips | -1 |
+-----------------+--------+
6 rows in set (0.00 sec)"
0,"Current config.generator could not handle split configs for the different services within a project, all configurations be collected and save to a single large template file. But for most project, the code repo contains more then one service, like nova repo/project contains nova-api, nova-compute. So the single template file is hard to be used/maintained for separated service. IMO, it will be cool if config.generator could generate separated configs based on service instead of project.
Example of ""split configs"" is glance-api.conf versus glance-registry.conf"
0,"ovs-vsctl has a switch for returning json output, which is arguably better for machine processing, and therefore more suitable for neutron's OVS agent.
Indeed get_vif_port_set in neutron/agent/linux/ovs_lib.py already uses json output.
However, get_vif_port_by_id performs match on the text output from ovs_vsctl.
While it is true that regular expressions are extremely powerful, apparently trivial errors in the regular expression itself can lead to errors like [1].
In [1] the evaluation of a regular expression is failing because the name of a VIF was not wrapped in double quotes.
Even if the could be worth looking into ovs-vsctl output to understand in which cases VIF names are wrapped in quotes and in which not, it is probably better to just switch to JSON output as parsing JSON is easier and therefore less likely to cause errors.
It is also worth noting that the error in regex parsing [1] causes the VIF to not be wired triggering the same failure as bug 1253896.
[1] http://logs.openstack.org/49/63449/16/experimental/check-tempest-dsvm-neutron-isolated/de067c3/logs/screen-q-agt.txt.gz#_2014-02-16_11_46_14_830"
0,"In order to leverage distributed routing with the NSX plugin - the replication_mode parameter should be set to 'service'.
Otherwise the backend wil throw 409 errors resulting in 500 NSX errors.

This should be noted in the configuration files."
1,"Method:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L50
is used to add extension attributes to the router object during the handling of the API response. When attributes are unspecified, the router is extended with default values.
In the case of boolean attributes things don't work as they should, because a default value as True takes over on the right side of the boolean expression on:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L56
The end user so is led to believe that the server did not honor the request, when effectively it did."
0,"The OVS Agent unit tests do not prevent it from trying to report its state which eats up >10 seconds per unit test.
Traceback (most recent call last):
  File ""neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 260, in _report_state
    self.use_call)
  File ""neutron/agent/rpc.py"", line 70, in report_state
    return self.call(context, msg)
  File ""neutron/common/log.py"", line 34, in wrapper
    return method(*args, **kwargs)
  File ""neutron/common/rpc.py"", line 161, in call
    context, msg, rpc_method='call', **kwargs)
  File ""neutron/common/rpc.py"", line 187, in __call_rpc_method
    return func(context, msg['method'], **msg['args'])
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 389, in call
    return self.prepare().call(ctxt, method, **kwargs)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 194, in send
    return self._send(target, ctxt, message, wait_for_reply, timeout)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 186, in _send
    'No reply on topic %s' % target.topic)
MessagingTimeout: No reply on topic q-plugin"
1,"This string: ""Disabled reason contains invalid characters or is too long""
found in
nova/api/openstack/compute/contrib/services.py:177, nova/api/openstack/compute/plugins/v3/services.py:159
should be fixed.
From the spanish translation team:
Se recomienda que esta cadena sea reestructurada en su versi贸n original, particularmente ""Disabled reason"".
Essentially, the string isn't the best English and should be re-written so it can be translated properly - particularly the use of ""Disabled reason""."
0,The Big Switch unit tests use Magic Mocks to stop external calls (e.g. HTTP and RPC). This is a waste of memory most of the time since no assertions are made on the Mocks.
1,"If a router is removed from the list passed to update_routers(), the iptables_driver removes the labels for the last(?) router passed, not the one removed."
0,"After commit commit 466e89970f11918a809aafe8a048d138d4664299 migrations should not anymore explicitly specify the engine used for mysql.

There are still some migrations which do that, and they should be amended."
0,"Virtual devices need a unit number when they are attached to a controller. We cannot have two devices on the same controller with the same unit number.
Currently, the selection of unit numbers is spread all over the driver code, leaking to high-level functions like spawn() and rescue(). We need to factor this out into helper functions which take care of choosing a proper unit number and creating additional controllers if needed.
High-level functions need to communicate only the intent like 'attach CDROM' or 'attach disk' and shouldn't bother with details like unit numbers."
0,"diff --git a/cinder/tests/test_backup_tsm.py b/cinder/tests/test_backup_tsm.py
index 37d528f..2548cbd 100644
--- a/cinder/tests/test_backup_tsm.py
+++ b/cinder/tests/test_backup_tsm.py
@@ -109,8 +109,7 @@ class TSMBackupSimulator:
             ret_msg += ('Total number of objects deleted: 1\n'
                         'Total number of objects failed: 0')
             retcode = 0
- for idx, backup in enumerate(self._backup_list[path]):
- index = idx
+ index = len(self._backup_list[path]) - 1
             del self._backup_list[path][index]
             if not len(self._backup_list[path]):
                 del self._backup_list[path]"
0,"disassociate_address does not return the appropriate EC2 response when disassociating an unassociated address. Despite this seeming failure, EC2 will respond success.
EC2 will return as such:
<DisassociateAddressResponse xmlns=""http://EC2.amazonaws.com/doc/2012-08-15/"">
    <requestId>aabbccdd-0146-4952-bdbe-710e4fee8387</requestId>
    <return>true</return>
</DisassociateAddressResponse>
yet, when the EC2 api encounters this scenario, it responds with a 400:
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0""?>
<Response><Errors><Error><Code>InvalidInstanceID.NotFound</Code><Message>Instance None could not be found.</Message></Error></Errors><RequestID>req-7a9b4d03-b9f5-4243-95d8-f1ccab180631</RequestID></Response>
The EC2 api should evaluate whether it receives an instance_id from:
instance_id = self.network_api.get_instance_id_by_floating_address(context, public_ip)
and, if so, then continue attempting to disassociate. Else, bail out early and return successfully."
0,"I found this error in jinkins's log: http://logs.openstack.org/02/67402/4/check/gate-nova-python27/b132ac8/console.html
2014-01-20 02:36:59.295 | ======================================================================
2014-01-20 02:36:59.295 | FAIL: nova.tests.compute.test_compute.ComputeVolumeTestCase.test_poll_volume_usage_with_data
2014-01-20 02:36:59.295 | tags: worker-0
2014-01-20 02:36:59.296 | ----------------------------------------------------------------------
2014-01-20 02:36:59.296 | Empty attachments:
2014-01-20 02:36:59.296 | stderr
2014-01-20 02:36:59.296 | stdout
2014-01-20 02:36:59.297 |
2014-01-20 02:36:59.297 | pythonlogging:'': {{{
2014-01-20 02:36:59.297 | INFO [nova.virt.driver] Loading compute driver 'nova.virt.fake.FakeDriver'
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Auditing locally available compute resources
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free ram (MB): 7680
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free disk (GB): 1028
2014-01-20 02:36:59.299 | AUDIT [nova.compute.resource_tracker] Free VCPUS: 1
2014-01-20 02:36:59.299 | INFO [nova.compute.resource_tracker] Compute_service record created for fake-mini:fakenode1
2014-01-20 02:36:59.299 | AUDIT [nova.compute.manager] Deleting orphan compute node 2
2014-01-20 02:36:59.300 | }}}
2014-01-20 02:36:59.300 |
2014-01-20 02:36:59.300 | Traceback (most recent call last):
2014-01-20 02:36:59.300 | File ""nova/tests/compute/test_compute.py"", line 577, in test_poll_volume_usage_with_data
2014-01-20 02:36:59.301 | self.compute._last_vol_usage_poll)
2014-01-20 02:36:59.301 | File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2014-01-20 02:36:59.301 | raise self.failureException(msg)
2014-01-20 02:36:59.302 | AssertionError: _last_vol_usage_poll was not properly updated <1390185067.18>"
0,missing code coverage in port_security tests
0,"Change-Id I663bd06eb50872f16fc9889dde917277739fefce introduced a race condition where if another test doesn't properly reset the _IS_NEUTRON flag, it will fail because it will think that it is using Neutron and error out."
1,"The resize operation when using the VCenter driver ends up resizing the original VM and not the newly cloned VM.
To recreate:
1) create a new VM from horizon using default debian image. I use a flavor of nano.
2) wait for it to complete and go active
3) click on resize and choose a flavor larger than what you used originally. i then usually choose a flavor of small.
4) wait for horizon to prompt you to confirm or revert the migration.
5) Switch over to vSphere Web Client. Notice two VMs for your newly created instance. One with a UUID name and the other with a UUID-orig name. ""-orig"" indicating the original.
6) Notice the original has be resized (cpu and mem are increased, disk is not, but that's a separate bug) and not the new clone. This is problem #1.
7) Now hit confirm in horizon. It works, but the logs contain a warning: ""The attempted operation cannot be performed in the current state (Powered on)."". I suspect its attempting to destroy the orig VM, but the orig was the VM resized and powered on, so it fails. This is problem #2.
Results in a leaked VM."
1,"We have following code in neutron.test.unit.L3NatTestCaseMixin.floatingip_with_assoc get ""set_context"" from external but not use it.
    @contextlib.contextmanager
    def floatingip_with_assoc(self, port_id=None, fmt=None, fixed_ip=None,
    ######################################
                              set_context=False): # <---- We get set_context here
    ######################################
        with self.subnet(cidr='11.0.0.0/24') as public_sub:
            self._set_net_external(public_sub['subnet']['network_id'])
            private_port = None
            if port_id:
                private_port = self._show('ports', port_id)
            with test_db_plugin.optional_ctx(private_port,
                                             self.port) as private_port:
                with self.router() as r:
                    sid = private_port['port']['fixed_ips'][0]['subnet_id']
                    private_sub = {'subnet': {'id': sid}}
                    floatingip = None
                    self._add_external_gateway_to_router(
                        r['router']['id'],
                        public_sub['subnet']['network_id'])
                    self._router_interface_action(
                        'add', r['router']['id'],
                        private_sub['subnet']['id'], None)
                    floatingip = self._make_floatingip(
                        fmt or self.fmt,
                        public_sub['subnet']['network_id'],
                        port_id=private_port['port']['id'],
                        fixed_ip=fixed_ip,
    ######################################
                        set_context=False) ### <---- But we don't really use it
    ######################################
                    yield floatingip"
1,"The server manager module references an attribute of a variable that could either be an HTTPSConnection object or None. If it's None, the attribute reference will fail."
1,"when I use ""nova image-create my_server_id name"" to create an image, the log /var/log/glance/api.log record the below texts:
<glance.common.wsgi.Resource object at 0x38a7150>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-23 20:24:19.168 20884 INFO glance.registry.api.v1.images [0f0f1d19-9d63-4f2a-88d1-f48c0e28f6f4 e0f87f6761614850b064f9717ac83a36 3b520afd03d94532b64fef4d863230f6] Successfully created image None
but the image id in DB is not None but an uuid."
0,"For example, see https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L664 and https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L666"
1,"I observed that when you have publish_error enabled and and the notification driver set as below:
notification_driver = cinder.openstack.common.notifier.rpc_notifier
publish_errors = true
You will get the following error when a purposely causing a error in cinder(for example, create volume from image and set the volume size something small that the image would not fit in)
2014-07-21 11:46:51.527 ERROR oslo.messaging.rpc.dispatcher [req-e3015168-bf95-4cea-af97-b13ed9edc141 231fdb65b4134c9b864767e851ad72db 5304a70aac9540a5b445cb6a6f62c917] Exception during message handling: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/manager.py"", line 337, in create_volume
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher _run_flow()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/manager.py"", line 330, in _run_flow
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher flow_engine.run()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 89, in run
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher for _state in self.run_iter():
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self._change_state(states.FAILURE)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/openstack/common/excutils.py"", line 82, in __exit__
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 120, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher for state in runner.run_iter(timeout=timeout):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/runner.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher misc.Failure.reraise_if_any(failures)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 788, in reraise_if_any
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher failures[0].reraise()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 795, in reraise
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher six.reraise(*self._exc_info)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 48, in _revert_task
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher result = task.revert(**kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 193, in revert
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher LOG.error(_(""Volume %s: create failed""), volume_id)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 1449, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self.logger.error(msg, *args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self._log(ERROR, msg, args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self.callHandlers(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher hdlr.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher self.emit(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher File Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 197, in main
    self._resolve_links()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 212, in _resolve_links
    f(self, *ca, **ckw)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_executors/impl_eventlet.py"", line 47, in complete
    thread.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
    return self._exit_event.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 120, in wait
    current.throw(*self._exc)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 129, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 143, in _dispatch_and_reply
    exc_info=exc_info)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
    self._log(ERROR, msg, args, **kwargs)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
    self.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
    self.callHandlers(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
    hdlr.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
    self.emit(record)
  File ""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
    dict(error=record.msg))
TypeError: info() takes exactly 4 arguments (3 given)""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher dict(error=record.msg))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher TypeError: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher
I see that in /opt/stack/cinder/cinder/openstack/common/log_handler.py that that the notifier.info(...) code(line #28) is not passing in the context as the first parameter as expected in the API thus the takes 4 or 3 given error above.
rpc.get_notifier('error.publisher').info('error_notification', dict(error=record.msg))"
1,"A GuestFS error is causing injection to fail. This result in a warning for metadata injection but results in a spawn error for key injection.
This is logged with debug level:
Exception AttributeError: ""GuestFS instance has no attribute '_o'"" in <bound method GuestFS.__del__ of <guestfs.GuestFS instance at 0x3ea9f38>> ignored
febootstrap-supermin-helper: ext2: parent directory not found: /lib: File not found by ext2_lookup
And causes this error: http://paste.openstack.org/show/62293/
Full logs available here: http://logs.openstack.org/58/63558/8/check/check-tempest-dsvm-neutron-pg/108e4ca/logs
Interestingly, it seems guestfs was not actually used when the relevant patches went throught the gate checks:
https://review.openstack.org/#/c/70237/
https://review.openstack.org/#/c/70354/
This was expected for patch #70354 but sounds strange for patch #70237
Finally, The traceback seeems different from that of bug 1221985"
1,"from neutron.common import utils
print utils.str2dict('inside_addr=10.0.1.2,inside_port=22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp')
returns
{'inside_addr': '10.0.1.2', 'inside_port': '22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp'}
expected value should be
{'outside_port': '2222', 'inside_addr': '10.0.1.2', 'protocol': 'tcp', 'inside_port': '22', 'outside_addr': '172.16.0.1'}
The reason is that in the third line of the implementation below, string.split(',', 1) only splits out two key-value pairs.
quote from neutron/common/utils.py:181:
def str2dict(string):
    res_dict = {}
    for keyvalue in string.split(',', 1):
        (key, value) = keyvalue.split('=', 1)
        res_dict[key] = value
    return res_dict
a quick fix might be remove "",1"" from string.split. But it turns out that str2dict/dict2str may also fail when input values containing characters like '=' or ','. A better fix might be using json encode/decode to deal with it."
0,"Create aggregate fails when input metadata ""availability_zone"" length >255(models.py value = Column(String(255), nullable=False)),Exception is raised but record in DB is not rolled back."
0,"With introduction of multi-segment support in ml2 plugin, agent misconfiguration could happen under exceptionnal circumstances:
Supposing a multi-segment provider network is created with different network types (suppose a flat and a vlan segment), and two ports are bound on an agent supporting the associated physical network.
Portbinding validation will occur on network's segments list returned by db.get_network_segments funtion . As this function may not always returns segments in the same order, one port may be bound to the flat segment while the other will be bound to vlan one.
In that case, ports wouldn't be properly plugged in agent, as they'd recieve two contradictory segment details for the same network.
OVS agent would probably bound the two ports within the same segment as they both would use the same LocalVLANMapping
LB agent would probably add two uplink interfaces under the same qbr[net-id] bridge"
0,Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177. Now table ml2_brocadenetworks has foreign key and downgrade of 492a106273f8_brocade_ml2_mech_dri fails. Error log: http://paste.openstack.org/show/88898/
0,"I noticed when performing a ""nova image-show"" on a current (not deleted) image, two HTTP requests were issued. Why isn't the Image retrieved on the first GET request?
In fact, it is. The problem lies in _extract_attributes(), called by _translate_from_glance(). This function loops through a list of expected attributes, and extracts them from the passed-in Image. The problem is that if the attribute 'deleted' is False, there won't be a 'deleted_at' attribute in the Image. Not finding the attribute results in getattr() making another GET request (to try to find the ""missing"" attribute?). This is unnecessary of course, since it makes sense for the Image to not have that attribute set."
1,"In neutron/plugins/cisco/cfg_agent/device_drivers/driver_mgr.py, if an exception occurs loading the Cfg Agent, a log message is created and it causes a second exception due to incorrect key used to access template name."
1,"We've hit this foreign key constraint error. This is due to a sync message coming in from the L3 agent over RPC. The message contains n update for a port that has just been deleted. This is just log noise because it all gets worked out quickly following the error. The full trace is here [1].
2014-09-18 21:22:39.735 29984 TRACE oslo.messaging.rpc.dispatcher DBReferenceError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`neutron`.`ml2_dvr_port_bindings`, CONSTRAINT `ml2_dvr_port_bindings_ibfk_1` FOREIGN KEY (`port_id`) REFERENCES `ports` (`id`) ON DELETE CASCADE)') 'INSERT INTO ml2_dvr_port_bindings (port_id, host, router_id, vif_type, vif_details, vnic_type, profile, cap_port_filter, driver, segment, status) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)' ('0fe7b532-343e-4ba0-83d9-c51b1c55f533', 'devstack-trusty-hpcloud-b4-2246632', '45248fa2-4372-4ad9-8e60-afabe39c6f6a', 'unbound', '', 'normal', '', 0, None, None, 'DOWN')
[1] http://paste.openstack.org/show/113360/"
0,"The HP Lefthand CLIQ proxy may return incorrect total_capacity_gb, and free_capacity_gb values if there are more than one cluster configured in the management group. The call to getCluster in _update_backend_status should include the cluster name."
0,"This feature will cause an ACPI event to be sent to the system while shutting down, and the acpid running inside the system can catch the event, thus giving the system a chance to shutdown cleanly.
[Impact]
 * VMs being shutdown with any signal/notification from the The hypervisor level, services running inside VMs have no chance to perform a clean shutoff
[Test Case]
 * 1. stop a VM
   2. the VM is shutdown without any notification
The can be easily seen by ssh into the system before shutting down. With the patch in place, the ssh session will be close during shutdown, because the sshd has the chance to close the connection before being brought down. Without the patch, the ssh session will just hang there for a while until timeout, because the connection is not promptly closed.
To leverage the clean shutdown feature, one can create a file named /etc/acpi/events/power that contains the following:
              event=button/power
              action=/etc/acpi/power.sh ""%e""
Then create a file named /etc/acpi/power.sh that contains whatever required to gracefully shutdown a particular server (VM).
With the apicd running, shutdown of the VM will cause the rule in /etc/acpi/events/power to trigger the script in /etc/acpi/power.sh, thus cleanly shutdown the system.
[Regression Potential]
 * none
Currently in libvirt stop and delete operations simply destroy the underlying VM. Some GuestOS's do not react well to this type of power failure, and it would be better if these operations followed the same approach a a soft_reboot and give the guest a chance to shutdown gracefully. Even where VM is being deleted, it may be booted from a volume which will be reused on another server."
1,"In an exception handling case a config error is raised; however, the call is currently formatted incorrectly. It passes the details as a second parameter rather than performing the string formatting in place.
It also refers to strerror, which may not be present on all exception types that could be raised during the certificate retrieval.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L358"
1,"The Nova Docker driver doesn't seem to respect the CPU limit defined by nova.
Please note I just reviewed the latest source code, haven't tested the behavior.
My assumption is that a docker container has access to all CPU resources by default.

On the docker command line this can be handled:

    docker run -c=<relative-weight> centos"
0,"In order to support VDX/NOS version greater than 4.1.0, NETCONF template needs to be enhanced. However it is necessary to support all versions of NOS, hence a run time check of the NOS version should be made and the appropriate template should be used."
0,"When the Big Switch plugin first starts, it doesn't have a consistency hash yet, so it is currently setting the HTTP header to 'False'. This requires special-casing on the backend to handle. It should just be blank."
1,"When L3 agent is restarted, it destroys all existing namespaces and then recreates them. This causes a network outage for the affected routers and floating IPs, even if those routers/floating IPs are still valid. We should be able to preserve existing, valid namespaces across an agent restart and avoid the network outage."
0,"cinder/tests/test_volume.py
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_driver
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_host
The location_info is incorrect, it needs to be 5 sections separated by colons, which means that the following condition does not get tested
691 if (dest_type != 'LVMVolumeDriver' or dest_hostname != self.hostname):
692 return false_ret"
0,"The NSX sync backend previously passed around a sqlalchemy model object
around which was nice because we did not need to query the database an
additional time to update the status of an object. Unfortinately, this
was add done within a db transaction which included a call to NSX which
could cause deadlock this needed to be removed."
0,"A recent patch removed the explicit patch stops from most of the unit tests to let the default mock.patch.stopall handle the cleanup[1]. However, there appears to be cases where stopall loses the reference to a patch and doesn't stop it correctly. In this particular case, the nexus test patch to sys.modules is not being stopped correctly so it's causing unit test failures in the gate[2].
1. https://github.com/openstack/neutron/commit/6546ba570367aba7209ec36544ec4cf742e0bd63
2. http://logs.openstack.org/10/86810/5/check/gate-neutron-python27/89dfa5c/testr_results.html.gz"
1,"Few issues with LXC and volumes that relate to the same code.
* Hard rebooting a volume will make attached volumes disappear from libvirt xml
* Booting an instance specifying an extra volume (passing in block_device_mappings on server.create) will result in the volume not being in the libvirt xml
This is due to 2 places in the code where LXC is treated differently
1. nova.virt.libvirt.blockinfo get_disk_mapping
2. nova.virt.libvirt.driver get_guest_storage_config"
1,"Steps to reproduce:
- Setup a devstack from scratch using nova-network
- delete the default network
  # nova-manage network delete 10.0.0.0/24
- change nova.conf to use VlanManager:
network_manager = nova.network.manager.VlanManager
- restart nova-network
- create a new network with a vlan id:
nova-manage network create --label=network --fixed_range_v4 10.0.1.0/24 --vlan 42
- boot a vm on the cirros image:
nova --debug boot --flavor 1 --image 0b969819-2d85-4f7f-af76-125c5bb5789f test
Expected behavior: The new VM goes to Active state
Actual behavior: The new VM goes to Error state, also nova-network log has this exception:
a7-abaf-78db50a4b62c] network allocations from (pid=13676) allocate_for_instance /opt/stack/nova/nova/network/manager.py:494
2014-04-07 15:32:02.137 ERROR nova.network [req-87a65a9e-9196-4203-9de2-f6911d2aef4b admin demo] No db access allowed in nova-network: File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 128, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/network/floating_ips.py"", line 119, in allocate_for_instance
    **kwargs)
  File ""/opt/stack/nova/nova/network/manager.py"", line 497, in allocate_for_instance
    requested_networks=requested_networks)
  File ""/opt/stack/nova/nova/network/manager.py"", line 1837, in _get_networks_for_instance
    networks = self.db.project_get_networks(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1370, in project_get_networks
    return IMPL.project_get_networks(context, project_id, associate)
  File ""/opt/stack/nova/nova/cmd/network.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())
I think the exception was introduced by this patch that disables direct database access from nova-network: https://review.openstack.org/#/c/79716/
However, VlanManager still relies on database access for the given scenario, and there are 3 other places in manager.py that rely on direct db access:
devuser@ubuntu:/opt/stack/nova$ grep self.db nova/network/manager.py -n
1389: vifs = self.db.virtual_interface_get_by_instance(context,
1446: vif = self.db.virtual_interface_get_by_address(context,
1837: networks = self.db.project_get_networks(context, project_id)
1914: not self.db.network_in_use_on_host(context, network['id'],
Therefore, I cannot currently use conductor with nova-network VlanManager, which is a regression from Havana.
===
devstack defaults the network_manager to FlatDHCPManager so we don't test VlanManager in the gate."
1,"I am using config drive to boot VMs. In icehouse, I observed that nova rescue fails and leaves the VM in SHUTOFF state.

Short error log:
instances/270e299b-90b2-46d5-bf9a-e7f6efe3742e/disk.config.rescue': No such file or directory

Difference in Havana and Icehouse code path:

# Havana
# Config drive
 if configdrive.required_by(instance):
      LOG.info(_('Using config drive'), instance=instance)
      extra_md = {}
      if admin_pass:
          extra_md['admin_pass'] = admin_pass

      for f in ('user_name', 'project_name'):
         if hasattr(context, f):
             extra_md[f] = getattr(context, f, None)
         inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
         with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
             configdrive_path = basepath(fname='disk.config')
             LOG.info(_('Creating config drive at %(path)s'),
                      {'path': configdrive_path}, instance=instance)

def basepath(fname='', suffix=suffix): << Adds suffix .rescue to disk.config.
    return os.path.join(libvirt_utils.get_instance_path(instance),
                                fname + suffix)

# Icehouse:
# Config drive
if configdrive.required_by(instance):
    LOG.info(_('Using config drive'), instance=instance)
    extra_md = {}
    if admin_pass:
        extra_md['admin_pass'] = admin_pass

    for f in ('user_name', 'project_name'):
        if hasattr(context, f):
            extra_md[f] = getattr(context, f, None)
        inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
        with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
            configdrive_path = self._get_disk_config_path(instance)
            LOG.info(_('Creating config drive at %(path)s'),
                         {'path': configdrive_path}, instance=instance)

@staticmethod
def _get_disk_config_path(instance):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                        'disk.config')

The suffix .rescue is missed here and hence, original disk.config is overwritten.

Following change fixed the issue for me:

configdrive_path = self._get_disk_config_path(instance, suffix)

@staticmethod
def _get_disk_config_path(instance, suffix=''):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                            'disk.config' + suffix)"
1,"devstack, `nova --version` reports 2.15.0.81
VIRT_DRIVER=docker
database: PostgreSQL

when starting nova-compute I'm getting an exception:

2013-11-20 11:41:27.413 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=15320) inner /opt/stack/nova/nova/openstack/common/lockutils.py:251
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/queue.py"", line 107, in switch
    self.greenlet.switch(value)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 164, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 788, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5065, in update_available_resource
    rt.update_available_resource(context)
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 248, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 326, in update_available_resource
    self._sync_compute_node(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 349, in _sync_compute_node
    self._create(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 365, in _create
    values)
  File ""/opt/stack/nova/nova/conductor/api.py"", line 236, in compute_node_create
    return self._manager.compute_node_create(context, values)
  File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 356, in compute_node_create
    return cctxt.call(context, 'compute_node_create', values=values)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
    return self._invoke(self.proxy.call, ctxt, method, **kwargs)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
    return cast_or_call(ctxt, msg, **self.kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
    result = rpc.call(context, real_topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
    return _get_impl().call(CONF, context, topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
    rpc_amqp.get_connection_pool(conf, Connection))
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
    rv = list(rv)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
    raise result
RemoteError: Remote error: DBError (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...L, NULL, 0, 5, 1, 11953, 54, 0, 512, 0, 'docker', '1.0', 'hg...
                                                             ^

(should be in fixed-width font, the circumflex accent points to '1.0')"
1,"liugya@liugya-ubuntu:~/src/nova-ce$ nova aggregate-set-metadata 1 a=a1
Aggregate 1 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 1 | agg1 | None | | 'a=a1' |
+----+------+-------------------+-------+----------+
liugya@liugya-ubuntu:~/src/nova-ce$ nova aggregate-set-metadata 1 a
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(100)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(101)update_metadata()
-> ""updatemetadata.start"",
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(102)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {u'a': None}, 'aggregate_id': 1}
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(118)update_metadata()
-> payload['meta_data'] = to_add
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(119)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(120)update_metadata()
-> ""updatemetadata.end"",
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(121)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {}, 'aggregate_id': 1} <<< meta_data is empty, this caused 3rd party do not know which meta_data was now removed after get notification of updatemetadata.end"
1,"nova.servicegroup.drivers.db.DbDriver._report_state() is called every service.report_interval seconds from a timer in order to periodically report the service state. It calls self.conductor_api.service_update().
If this ends up calling nova.conductor.rpcapi.ConductorAPI.service_update(), it will do an RPC call() to nova-conductor.
If anything happens to the RPC server (failover, switchover, etc.) by default the RPC code will wait 60 seconds for a response (blocking the timer-based calling of _report_state() in the meantime). This is long enough to cause the status in the database to get old enough that other services consider this service to be ""down"".
Arguably, since we're going to call service_update( ) again in service.report_interval seconds there's no reason to wait the full 60 seconds. Instead, it would make sense to set the RPC timeout for the service_update() call to to something slightly less than service.report_interval seconds.
I've also submitted a related bug report (https://bugs.launchpad.net/bugs/1368917) to improve RPC loss of connection in general, but I expect that'll take a while to deal with while this particular case can be handled much more easily."
1,"I do not have concreate steps to reporduce this but following is the traceback from the logs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free ram (MB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:388^[[00m
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free disk (GB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: VCPU information unavailable^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:396^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: no assignable PCI devices^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:403^[[00m"
1,"When metadata networking is enabled, neutron will generate a dnsmasq configuration that includes classless routes.
The RFC states that those routes should include the default route if they are defined, and that the DHCP client should ignore the default router.
This causes standards-abiding DHCP clients like dhcpcd to not provide a default gateway when running on nova.
At https://github.com/openstack/neutron/pull/22 you can see an example of how to fix this."
1,"When L3 agent starts or restarts, it almost immediately goes in to a _sync_routers_task run. This task is synchronized with _rpc_loop so that only one can happen at a time.
The problem with this is that -- at least at scale -- the _sync_routers_task can take a VERY LONG time to run. I've observed it take 1-2 hours! This is WAY too long to wait before I can do something with my router like add a floating ip.
The thing is, _sync_routers_task is important to do periodically but it is mostly just checking that things are still in the right state. It should never take precedence over responding to RPC messages. The RPC messages represent work that the system has just been asked to perform. It is silly to make it wait a long time for a maintenance task to complete."
1,"  if node.hasAttribute(""access_ipv4""):
            rebuild[""access_ip_v4""] = node.getAttribute(""access_ip_v4"")

        if node.hasAttribute(""access_ipv6""):
            rebuild[""access_ip_v6""] = node.getAttribute(""access_ip_v6"")

access_ipv4 should be access_ip_v4

We can't merge this patch https://review.openstack.org/#/c/41349/
So we need fix current code."
1,"I found that when I run:
% nova volume-detach my_instance c54ad11f-4e51-41a0-97db-7e551776db59
where the volume with given id is currently attached to my running instance named my_instance, the operation completes successfully. Nevertheless a subsequent attach of the same volume again will fail. So:
% nova volume-attach my_instance c54ad11f-4e51-41a0-97db-7e551776db59 /dev/sdb
fails with the error that the volume's vmdk file is not found.
Cause:
During volume detach a delete_virtual_disk_spec is used to remove the device from the running instance. This spec also ""destroy""s the underlying vmdk file. The offending line is : https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vm_util.py#L471
Possible fix:
The fileOperation field of the device config during this reconfigure operation should be left unset. We should continue setting device_config.operation field to ""remove"". This will remove the device from the VM without deleting the underlying vmdk backing."
1,"If libvirt wasn't compiled with the appropriate flags, listDevices may return NOT_SUPPORTED.
This is currently not gracefully handled, which prevents the compute node from starting up.
http://paste.openstack.org/show/75375/
A better fix is to handle the error, and issue a warning when we detect this case."
1,"The Big Switch servermanager records the consistency hash to the database every time it gets updated but it does not retrieve the latest value from the database whenever it includes it in an HTTP request. This is fine in single neutron server deployments because the cached version on the object is always the latest, but this isn't always the case in HA deployments where another server updates the consistency DB."
1,"I've been seeing this in the nova scheduler log a lot this week when looking through gate and check queue failures:
http://paste.openstack.org/show/53791/
Looks like it's probably due to a bad translation.
(9:31:29 PM) clarkb: mriedem: yes that is the sort of traceback that should be fixed. It is possible that that particular error is due to a bad translation
(9:31:58 PM) clarkb: since I think we are using local=en_US on the slaves and en is the source locale
http://logs.openstack.org/42/56642/2/gate/gate-tempest-devstack-vm-neutron/a712b82/logs/screen-n-sch.txt.gz"
0,"The N1kv plugin includes the 'fields' argument in the call to the super class function. Hence, only the specified fields are returned by the get_<..> function in the super class.
The problem is that a field that has been removed may be needed in the subsequent processing in the N1kv's get_<...> function.
The (trivial) solution is to set the fields argument to None in the call to the super class function.
A patch with the solution will be submitted momentarily."
1,"Using Devstack (latest master 1/9/2012)

If I enable multi process metadata service (metadata_workers=5 in nova.conf)

the iptables are modified multiple times:
2013-01-10 00:25:16.0 25535 INFO nova.wsgi [-] metadata listening on 0.0.0.0:8775
2013-01-10 00:25:16.1 25535 INFO nova.service [-] Starting 5 workers
2013-01-10 00:25:16.3 25535 INFO nova.service [-] Started child 255442013-01-10 00:25:16.6 25535 INFO nova.service [-] Started child 255452013-01-10 00:25:16.8 25535 INFO nova.service [-] Started child 25546
2013-01-10 00:25:16.9 25544 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.10 25544 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189"
1,"These methods that handle block device mapping pass instance['name'] to attach/detach_volume(), but it should be instance itself.

    def _attach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.attach_volume(
                    connection_info, instance['name'], mountpoint)

    def _detach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.detach_volume(
                    connection_info, instance['name'], mountpoint)"
1,"On a successful test run cinder vol and api services are emmitting tons of oslo.messaging errors in their consoles of the form:
2014-03-18 23:16:34.316 27093 ERROR oslo.messaging.notify._impl_messaging [req-ecce1edc-8383-4fe9-ad11-704ef9deb1e8 fee2f4dc1ff648f0a37e2ef527d9fc20 39971f7616a447e5ad26dbe3ac202a32 - - -] Could not send notification to notifications. ... <payload>
This can be seen in a live log at: http://logs.openstack.org/95/80895/3/check/check-tempest-dsvm-full/bf79bbb/console.html#_2014-03-18_23_20_29_508
This ends up being hugely scary for deployers, and it's lots of ERRORs in logs, even when things are working perfectly normally."
1,"swift-ring-builder command shows nasty stacktraces if the builder file is empty or invalid.
If, its empty file, throws EOF error. if a file is corrupted, throws unpickling error.
Also, during these cases throws an error code of 1, which is wrong since the chosen values for swift-ring-builder is
Exit codes: 0 = operation successful
            1 = operation completed with warnings
            2 = error
-- empty file--
[keshava@Kbook tmp]$ >object.builder
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
EOFError
[keshava@Kbook tmp]$ echo $?
1
-- a corrupted file --
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
cPickle.UnpicklingError: invalid load key, 'n'.
[keshava@Kbook tmp]$ echo $?
1
The error codes needs to be corrected, and also a meaningful user information should be provided if a file is empty or corrupted"
0,PostgreSQL connection strings may contain passwords too which need to be filtered out (like it's done for MySQL).
0,"list operations in ovs_lib returns an empty list even if Runtime Error occurs.
As a result, when Runtime Error occurs, a caller thinks all ovs ports have disappeared.
ovs-vsctl sometimes fails (mostly raises alarm error?)
ovs_lib should provide a way to distinguish these two situations.

list operations in ovs_lib
- get_vif_port_set (used by OVS agent and ryu agent)
- get_vif_ports (used by NEC agent, OVS cleanup)
- get_bridge (used by OVS agent, OVS cleanup)

It affects all agent using the above list operation.

It affects OVS agent and NEC agent. It triggers unexpected port deletions.

In OVS cleanup, there is no negative effect. It just nothing for this case."
1,"Updating the IKE policy after the site creation fails stating ike policy in already in use
Updating IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: url: http://<controller_ip>:9696/v2.0/vpn/ikepolicies/07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: X-Auth-Token: 86c8be3ce0204aa8bdd1458021eacec4
_PUT-REST: data: {""ikepolicy"":{""name"": ""IKE1"",""encryption_algorithm"": ""aes-256""}}
{u'NeutronError': {u'message': u'IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5 is still in use', u'type': u'IKEPolicyInUse', u'detail': u''}}
None"
1,The docstring for _send_delete_port_request wasn't properly updated as part of bug/1373547 to reflect that the last port check and subsequent call to delete_vm_network were removed. Need to update this docstring to reflect that change.
1,"The Hyper-V agent is currently enabling metrics collection per vm, instead of per disk.
This leads to erroneous values collected for disk metrics."
1,"by setting nova.conf:
compute_driver = fake.FakeDriver
I got this error on conductor when nova-compute update_available_resource :
2014-02-26 08:48:04.631 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
                                                             ^
 'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014, 2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/conductor/manager.py"", line 466, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher result = self.db.compute_node_update(context, node['id'], values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/db/api.py"", line 228, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher return IMPL.compute_node_update(context, compute_id, values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 166, in wrapped
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher compute_ref.update(values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher self.commit()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher self._prepare_impl()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher self.session.flush()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher File ""/home/croisets/stack/nova/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher raise exception.DBError(e)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher DBError: (DataError) invalid input syntax for integer: ""1.0""
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher ^
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher 'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014,
2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}"
1,"This is not really useful:
http://logs.openstack.org/17/123917/2/check/check-tempest-dsvm-neutron/4bc2052/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-09-25_17_35_11_635
2014-09-25 17:35:11.635 ERROR nova.virt.libvirt.driver [req-50afcbfb-203e-454d-a7eb-1549691caf77 TestNetworkBasicOps-985093118 TestNetworkBasicOps-1055683132] [instance: 960ee0b1-9c96-4d5b-b5f5-be76ae19a536] detaching network adapter failed.
2014-09-25 17:35:11.635 27689 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher incoming.message))
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/nova/nova/compute/manager.py"", line 393, in decorated_function
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher return function(self, context, *args, **kwargs)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4411, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher self.driver.detach_interface(instance, condemned)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 1448, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher raise exception.InterfaceDetachFailed(instance)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher InterfaceDetachFailed: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher
The code is logging that there was an error, but not the error itself:
        try:
            self.vif_driver.unplug(instance, vif)
            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
            if state == power_state.RUNNING or state == power_state.PAUSED:
                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
            virt_dom.detachDeviceFlags(cfg.to_xml(), flags)
        except libvirt.libvirtError as ex:
            error_code = ex.get_error_code()
            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
                LOG.warn(_LW(""During detach_interface, ""
                             ""instance disappeared.""),
                         instance=instance)
            else:
                LOG.error(_LE('detaching network adapter failed.'),
                         instance=instance)
                raise exception.InterfaceDetachFailed(
                        instance_uuid=instance['uuid'])
We should log the original libvirt error."
0,"This is a spin-off of https://bugs.launchpad.net/nova/+bug/1347028

As per the example given there - currently source=blank, destination=volume will not work. We should either make it create an empty volume and attach it, or disallow it in the API."
1,"The EC2 API allows the setting of the InstanceInstantiatedShutdownBehavior to either ""stop"" or ""terminate"".
This settings is ignored and all instances go to a stop state rather than termination.

From the comments, it looks like this was removed in I91845a64 -- maybe Vish can comment before I dive in?"
0,"When creating an instance and assigning a public network to it without admin authority, ExternalNetworkAttachForbidden
will be raised. But this exception is not handled in V3 api.

2014-08-07 19:40:55.032 ERROR nova.api.openstack.extensions [req-a3a824a2-d477-4720-98c7-d3161de268ba demo demo] Unexpected exception in API method
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions return f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/validation/__init__.py"", line 39, in wrapper
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions return func(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 507, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions **create_kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/hooks.py"", line 131, in inner
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions rv = f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 1351, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions legacy_bdm=legacy_bdm)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 967, in _create_instance
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 734, in _validate_and_build_base_options
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions requested_networks, max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/compute/api.py"", line 447, in _check_requested_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 709, in validate_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions neutron=neutron)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 169, in _get_available_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions network_uuid=net['id'])
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions ExternalNetworkAttachForbidden: It is not allowed to create an interface on external network 447d82c5-bf58-4f39-ac2f-a30227a464e2"
1,"volume cloning seems to fail when using qpid if some metadata is passed, see the following:
 # cinder create --metadata 'Type=Test' --source-volid d1ce1fc3-10fe-475f-9dc4-51261d04c323 1
 ERROR: The server has either erred or is incapable of performing the requested operation.
interestingly, this does not seem to happen when a new (non clone) volume is created
cinder api.log reports some rather long traceback:
 InternalError: Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 511, in dispatch
    self.engine.dispatch()
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 815, in dispatch
    self.process(ssn)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1050, in process
    self.send(snd, msg)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1261, in send
    body = enc(msg.content)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/message.py"", line 28, in encode
    sc.write_primitive(type, x)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 300, in write_list
    type = self.encoding(o)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 59, in encoding
    raise CodecException(""no encoding for %r"" % obj)
 CodecException: no encoding for <cinder.db.sqlalchemy.models.VolumeMetadata object at 0x458e610>"
1,"The following stacktrace has been observed using NSX DHCP:
2014-05-02 14:00:36.295 30957 DEBUG neutron.plugins.vmware.api_client.base [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Released connection https://192.168.1.13:443. 10 connection(s) available. release_connect
ion /opt/stack/neutron/neutron/plugins/vmware/api_client/base.py:176
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.eventlet_request [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Completed request 'POST /ws.v1/lservices-node/20e0dc1c-a1da-455f-8841-3c52d78
6696c/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/vmware/api_client/eventlet_request.py:152
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.client [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] Request returns ""<httplib.HTTPResponse instance at 0x4c49368>"" request /opt/stack/neutron/n
eutron/plugins/vmware/api_client/client.py:93
2014-05-02 14:00:36.297 30957 ERROR neutron.api.v2.resource [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] add_router_interface failed
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 193, in _handle_action
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1719, in add_router_interface
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource context, router_id, interface=router_iface_info)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcpmeta_modes.py"", line 157, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/combined.py"", line 89, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource plugin, context, router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/nsx.py"", line 312, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource context, subnet_id, is_enabled)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 294, in lsn_metadata_configure
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource self.lsn_port_metadata_setup(context, lsn_id, subnet)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 225, in lsn_port_metadata_setup
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource lsn_port_id = self.lsn_port_create(self.cluster, lsn_id, data)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 453, in lsn_port_create
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource subnet_info['mac_address'], lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 442, in lsn_port_save
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource context, lsn_port_id, subnet_id, mac_addr, lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource File ""/opt/stack/neutron/neutron/plugins/vmware/dbexts/lsn_db.py"", line 96, in lsn_port_add_for_lsn
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource with context.session.begin(subtransactions=True):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource AttributeError: 'NSXCluster' object has no attribute 'session'
This is because the wrong parameter is passed to during the DB operation."
1,"in update_network API call, the network dictionary has the contents: {u'network': {u'admin_state_up': True}}
_network_admin_state functions expects network name in the dictionary and it raises error. This has to be fixed. Network name should not be expected.
2014-03-11 16:20:41.920 29794 ERROR neutron.api.v2.resource [-] update failed
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource result = method(request=request, **args)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 486, in update
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource obj = obj_updater(request.context, id, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 233, in inner
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource retval = f(*args, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 133, in update_network
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource self._network_admin_state(network)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 597, in _network_admin_state
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource raise plum_excep.PLUMgridException(err_msg=err_message)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource PLUMgridException: An unexpected error occurred in the PLUMgrid Plugin: Network Admin State Validation Falied:"
0,"Currently glance ""tries"" to load all stores even though the requirements are not meant. If a store cannot be loaded, it'll be disabled.
Although this is harmless, it is not user-friendly and creates some confusion when reading logs. Instead of trying to load and failing, glance should enable 2 stores by default - filesystem and http - and let the others be enabled manually by the user, which is what GridFS does.
TL;DR: The proposal is to remove all stores but file and http from here[0]
[0] https://git.openstack.org/cgit/openstack/glance/tree/glance/store/__init__.py#n39"
0,"Currently even though service is running in with debug=True, rpc messages are not logged. Previously in icehouse we could see those logs. It helps with debugging when we can see the communication between agents and server."
1,"In resource_tracker.py upon failure, we raise a generic ""ComputeResourcesUnavailable"". The logging presents good information as to why there was a failure. We should either provide back data in the ComputeResourcesUnavailble object or raise different exceptions based on what is not available. This may point to a bigger issue of a broken scheduler."
0,"1. Create a server from a image, then show the server details, the ""image"" is a dict, such as:
          {
                ""id"": image_id,
                ""links"": [{
                    ""rel"": ""bookmark"",
                    ""href"": bookmark,
                }],
            }
2. Create a server from a volume, then show the server details, the ""image"" is a dict, such as:
        """"
3. It's inconsistent ""image"" value, i think it's a bug."
1,"SLAAC and DHCPv6 stateless work only with subnets with mask /64 and more (/63, /62) because EUI-64 calculated IP takes 8 octets.
If subnet mask is /65, /66, .., /128 SLAAC/DHCP stateless should be disabled.
API call for creating subnet with SLAAC/DHCP stateless and mask more than /64 should fail.
Example:
let's create net and subnet with mask /96:
$ neutron net-create 14
$ neutron subnet-create 14 --ipv6-ra-mode=slaac --ipv6-address-mode=slaac --ip-version=6 2003::/96
Created a new subnet:
...
| allocation_pools | {""start"": ""2003::2"", ""end"": ""2003::ffff:fffe""} |
| cidr | 2003::/96 |
.... |
| gateway_ip | 2003::1 |
...
| ipv6_address_mode | slaac |
| ipv6_ra_mode | slaac |
...
Let's create port in this network:
$ neutron port-create 14 --mac-address=11:22:33:44:55:66
Created a new port:
...
| fixed_ips | {""subnet_id"": ""1bfe4522-3b71-4e74-bb80-44c853ff868d"", ""ip_address"": ""2003::1322:33ff:fe44:5566""} |
...
| mac_address | 11:22:33:44:55:66 |
...
As you see port gets IP 2003::1322:33ff:fe44:5566 which is not from original network 2003::/96."
1,"It appears that nova compute is expecting (in a number of cases) an instance to be in powering-off state, but the state is actually none. This appears to have a wide range of failure modes.
I'm seeing this ~2800 times in the last 7 days
http://logstash.openstack.org/#eyJzZWFyY2giOiJ0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIEFORCBtZXNzYWdlOlwib3Nsby5tZXNzYWdpbmcucnBjLmRpc3BhdGNoZXIgVW5leHBlY3RlZFRhc2tTdGF0ZUVycm9yOiBVbmV4cGVjdGVkIHRhc2sgc3RhdGU6IGV4cGVjdGluZyAodSdwb3dlcmluZy1vZmYnLCkgYnV0IHRoZSBhY3R1YWwgc3RhdGUgaXMgTm9uZVwiIEFORCBidWlsZF9zdGF0dXM6XCJGQUlMVVJFXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIiLCJzdGFtcCI6MTQwMzcxMTkxODcxNH0=
Example traceback:
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None
 to caller
2014-06-25 05:01:31.058 ERROR oslo.messaging._drivers.common [req-ae166761-731e-4ead-b9da-eb8e8b1cf6af None None]
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 278, in decorated_function
    LOG.info(_(""Task possibly preempted: %s"") % e.format_message())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 336, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 314, in decorated_function
    kwargs['instance'], e, sys.exc_info())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 302, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2346, in stop_instance
    instance.save(expected_task_state=task_states.POWERING_OFF)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 187, in wrapper
    ctxt, self, fn.__name__, args, kwargs)
  File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 354, in object_action
    objmethod=objmethod, args=args, kwargs=kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
    retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
    raise result
UnexpectedTaskStateError_Remote: Unexpected task state: expecting (u'powering-off',) but the actual state is None
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 404, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 196, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 473, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/opt/stack/new/nova/nova/db/api.py"", line 780, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 164, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2229, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2280, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None"
1,"The compute.instance.delete.end notification used to have a value for deleted_at. It seems that deleted_at is being set in the database, but the final notification for a deleted instance is emitted without it. It is important that the final notification for an instance be sent with up-to-date details as it is potentially the last notification that will be sent.
Sample Notification: http://paste.openstack.org/show/48702/"
0,"Some versions of libvirt (such as RHEL6's 0.10.2) require a port specification with a GlusterFS network disk, or they reject the disk with an ""XML error: missing port for host"" error.
We can provide a blank value, rather than omitting the field, which allows qemu to default to a reasonable port."
1,In Havana user_group was taken from default section while currently it needs to be in [haproxy] section.
1,"When creating LVM volume by source volume, cinder create source volume's snapshot first, then destination volume and active it, at last copy the volume.
Step:
1. self.create_snapshot
2.self._create_volume
3.self.vg.activate_lv
4.volutils.copy_volume
But If we create or active target volume fail since insufficient free space or other system error for example, the source volume's snapshot wouldn't be remove.
Temporary snapshot would remain, and what is worse we can't delete the source volume successfully."
0,"The new Fibre Channel Zone Manager settings in the cinder.conf aren't in their own group. Also the zone fabric settings don't use a group.
The fabric settings use a dynamic name to group the values together instead of using the config groups
For example,
  the current fc fabric config options looked like
fc_fabric_address_BRCD_NAME1=some address
fc_fabric_user_BRCD_NAME1=some user
...
It should be
[BRCD_NAME1]
fc_fabric_address=some address
fc_fabric_user=some user"
0,"In test_create_network_multiprovider() , an invalid comparison is used ...

tz = network['network'][mpnet.SEGMENTS]
for tz in data['network'][mpnet.SEGMENTS]: <=== tz from previous statement is lost
   for field in [pnet.NETWORK_TYPE, pnet.PHYSICAL_NETWORK,
                         pnet.SEGMENTATION_ID]:
        self.assertEqual(tz.get(field), tz.get(field)) <===== this is always true"
1,"I observed this error here:

http://logs.openstack.org/77/108177/10/experimental/check-tempest-dsvm-neutron-dvr/9e67d95/logs/screen-q-svc.txt.gz?level=TRACE#_2014-07-31_15_05_48_864

And in few other places. This seems to be triggered (for instance) during the following testcase:

tempest.scenario[...]test_server_connectivity_pause_unpause

It looks like the scheduling process fails because of a duplicate entry to the agent/router binding table. This might be an effect of fix:

https://review.openstack.org/#/c/73234/"
0,"The fake compute driver does not provide the attribute supported_instances to ImagePropertiesFilter scheduler filter.
So ImagePropertiesFilter refuses to deploy images with hypervisor_type=fake property on fake computes.
Consequently, fake computes can not be used in multi hypervisor_types deployments because in this case hypervisor_type property on image is mandatory to avoid mixing one hypervisor_type image with another hypervisor_type compute."
0,"[openstack-dev] [neutron] unnecessary return statement in ovs_lib
Thu Jan 16 02:48:46 UTC 2014
Came across the following issue while looking at ovs_lib [1]:
The BaseOVS class has the add_bridge() method which after creating an OVS
bridge, returns an OVSBridge object. BaseOVS class is only used by
OVSBridge defined in the same file. OVSBridge has a create() method that
calls the add_bridge() nethod mentioned earlier but do not use the return
value. (See the methods add_bridge and create below.)
What seems odd is the return statement at the end of add_bridge() which is
not used anywhere and doesn't make much sense as far as I can see but I may
be missing something. The OVSBase is never directly used anywhere in
Neutron directory. Of course the return does not do any harm beyond
creating an unused object but it looks to me that it should be removed
unless there is a good reason (or a potential future use case) for it.
class BaseOVS(object):
        ...
    def add_bridge(self, bridge_name):
        self.run_vsctl([""--"", ""--may-exist"", ""add-br"", bridge_name])
        return OVSBridge(bridge_name, self.root_helper)
class OVSBridge(BaseOVS):
        ...
    def create(self):
        self.add_bridge(self.br_name)
[1]
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/ovs_lib.py"
1,"As of the following commit: https://github.com/openstack/nova/commit/8a7b95dccdbe449d5235868781b30edebd34bacd our nova-compute service on the seed node is throwing DBErrors. If I reset my Nova tree to the previous commit and downgrade the database to 232 then I am able to use nova successfully again. With that commit all boots fail with a No hosts found message, presumably related to the following messages in the nova-compute log:
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/hubs/hub.py"", line 187, in switch
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup return self.greenlet.switch()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/greenthread.py"", line 194, in main
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup result = function(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup service.start()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/service.py"", line 193, in start
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup self.manager.pre_start_hook()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 835, in pre_start_hook
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup self.update_available_resource(nova.context.get_admin_context())
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 5121, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup rt.update_available_resource(context)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup return f(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 353, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup self._sync_compute_node(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 384, in _sync_compute_node
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup self._update(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 456, in _update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup context, self.compute_node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/api.py"", line 241, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup return self._manager.compute_node_update(context, node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/rpcapi.py"", line 364, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup prune_stats=prune_stats)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup wait_for_reply=True, timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 87, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 390, in send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup return self._send(target, ctxt, message, wait_for_reply, timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 383, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup raise result
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup RemoteError: Remote error: DBError (ProgrammingError) (1064, 'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \': ""\'amd64\'"", u\'baremetal_driver\': ""\'nova.virt.baremetal.pxe.PXE\'""} WHERE compute\' at line 1') 'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u'cpu_arch': u'amd64', u'baremetal_driver': u'nova.virt.baremetal.pxe.PXE'}, 1L)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup [u'Traceback (most recent call last):\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_executors/base.py"", line 36, in _dispatch\n incoming.reply(self.callback(incoming.ctxt, incoming.message))\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__\n return self._dispatch(endpoint, method, ctxt, args)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch\n result = getattr(endpoint, method)(ctxt, **new_args)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/manager.py"", line 458, in compute_node_update\n result = self.db.compute_node_update(context, node[\'id\'], values)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/api.py"", line 228, in compute_node_update\n return IMPL.compute_node_update(context, compute_id, values)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 110, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 166, in wrapped\n return f(*args, **kwargs)\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update\n compute_ref.update(values)\n', u' File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 447, in __exit__\n self.rollback()\n', u' File ""/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 58, in __exit__\n compat.reraise(exc_type, exc_value, exc_tb)\n', u' File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 444, in __exit__\n self.commit()\n', u' File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 354, in commit\n self._prepare_impl()\n', u' File ""/usr/lib64/python2.7/site-packages/sqlalche
Feb 12 23:08:24 localhost nova-compute: my/orm/session.py"", line 334, in _prepare_impl\n self.session.flush()\n', u' File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap\n raise exception.DBError(e)\n', u'DBError: (ProgrammingError) (1064, \'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \\\': ""\\\'amd64\\\'"", u\\\'baremetal_driver\\\': ""\\\'nova.virt.baremetal.pxe.PXE\\\'""} WHERE compute\\\' at line 1\') \'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s\' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u\'cpu_arch\': u\'amd64\', u\'baremetal_driver\': u\'nova.virt.baremetal.pxe.PXE\'}, 1L)\n']."
1,"In nova source code, some LOG messages in nova/virt/baremetal/virtual_power_driver.py are not wrapped with _() which will causei18n problems, we need fix this small but important issue, to make our codes professional."
0,"Change https://review.openstack.org/#/c/69600/ adds a new method refresh_instance_security_rules to the nova ComputeDriver interface but it missed adding a test to nova.tests.virt.test_virt_driver to make sure all virt drivers are explicitly handling it.
There is also a typo in the method's docstring."
1,"Expectation:
cinder.exception.CoraidESMConfigureError: ESM configure request failed: Reply is empty.
Actual:
cinder.exception.CoraidESMConfigureError: Reply is empty."
0,"Thie bug is a generic case for bug 1339775 [0], there are a lot same cases in codebase which are outside of v2 stuff, but the result is smimilar, UnicodeError exception will be raised.
[0] https://bugs.launchpad.net/glance/+bug/1339775"
1,"https://github.com/openstack/cinder/commit/0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 appears to have introduced a bug making it possible to show the details of a volume under another project_id.

$ . ~/devstack/openrc demo demo
$ cinder list
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | available | demo1 | 1 | lvmdriver-1 | false | |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder list --all-tenants 1
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
| ID | Tenant ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | 807f2a0ef357420e9a70ac1a5fef7a4c | available | demo1 | 1 | lvmdriver-1 | false | |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder show ad3194a3-8304-4d17-8f66-f1a1a261d339
+------------------------------+--------------------------------------+
| Property | Value |
+------------------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| created_at | 2014-08-12T11:06:33.000000 |
| description | None |
| encrypted | False |
| id | ad3194a3-8304-4d17-8f66-f1a1a261d339 |
| metadata | {} |
| name | admin1 |
| os-vol-tenant-attr:tenant_id | e66effcd5db642dfabceabc76ea78196 |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | available |
| user_id | 7e540d1944a74bd4a90aa9a8b15d09b0 |
| volume_type | lvmdriver-1 |
+------------------------------+--------------------------------------+"
1,"Current code use ModelBase.save() always submit a commit even inside a block fo with session.begin().
this is not purpose of using session.begin() to organize some operations in one transaction
1)session.begin() will return a SessionTransaction instance, then call SessionTransaction.__enter__()
and do something in with block, then call SessionTransaction.__exit__(), in method __exit__() will commit or rollback automatically. See https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/orm/session.py#L454
2) There is also suggestion metioned in https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/session.py#L71
3) ModelBase.save() begin another transaction see https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/models.py#L47
so we'd better don't use ModelBase.save() inside of block session.begin()"
0,"When using 'cinder extend' command, tgt update process doesn't implemented.
So, If you want to see extended size of volume inside your vm, you must run 'tgt-admin --update iqn.2010-10.org.openstack:volume-xxxxx' in hostmachine that has the extended volume .
See my test case as follow.
1. Create cinder volume
# cinder create --volume-type LVM01-type --display-name test-extend-vol01 3
2. List cinder volume & confirm the size 3 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 | 3 | LVM01-type | false | |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
3. run lvs command to confirm the size 3 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 3.00g
4. run 'tgt-admin --show' command to confirm the size 3 GB
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET 00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET 00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
5. Extend cinder volume
# cinder extend 885f850f-8e03-4576-a60d-5d3d1362a9da 5
6. List cinder volume and confirm the extended size to 5 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 | 5 | LVM01-type | false | |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
7. Run 'lvs' command to confirm the extended size to 5 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 5.00g
8. Run 'tgt-admin --show' command to confirm the extended size to 5 GB , but it still 3 GB because didn't implemented 'tgt update'
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET 00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET 00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
9. Run 'tgt-admin --update' command to update
# tgt-admin --update iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
10. Run 'tgt-admin --show' command to confirm 5 GB updated
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET 00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET 00010001
            SCSI SN: beaf11
            Size: 5369 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
Please check this bug and answer to me.
Thank you."
0,"Cinder storage-assisted volume migration and retype work by creating a new copy of the volume, waiting for it to sync, and deleting the original copy. During the sync time (which may be long), if the Cinder process goes down for any reason, we will end up with a volume that has two copies, which wastes resources."
1,"The values for tcp_keepalive, tcp_keepidle, tcp_keepalive_count and tcp_keepalive_interval in cinder.conf are ignored."
0,Some UT use <mock>.assert_has_calls([]) as a way to check if mock wasn't called - this doesn't work because assert_has_calls only checks if passed calls are present in mock_calls and hence it is always true regardless of whether mock was called or not. This can lead to falsely passed tests.
1,"During instance build, there is a call to _default_block_device_names() that contains a db call to update the instance. If the instance is deleted before this call it results in a InstanceNotFound exception that goes unhandled and ends up in the compute log. Since this is an expected error, it should be handled correctly."
1,"When moving VHDs on the filesystem a coalesce may be in progress. The result of this is that the VHD file is not valid when it is copied as it is being actively changed - and the VHD cookie is invalid.
Seen in XenServer CI: http://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/36/109836/4/23874/run_tests.log
2014-08-28 12:26:37.538 | Traceback (most recent call last):
2014-08-28 12:26:37.543 | File ""tempest/api/compute/servers/test_server_actions.py"", line 251, in test_resize_server_revert
2014-08-28 12:26:37.550 | self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE')
2014-08-28 12:26:37.556 | File ""tempest/services/compute/json/servers_client.py"", line 179, in wait_for_server_status
2014-08-28 12:26:37.563 | raise_on_error=raise_on_error)
2014-08-28 12:26:37.570 | File ""tempest/common/waiters.py"", line 77, in wait_for_server_status
2014-08-28 12:26:37.577 | server_id=server_id)
2014-08-28 12:26:37.583 | BuildErrorException: Server e58677ac-dd72-4f10-9615-cb6763f34f50 failed to build and is in ERROR status
2014-08-28 12:26:37.589 | Details: {u'message': u'[\'XENAPI_PLUGIN_FAILURE\', \'move_vhds_into_sr\', \'Exception\', ""VDI \'/var/run/sr-mount/16f5c980-eeb6-0fd3-e9b1-dec616309984/os-images/instancee58677ac-dd72-4f10-9615-cb6763f34f50/535cd7f2-80a5-463a-935c-9c4f52ba0ecf.vhd\' has an invalid footer: \' invalid cook', u'code': 500, u'created': u'2014-08-28T11:57:01Z'}"
1,"When unshelving a shelved server, the server cannot be changed to 'Active' forever:
$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 919234d1-4a3d-4c26-bddd-77d004f7d41e | vm01 | SHELVED_OFFLOADED | unshelving | Shutdown | private=10.0.0.3 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
and nova-scheduler outputs the following error messages:
2013-10-01 17:22:55.456 ERROR nova.openstack.common.rpc.amqp [req-51700174-fc3a-48f4-8962-8f54d6f30164 admin demo] Exception during message handling
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp **args)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 439, in inner
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp return catch_client_exception(exceptions, func, *args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 420, in catch_client_exception
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp return func(*args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/manager.py"", line 298, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp filter_properties)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 144, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp filter_properties, instance_uuids)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 288, in _schedule
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp scheduler_hints = filter_properties.get('scheduler_hints') or {}
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp AttributeError: 'list' object has no attribute 'get'"
0,The following patch renames the rest of q_exc to n_exc which were left when quantum was renamed to neutron.
1,"When scheduling VMs and the retry logic kicks in, the failed compute exception text is saved to be displayed for triaging purposes in the conductor/scheduler logs. When the conductor tries to display the exception text when the maximum scheduling attempts have been reached, the exception always shows 'None' for the exception text.
Snippet from scheduler_utils.py...
 msg = (_('Exceeded max scheduling attempts %(max_attempts)d '
'for instance %(instance_uuid)s. '
'Last exception: %(exc)s.')
% {'max_attempts': max_attempts,
'instance_uuid': instance_uuid,
'exc': exc})
That is, 'exc' is erroneously ALWAYS None in this case."
1,"This is on the master branch (current havana trunk). Ran tempest against the powervm driver with a backing VIOS hypervisor, there are no instances left over after the tests are done (everything was successful):
http://paste.openstack.org/show/44772/
But on the backing hypervisor, there are leftover image files:
http://paste.openstack.org/show/44773/
I found that the powervm method that is supposed to remove the files isn't using the --force option with the rm command:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L800
Also, this method isn't even used in the code:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L785"
1,"2014-03-03 09:25:31.621 5910 ERROR root [-] Original exception being dropped: ['Traceback (most recent call last):\n', ' File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 206, in _get_resource\n r = self._get_by_id(context, model, id)\n', ' File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 144, in _get_by_id\n return query.filter(model.id == id).one()\n', ' File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2323, in one\n raise orm_exc.NoResultFound(""No row was found for one()"")\n', 'NoResultFound: No row was found for one()\n']
2014-03-03 09:25:31.622 5910 WARNING neutron.services.loadbalancer.drivers.common.agent_driver_base [req-6949f3b4-f991-48b7-8424-b3bfbca7c822 None] Cannot update status: member d439c879-55f7-400f-b6a8-32753f057b05 not found in the DB, it was probably deleted concurrently
There is no need for error log about original exception being dropped as warning log is enough.
This happens due to using save_and_reraise_exception() in loadbalancer_db code:
    def _get_resource(self, context, model, id):
        try:
            r = self._get_by_id(context, model, id)
        except exc.NoResultFound:
            with excutils.save_and_reraise_exception():
                if issubclass(model, Vip):
                    raise loadbalancer.VipNotFound(vip_id=id)
                elif issubclass(model, Pool):
                    raise loadbalancer.PoolNotFound(pool_id=id)
                elif issubclass(model, Member):
                    raise loadbalancer.MemberNotFound(member_id=id)
                elif issubclass(model, HealthMonitor):
                    raise loadbalancer.HealthMonitorNotFound(monitor_id=id)
        return r
where the whole purpose of exception handler is to reraise proper type of exception.
I think save_and_reraise_exception() was designed for cases when new exceptions raised inside exception handler are not expected.
In this particular case I don't see the reason for using save_and_reraise_exception().
As an option I think a parameter can be added to save_and_reraise_exception() constructor to disable logging."
0,"virt/baremetal/volume_driver.py calls db api directly then fails.

2013-08-06 15:59:47.239 ERROR nova.compute [req-117555a1-bb81-43ec-9eee-ce976729093b demo demo] No db access allowed in nova-compute: File ""/usr/lib/python2.7/dist-packages/eventlet/greenpool.py"", line 80, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
    **args)
  File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 77, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 216, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 245, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3424, in attach_volume
    mountpoint, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3456, in _attach_volume
    mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/driver.py"", line 357, in attach_volume
    instance, mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/volume_driver.py"", line 225, in attach_volume
    fixed_ips = nova_db_api.fixed_ip_get_by_instance(ctx, instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 496, in fixed_ip_get_by_instance
    return IMPL.fixed_ip_get_by_instance(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())"
1,"The following exception was originally observed against the old rpc code, but the same problem exists in oslo.messaging.
 Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 78, in inner_func
     return infunc(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 698, in _consumer_thread
     self.consume()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 689, in consume
     it.next()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 606, in iterconsume
     yield self.ensure(_error_callback, _consume)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 540, in ensure
     return method(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 597, in _consume
     nxt_receiver = self.session.next_receiver(timeout=timeout)
   File ""<string>"", line 6, in next_receiver
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 665, in next_receiver
     if self._ecwait(lambda: self.incoming, timeout):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 50, in _ecwait
     result = self._ewait(lambda: self.closed or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 571, in _ewait
     result = self.connection._ewait(lambda: self.error or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 214, in _ewait
     self.check_error()
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 207, in check_error
     raise self.error
 InternalError: Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 667, in write
     self._op_dec.write(*self._seg_dec.read())
   File ""/usr/lib/python2.6/site-packages/qpid/framing.py"", line 269, in write
     if self.op.headers is None:
 AttributeError: 'NoneType' object has no attribute 'headers'
It's possible for something to put the qpid client into a bad state. In particular, I have observed a case that will cause session.next_receiver() to immediately raise an InternalError. This exception makes it all the way out. If the eventlet executor is used, the forever_retry_uncaught_exceptions() decorator will get hit. It will go back into this code and get the same error, stuck in an infinite loop of retrying.
The connection needs to be reset in this case to recover."
0,"The core of the domain model should be as dependency free as possible.
However, since 830f27ba342ca0881e3677ac11c3d818962cba3e (change id Ic52ffb46df9438c247ba063748cadd69b9c90bcd) we have depended on configuration in glance.domain.__init__.
This configuration should instead be depended on in the infrastructure (probably in glance.gateway) and passed in as an initialization parameter to the domain model objects."
1,"jpipes@uberbox:~/repos/openstack/neutron$ git log --oneline | head -1
84aeb9a Merge ""Imported Translations from Transifex""
Running tox -eALL resulted in:
i18n runtests: commands[0] | python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py
Traceback (most recent call last):
  File ""./tools/check_i18n.py"", line 151, in <module>
    debug):
  File ""./tools/check_i18n.py"", line 112, in check_i18n
    ASTWalker())
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 106, in walk
    walker.preorder(tree, visitor)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 63, in preorder
    self.dispatch(tree, *args) # XXX *args make sense?
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 62, in visitConst
    self.lines[node.lineno - 1][:-1], msg),
TypeError: not enough arguments for format string
ERROR: InvocationError: '/home/jpipes/repos/openstack/neutron/.tox/i18n/bin/python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py'"
0,Remove unused RPC calls since n1kv plugin does not communicate with l2 agent.
1,"This is a follow on to https://bugs.launchpad.net/neutron/+bug/1244853.
I found today that the same problem can happen with external gateway devices. Those should be identified and removed in a manner similar to the fix before the other bug."
1,When a subnet is created (except the first one) another ip is added to the dhcp port. Midonet plugin should react and add the correct route opt121
1,Problem is the line https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L1012. A flat network will return an object instead of 0
0,"After the volume is created, qemu-img is supposed to copy the downloaded image to the volume. This fails as the disk is enabled and is not accessible. To fix this, the flow of creating a volume from an image must be changed when using Windows. So instead of creating the volume before copying the image to it, it would be better to skip the volume creation and let the driver create the disk based on the downloaded image and then import it as an iSCSI disk.

Windows does not support using dynamic vhds as iSCSI disks and as qemu-img cannot create fixed vhd images, there must be an intermediate conversion before importing the disk.

Trace: http://paste.openstack.org/show/74765/"
0,"The default quota driver is conf file driven, which isn't particularly useful in non-trivial clouds.

Can we change the default to quantum.db.quota_db.DbQuotaDriver please?"
0,"we should support update extra_dhcp_opt with value None, if opt_value is None, then delete extra_dhcp_opt on that port"
1,"Logstash search: http://logstash.openstack.org/#eyJzZWFyY2giOiJmaWxlbmFtZTpjb25zb2xlLmh0bWwgQU5EIG1lc3NhZ2U6XCJhc3NlcnRpb25lcnJvcjogY29uc29sZSBvdXRwdXQgd2FzIGVtcHR5XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzODQ2NDEwNzIxODl9

An example failure is http://logs.openstack.org/92/55492/8/check/check-tempest-devstack-vm-full/ef3a4a4/console.html

console.html
===========

2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,775 Request: POST http://127.0.0.1:8774/v2/3f6934d9aabf467aa8bc51397ccfa782/servers/10aace14-23c1-4cec-9bfd-2c873df1fbee/action
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Body: {""os-getConsoleOutput"": {""length"": 10}}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:21,000 Response Status: 200
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Nova request id: req-7a2ee0ab-c977-4957-abb5-1d84191bf30c
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Headers: {'content-length': '14', 'date': 'Sat, 16 Nov 2013 21:41:20 GMT', 'content-type': 'application/json', 'connection': 'close'}
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Body: {""output"": """"}
2013-11-16 21:54:27.999 | }}}
2013-11-16 21:54:27.999 |
2013-11-16 21:54:27.999 | Traceback (most recent call last):
2013-11-16 21:54:27.999 | File ""tempest/api/compute/servers/test_server_actions.py"", line 281, in test_get_console_output
2013-11-16 21:54:28.000 | self.wait_for(get_output)
2013-11-16 21:54:28.000 | File ""tempest/api/compute/base.py"", line 133, in wait_for
2013-11-16 21:54:28.000 | condition()
2013-11-16 21:54:28.000 | File ""tempest/api/compute/servers/test_server_actions.py"", line 278, in get_output
2013-11-16 21:54:28.000 | self.assertTrue(output, ""Console output was empty."")
2013-11-16 21:54:28.000 | File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2013-11-16 21:54:28.000 | raise self.failureException(msg)
2013-11-16 21:54:28.001 | AssertionError: Console output was empty.

n-api
===="
1,"Detach volume fails with ""Unexpected KeyError"" in EC2 interface when I detach a ""attaching"" status volume.
The volume with ""attaching"" status don't contain a property""instance_uuid"", a KeyError will be raised at the following function.
    def _get_instance_from_volume(self, context, volume):
        if volume['instance_uuid']:
            ......
Attaching volume dict:
{
 'status': u'attaching',
 'volume_type_id': u'None',
 'display_name': None,
 'attach_time': '',
 'availability_zone': u'nova',
 'created_at': u'2014-02-13T16: 50: 53.620080',
 'attach_status': 'detached',
 'display_description': None,
 'volume_metadata': {
 },
 'snapshot_id': None,
 'mountpoint': '',
 'id': u'99d118ee-3666-4983-8825-f8c096bccbd1',
 'size': 1
}"
0,"Let's go in codes directly, to get resources from extension, the child class[0] has an __init__() to register itself, and the farther class [1]'s __init__() has already did this. If there are no some specified variables needed by child class, the child's __init__ could be removed. In <nova>/nova/api/openstack/compute/contrib/, nearly all of extensions class don't have such a duplicate __init__(). Removing the __init__() in child class could help keeping codes consistent and clean.
[0] https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/fixed_ips.py#L88
[1] https://github.com/openstack/nova/blob/master/nova/api/openstack/extensions.py#L63"
1,"After creating a volume type 'abc' a number of default quotas are added to the Default tab's quota table:
- Volumes Abc
- Snapshots Abc
- Gigabytes Abc
I expected these defaults to be removed from the table once I remove the volume type. However, after removing the volume type they are still there. I verified that volume type was removed from the Volumes tab, and also marked deleted in Cinder database."
0,"We have a whole bunch of config options that control periodic tasks that can be turned off when setting the option to 0 or -1. We should document in the config option help messages what the expected behavior when setting these options to < 1.
Related patch: https://review.openstack.org/#/c/60641"
1,"When Instance.save called the expected_task_state parameter passed to it in order to specify what is instance expected task state.
There many cases when we would expect that there no task state for the vm (task_state=None in db). The expected_task_state=None is overwritten by the default substitution in the save() arguments and due to this not passed correctly to the db.instance_update_and_get_original method call."
0,"Step 1: Create an empty image (using v1 or v2 - doesn't matter).
   glance image-show <IMAGE-ID> will show the image status as ""queued"".
Step 2: Upload an image file for the image, but cancel the upload midway.
   glance image-show <IMAGE-ID> will show the image status as ""saving"" even though the upload has been cancelled.
Expected result: The image status should be ""queued"" instead of ""saving""."
1,"ofagent has code for agent-on-DomU support inherited from OVS agent.
However, it's incomplete and broken. Because ofagent uses a direct
OpenFlow channel instead of ovs-ofctl command to program a switch,
the method to use the special rootwrap can not work."
0,"When DVR is enabled and enable_isolated_metadata=True in dhcp_agent.ini, the agent should only inject a metadata host route when there is no gateway on the subnet. But it does it all the time:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0 src 10.0.0.5
169.254.169.254 via 10.0.0.4 dev eth0
The ""opts"" file for dnsmasq confirms it was the Neutron code that configured this.
The code in neutron/agent/linux/dhcp.py:get_isolated_subnets() is only looking at ports where the device_owner field is DEVICE_OWNER_ROUTER_INTF, it also needs to look for DEVICE_OWNER_DVR_INTERFACE. Simlar changes have been made in other code.
Making that simple change fixes the problem:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0 src 10.0.0.5
I have a patch I'll get out for this."
1,"tested with mysql
1) upgrade head
2) downgrade havana
3) upgrade head
4) BOOM -> http://paste.openstack.org/show/91769/
5) This is easy [1]
6) Try 1-3 again
7) BOOM -> http://paste.openstack.org/show/91770/
8) This is easy as well [2]
9) Repeat again steps 1-3
10) BOOM -> http://paste.openstack.org/show/91771/
I'm clueless so far about the last failure.
[1]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     # autoincrement=None, existing_type=None,
     # existing_server_default=False, existing_nullable=None,
     # existing_autoincrement=None, schema=None, **kw)
+ bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
- bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
[2]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     # autoincrement=None, existing_type=None,
     # existing_server_default=False, existing_nullable=None,
     # existing_autoincrement=None, schema=None, **kw)
+ bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
- bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
@@ -123,7 +123,7 @@ def parse_modify_command(command):
                 existing['existing_server_default'] = default.arg
             else:
                 existing['existing_server_default'] = default.arg.compile(
- dialect=bind.engine.name)
+ dialect=bind.dialect)
         kwargs.update(existing)
         op.alter_column(table, column, **kwargs)"
1,"The BigSwitch plugin has helper methods for writing certificates to the file system that are incorrectly defined.
They are missing the self argument that will be passed in.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L368
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L319
The unit tests were also incorrect in this case since they were refactored at right before the merge to avoid any file-system writes during unit tests."
1,"1. create a availability-zone: second_zone, add ns17 to the aggregate
[root@ns11 nova]# nova availability-zone-list
+----------------------------------+----------------------------------------+
| Name | Status |
+----------------------------------+----------------------------------------+
| first_zone | available |
| |- ns11.sce.cn.ibm.com | |
| | |- nova-compute | enabled :-) 2013-09-25T09:51:54.352341 |
| second_zone | available |
| |- ns17-osee.cn.ibm.com | |
| | |- nova-compute | enabled :-) 2013-09-25T09:51:58.283034 |
+----------------------------------+----------------------------------------+
2. nova boot with the zone:
[root@ns11 藴]# nova boot --image d7c64f67-3de7-4aa0-88fb-1292ff520404 --flavor 1 --availability-zone second_zone:ns17-osee.cn.ibm.com test_zone17_2
3. [root@ns11 nova]# nova list
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| 6f3ec942-6c8e-46fa-ad7e-d2c900921f65 | test_zone17 | ACTIVE | None | Running | flat_net01=172.10.0.16 |
| 7273973d-a9e1-4917-ad59-c54ece9b0765 | test_zone17_1 | ACTIVE | None | Running | flat_net01=172.10.0.18 |
| 27f71b26-b775-4516-ba2a-13d9e91e96dd | test_zone17_2 | ACTIVE | None | Running | flat_net01=172.10.0.19 |
| 6350a439-2dd3-4b14-b99e-219fad27111b | zhikun_testvm | ACTIVE | None | Running | flat_net01=172.10.0.13 |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
4. but the zone of the vm is: nova
[root@ns11 nova]# nova show 27f71b26-b775-4516-ba2a-13d9e91e96dd
+--------------------------------------+----------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------+
| status | ACTIVE |
| flat_net01 network | 172.10.0.19 |
| updated | 2013-09-25T09:50:23Z |
| OS-EXT-STS:task_state | None |
| OS-EXT-SRV-ATTR:host | ns17-osee.cn.ibm.com |
| key_name | None |
| image | cirros_kvm (d7c64f67-3de7-4aa0-88fb-1292ff520404) |
| accessIPv6 | |
| progress | 0 |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-AZ:availability_zone | nova |
| config_drive | |
+--------------------------------------+----------------------------------------------------------+
[root@ns11 nova]# date
Wed Sep 25 18:05:26 CST 2013
5. I did not configure default_zone in nova.conf
6. It is very strange that this problem could not always been recreated. usually it happens when we create a new AZ, and then use the AZ boot instance. Seems wait for some time, may be one night or sooner, the AZ will become to correct one. I'm confused.
# Expected results
The AZ should take effect any time if we did not set default AZ in nova.conf"
1,"This break the create volume from image feature with RBD backend and perhaps other backends as well?
On a grizzly setup with both glance and cinder configured with RBD
backend, I cannot find a way to create a new volume from a (raw) image.
The resulting volume is correctly created but remains full of NULL bytes.
$ glance show 92af7175-9478-42c4-8ed0-b336362ff3f7
URI: https://example.com:9292/v1/images/92af7175-9478-42c4-8ed0-b336362ff3f7
Id: 92af7175-9478-42c4-8ed0-b336362ff3f7
Public: Yes
Protected: No
Name: precise-server-cloudimg-amd64.raw
Status: active
Size: 2147483648
Disk format: raw
Container format: bare
Minimum Ram Required (MB): 0
Minimum Disk Required (GB): 0
Owner: 8226a848c4eb43e69b1a0f00c582e31f
Created at: 2013-08-11T07:22:13
Updated at: 2013-08-11T07:29:10
$ cinder create 10 --image-id 92af7175-9478-42c4-8ed0-b336362ff3f7
--display-name boot-from-volume
In the following logs, cinder-scheduler gets the correct image_id
parameter but cinder-volume gets None instead which looks weird.
==> cinder-scheduler.log <==
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'5e5eca077ae149a6a64b1760bc55cc55', u'_context_read_deleted': u'no',
u'args': {u'request_spec': {u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'volume_properties':
{u'status': u'creating', u'volume_type_id': None, u'display_name':
u'boot-from-volume', u'availability_zone': u'nova', u'attach_status':
u'detached', u'source_volid': None, u'metadata': {}, u'volume_metadata':
[], u'display_description': None, u'snapshot_id': None, u'user_id':
u'01d6651d807649718e01be2999b11af0', u'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'id':
u'79e98020-555c-4179-97ff-867a79a37160', u'size': 10}, u'
volume_type': {}, u'image_id': u'92af7175-9478-42c4-8ed0-b336362ff3f7',
u'source_volid': None, u'snapshot_id': None}, u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'filter_properties': {},
u'topic': u'cinder-volume', u'image_id':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'snapshot_id': None},
u'_context_tenant': u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': False,
u'version': u'1.2', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_timestamp': u'2013-08-11T22:34:22.569488', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'm
ethod': u'create_volume', u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'adm
in'], 'timestamp': u'2013-08-11T22:34:22.569488', 'auth_token':
'<SANITIZED>', 'remote_address': u'1.2.3.4', 'quota_class': None,
'is_admin': False, 'user': u'01d66
51d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp] Making
asynchronous cast on cinder-volume.sun2-test...
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp]
UNIQUE_ID is b3bc44f910054661a7a532bd6bfbe5bb.
==> cinder-volume.log <==
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'b3bc44f910054661a7a532bd6bfbe5bb', u'args': {u'request_spec': None,
u'volume_id': u'79e98020-555c-4179-97ff-867a79a37160',
u'allow_reschedule': True, u'filter_properties':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'source_volid': None,
u'image_id': None, u'snapshot_id': None}, u'_context_tenant':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_auth_token':
'<SANITIZED>', u'_context_timestamp': u'2013-08-11T22:34:22.569488',
u'_context_is_admin': False, u'version': u'1.4', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_
read_deleted': u'no', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'method': u'create_volume',
u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23 DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'admin'], 'timestamp':
u'2013-08-11T22:34:22.569488', 'auth_token': '<SANITIZED>',
'remote_address': u'1.2.3.4', 'quota_class': None, 'is_admin': False,
'user': u'01d6651d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23 DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating lv of size 10G
2013-08-12 00:34:23 INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating
2013-08-12 00:34:23 DEBUG [cinder.utils] Running cmd (subprocess):
rbd --help
2013-08-12 00:34:23 DEBUG [cinder.utils] Running cmd (subprocess):
rbd create --pool volumes --size 10240
volume-79e98020-555c-4179-97ff-867a79a37160 --new-format
2013-08-12 00:34:23 DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating export
2013-08-12 00:34:24 INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: created successfully
2013-08-12 00:34:24 INFO [cinder.volume.manager] Clear capabilities
cinder.conf:
[DEFAULT]
rootwrap_config = /etc/cinder/rootwrap.conf
api_paste_confg = /etc/cinder/api-paste.ini
iscsi_helper = tgtadm
volume_name_template = volume-%s
volume_group = cinder-volumes
verbose = True
auth_strategy = keystone
state_path = /var/lib/cinder
lock_path = /var/lock/cinder
volumes_dir = /var/lib/cinder/volumes
rabbit_host=127.0.0.1
sql_connection=mysql://cinder:cinder@127.0.0.1/cinder?charset=utf8
api_paste_config=/etc/cinder/api-paste.ini
debug=True
rabbit_userid=nova
osapi_volume_listen=0.0.0.0
rabbit_virtual_host=/
scheduler_driver=cinder.scheduler.simple.SimpleScheduler
rabbit_hosts=127.0.0.1:5672
rabbit_ha_queues=False
rabbit_password=secret
rabbit_port=5672
rpc_backend=cinder.openstack.common.rpc.impl_kombu
sql_idle_timeout=3600
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_user=volumes
max_gigabytes=25000
rbd_pool=volumes
rbd_secret_uuid=XXXX
glance_api_version=2
glance-api.conf:
[DEFAULT]
verbose = True
debug = True
default_store = rbd
bind_host = 0.0.0.0
bind_port = 9292
log_file = /var/log/glance/api.log
backlog = 4096
sql_connection = mysql://glance:glance@127.0.0.1/glance
sql_idle_timeout = 3600
workers = 2
show_image_direct_url = True
registry_host = 0.0.0.0
registry_port = 9191
registry_client_protocol = http
notifier_strategy = noop
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False
filesystem_store_datadir = /var/lib/glance/images/
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_user = images
rbd_store_pool = images
rbd_store_chunk_size = 8
delayed_delete = False
scrub_time = 43200
scrubber_datadir = /var/lib/glance/scrubber
image_cache_dir = /var/lib/glance/image-cache/
[keystone_authtoken]
auth_host = 127.0.0.1
auth_port = 35357
auth_protocol = http
admin_tenant_name = services
admin_user = glance
admin_password = secret
[paste_deploy]
flavor=keystone+cachemanagement"
1,"1) cinder create --name vol1 1
2) cinder create --source-volid 749c9cfd-e969-4199-a090-87daff1a9d54 --display-name vo2 1 [here volume created but status : error]
+-------------------+--------------------------------------+
| Property | Value |
+-------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| created_at | 2014-02-25T12:35:43.000000 |
| description | None |
| id | d55009de-0454-48a8-b73d-01f65c071b6c |
| metadata | {} |
| name | fastvol_clone1 |
| size | 1 |
| snapshot_id | None |
| source_volid | 749c9cfd-e969-4199-a090-87daff1a9d54 |
| status | error |
| user_id | bb6690d82af84ae4b7152498088b517f |
| volume_type | fast_clone |
+-------------------+--------------------------------------+
3) if you try to delete above created volume - volume not getting deleted and volume status continuously say ""Error in deleting""
c-vol log : at step2
2014-02-26 10:34:09.395 ERROR cinder.volume.drivers.vmware.api [-] Not authenticated error occurred. Will create session and try API call again: Error(s): NotAuthenticated occurred in the cal
l to RetrievePropertiesEx..
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api Traceback (most recent call last):
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 194, in _invoke_api
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api return api_method(*args, **kwargs)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim_util.py"", line 212, in get_objects
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api options=options)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 174, in vim_request_handler
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api retrieve_properties_ex_fault_checker(response)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 153, in retrieve_properties_ex_fault_checker
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api exc_msg_list)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api VimFaultException: Error(s): NotAuthenticated occurred in the call to RetrievePropertiesEx.
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api
2014-02-26 10:34:09.611 ERROR suds.client [-] <?xml version=""1.0"" encoding=""UTF-8""?>
<SOAP-ENV:Envelope xmlns:ns0=""urn:vim25"" xmlns:ns1=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:SOAP-ENV=""http://schemas.xmlsoap.org
/soap/envelope/"">
   <ns1:Body>
      <ns0:Login>
         <ns0:_this type=""SessionManager"">SessionManager</ns0:_this>
         <ns0:userName>root</ns0:userName>
         <ns0:password>vmware</ns0:password>
      </ns0:Login>
   </ns1:Body>
</SOAP-ENV:Envelope>
2014-02-26 10:34:09.613 ERROR cinder.openstack.common.loopingcall [-] in dynamic looping call
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 123, in _inner
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall idle = self.f(*self.args, **self.kw)
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 83, in _func
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall raise excep
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall VimFaultException: Server raised fault: 'Cannot complete login due to an incorrect user name or password.'
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall"
